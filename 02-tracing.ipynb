{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc5eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "```xml\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "# Workshop 2: Tracing and Observability\n",
    "\n",
    "Welcome to Workshop 2! In this notebook, you'll learn how to add comprehensive observability to your Azure OpenAI applications using OpenTelemetry and Azure Monitor.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **OpenTelemetry Fundamentals** - Understanding traces, spans, and telemetry\n",
    "2. **Instrument Azure OpenAI calls** - Automatic tracing of API calls\n",
    "3. **Azure Application Insights Integration** - Send traces to Azure Monitor\n",
    "4. **Custom Instrumentation** - Add your own traces and metrics\n",
    "5. **Analyze Performance** - Use traces to debug and optimize\n",
    "6. **Production Monitoring** - Set up alerts and dashboards\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Workshop 1 (Deploy Your First Model)\n",
    "- Azure AI Foundry project with Application Insights configured\n",
    "- Environment variables set up correctly\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this workshop, you will:\n",
    "- Understand distributed tracing concepts\n",
    "- Instrument Azure OpenAI applications with OpenTelemetry\n",
    "- Analyze traces in Azure Application Insights\n",
    "- Create custom spans for business logic\n",
    "- Set up monitoring for production applications\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Install required packages for tracing\n",
    "%pip install opentelemetry-sdk opentelemetry-instrumentation-openai-v2 azure-monitor-opentelemetry azure-core-tracing-opentelemetry opentelemetry-exporter-otlp\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 1. Environment Setup and Imports\n",
    "\n",
    "Let's set up our environment and import the necessary libraries for tracing.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# OpenTelemetry imports\n",
    "from opentelemetry import trace, metrics\n",
    "from opentelemetry.instrumentation.openai_v2 import OpenAIInstrumentor\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanProcessor\n",
    "from opentelemetry.sdk.metrics import MeterProvider\n",
    "from opentelemetry.sdk.metrics.export import ConsoleMetricExporter, PeriodicExportingMetricReader\n",
    "\n",
    "# Azure Monitor integration\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "from azure.core.settings import settings\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"üîß Tracing Workshop Environment Check:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Check required environment variables\n",
    "required_vars = [\n",
    "    'PROJECT_ENDPOINT',\n",
    "    'AZURE_AI_FOUNDRY_RESOURCE_NAME', \n",
    "    'MODEL_DEPLOYMENT_NAME'\n",
    "]\n",
    "\n",
    "for var in required_vars:\n",
    "    value = os.getenv(var)\n",
    "    status = \"‚úÖ\" if value else \"‚ùå\"\n",
    "    print(f\"{status} {var}: {'Set' if value else 'Not set'}\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 2. Understanding OpenTelemetry Concepts\n",
    "\n",
    "Before we start instrumenting, let's understand the key concepts of observability.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "def explain_telemetry_concepts():\n",
    "    \"\"\"\n",
    "    Explain key OpenTelemetry concepts with examples.\n",
    "    \"\"\"\n",
    "    print(\"üìö OpenTelemetry Concepts:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    concepts = {\n",
    "        \"üîç Traces\": {\n",
    "            \"definition\": \"End-to-end journey of a request through your application\",\n",
    "            \"example\": \"User question ‚Üí API call ‚Üí Model inference ‚Üí Response\",\n",
    "            \"use_case\": \"Understanding request flow and finding bottlenecks\"\n",
    "        },\n",
    "        \"‚è±Ô∏è Spans\": {\n",
    "            \"definition\": \"Individual operations within a trace (building blocks)\",\n",
    "            \"example\": \"Database query, HTTP request, function execution\",\n",
    "            \"use_case\": \"Measuring duration and recording operation details\"\n",
    "        },\n",
    "        \"üè∑Ô∏è Attributes\": {\n",
    "            \"definition\": \"Key-value pairs that provide context to spans\",\n",
    "            \"example\": \"model_name=gpt-4o, user_id=123, temperature=0.7\",\n",
    "            \"use_case\": \"Filtering and analyzing traces by specific criteria\"\n",
    "        },\n",
    "        \"üìä Metrics\": {\n",
    "            \"definition\": \"Numerical measurements aggregated over time\",\n",
    "            \"example\": \"Request count, response latency, token usage\",\n",
    "            \"use_case\": \"Monitoring performance trends and alerting\"\n",
    "        },\n",
    "        \"üìù Logs\": {\n",
    "            \"definition\": \"Structured or unstructured text records of events\",\n",
    "            \"example\": \"Error messages, debug information, business events\",\n",
    "            \"use_case\": \"Debugging issues and understanding application behavior\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for concept, details in concepts.items():\n",
    "        print(f\"\\n{concept}\")\n",
    "        print(f\"  üìñ Definition: {details['definition']}\")\n",
    "        print(f\"  üí° Example: {details['example']}\")\n",
    "        print(f\"  üéØ Use Case: {details['use_case']}\")\n",
    "\n",
    "explain_telemetry_concepts()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 3. Setting Up Local Console Tracing\n",
    "\n",
    "Let's start with console tracing to see traces locally before sending them to Azure.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Configure local console tracing\n",
    "def setup_console_tracing():\n",
    "    \"\"\"\n",
    "    Set up OpenTelemetry with console output for local debugging.\n",
    "    \"\"\"\n",
    "    print(\"üñ•Ô∏è Setting up Console Tracing...\")\n",
    "    \n",
    "    # Configure tracing\n",
    "    trace.set_tracer_provider(TracerProvider())\n",
    "    tracer = trace.get_tracer(__name__)\n",
    "    \n",
    "    # Add console exporter for local viewing\n",
    "    console_exporter = ConsoleSpanExporter()\n",
    "    span_processor = BatchSpanProcessor(console_exporter)\n",
    "    trace.get_tracer_provider().add_span_processor(span_processor)\n",
    "    \n",
    "    # Configure Azure SDK tracing\n",
    "    settings.tracing_implementation = \"opentelemetry\"\n",
    "    \n",
    "    print(\"‚úÖ Console tracing configured\")\n",
    "    return tracer\n",
    "\n",
    "# Set up console tracing\n",
    "console_tracer = setup_console_tracing()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 4. Instrument Azure OpenAI SDK\n",
    "\n",
    "Now let's instrument the OpenAI SDK to automatically trace all API calls.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Instrument the OpenAI SDK\n",
    "def setup_openai_instrumentation():\n",
    "    \"\"\"\n",
    "    Instrument the OpenAI SDK for automatic tracing.\n",
    "    \"\"\"\n",
    "    print(\"üîß Instrumenting OpenAI SDK...\")\n",
    "    \n",
    "    # Instrument OpenAI SDK\n",
    "    OpenAIInstrumentor().instrument()\n",
    "    \n",
    "    # Optionally enable content recording (contains sensitive data)\n",
    "    content_recording = os.getenv('AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED', 'false').lower() == 'true'\n",
    "    if content_recording:\n",
    "        os.environ[\"AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED\"] = \"true\"\n",
    "        print(\"‚ö†Ô∏è Content recording enabled - traces will include prompts and responses\")\n",
    "    else:\n",
    "        print(\"üîí Content recording disabled - traces will not include sensitive content\")\n",
    "    \n",
    "    print(\"‚úÖ OpenAI SDK instrumentation complete\")\n",
    "\n",
    "setup_openai_instrumentation()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 5. Connect to Azure AI Foundry and Create Client\n",
    "\n",
    "Let's connect to our AI Foundry project and create an instrumented OpenAI client.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Connect to Azure AI Foundry with tracing\n",
    "try:\n",
    "    print(\"üîó Connecting to Azure AI Foundry...\")\n",
    "    \n",
    "    # Initialize project client\n",
    "    credential = DefaultAzureCredential()\n",
    "    project_client = AIProjectClient(\n",
    "        endpoint=os.getenv('PROJECT_ENDPOINT'),\n",
    "        credential=credential\n",
    "    )\n",
    "    \n",
    "    # Get OpenAI client (now instrumented)\n",
    "    openai_client = project_client.get_openai_client()\n",
    "    \n",
    "    print(\"‚úÖ Connected to Azure AI Foundry with tracing enabled\")\n",
    "    print(f\"üìç Project: {os.getenv('PROJECT_ENDPOINT')}\")\n",
    "    print(f\"ü§ñ Model: {os.getenv('MODEL_DEPLOYMENT_NAME')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    exit()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 6. Test Basic Tracing with Console Output\n",
    "\n",
    "Let's make some API calls and observe the traces in the console.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Test basic tracing with console output\n",
    "def test_console_tracing():\n",
    "    \"\"\"\n",
    "    Test OpenAI API calls with console tracing to see spans locally.\n",
    "    \"\"\"\n",
    "    print(\"üß™ Testing Console Tracing:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Create a custom span for our business logic\n",
    "    with console_tracer.start_as_current_span(\"workshop_demo\") as demo_span:\n",
    "        # Add attributes to our custom span\n",
    "        demo_span.set_attribute(\"workshop.name\", \"tracing_demo\")\n",
    "        demo_span.set_attribute(\"user.type\", \"workshop_participant\")\n",
    "        \n",
    "        print(\"Making API call with tracing...\")\n",
    "        \n",
    "        # This API call will be automatically traced by OpenAI instrumentation\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=os.getenv('MODEL_DEPLOYMENT_NAME'),\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant explaining AI concepts.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Explain what observability means in AI applications in 2 sentences.\"}\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Add response details to our span\n",
    "        demo_span.set_attribute(\"response.tokens\", response.usage.total_tokens)\n",
    "        demo_span.set_attribute(\"response.model\", response.model)\n",
    "        \n",
    "        print(f\"\\nüìù Response: {response.choices[0].message.content}\")\n",
    "        print(f\"üìä Tokens used: {response.usage.total_tokens}\")\n",
    "        \n",
    "        # Simulate some processing time\n",
    "        time.sleep(0.1)\n",
    "        demo_span.add_event(\"Processing complete\")\n",
    "\n",
    "# Run the test\n",
    "test_console_tracing()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 7. Azure Application Insights Integration\n",
    "\n",
    "Now let's configure Azure Application Insights to send traces to the cloud for persistent storage and analysis.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Set up Azure Application Insights integration\n",
    "def setup_azure_monitor():\n",
    "    \"\"\"\n",
    "    Configure Azure Monitor to send traces to Application Insights.\n",
    "    \"\"\"\n",
    "    print(\"‚òÅÔ∏è Setting up Azure Monitor Integration...\")\n",
    "    \n",
    "    try:\n",
    "        # Get Application Insights connection string from the project\n",
    "        connection_string = project_client.telemetry.get_application_insights_connection_string()\n",
    "        \n",
    "        if connection_string:\n",
    "            print(\"‚úÖ Retrieved Application Insights connection string from project\")\n",
    "            print(f\"üìç Connection: {connection_string[:50]}...\")\n",
    "            \n",
    "            # Configure Azure Monitor\n",
    "            configure_azure_monitor(connection_string=connection_string)\n",
    "            print(\"‚úÖ Azure Monitor configured successfully\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå No Application Insights connection string found\")\n",
    "            print(\"üí° Make sure Application Insights is configured in your AI Foundry project\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Azure Monitor setup failed: {e}\")\n",
    "        print(\"üí° You can still use console tracing for this workshop\")\n",
    "        return False\n",
    "\n",
    "# Configure Azure Monitor\n",
    "azure_monitor_enabled = setup_azure_monitor()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 8. Advanced Tracing with Custom Spans\n",
    "\n",
    "Let's create a more complex example with custom spans to trace business logic.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Advanced tracing example with custom spans\n",
    "def advanced_ai_workflow(user_question: str, user_id: str = \"workshop_user\"):\n",
    "    \"\"\"\n",
    "    Example of a complex AI workflow with custom tracing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get tracer for creating custom spans\n",
    "    tracer = trace.get_tracer(__name__)\n",
    "    \n",
    "    with tracer.start_as_current_span(\"ai_workflow\") as workflow_span:\n",
    "        # Add workflow-level attributes\n",
    "        workflow_span.set_attribute(\"user.id\", user_id)\n",
    "        workflow_span.set_attribute(\"workflow.type\", \"question_answering\")\n",
    "        workflow_span.set_attribute(\"input.question_length\", len(user_question))\n",
    "        \n",
    "        # Step 1: Question preprocessing\n",
    "        with tracer.start_as_current_span(\"preprocess_question\") as preprocess_span:\n",
    "            preprocess_span.add_event(\"Starting question preprocessing\")\n",
    "            \n",
    "            # Simulate preprocessing\n",
    "            cleaned_question = user_question.strip().lower()\n",
    "            question_type = \"technical\" if any(word in cleaned_question for word in [\"ai\", \"ml\", \"algorithm\", \"model\"]) else \"general\"\n",
    "            \n",
    "            preprocess_span.set_attribute(\"question.type\", question_type)\n",
    "            preprocess_span.set_attribute(\"question.cleaned_length\", len(cleaned_question))\n",
    "            \n",
    "            time.sleep(0.05)  # Simulate processing time\n",
    "            preprocess_span.add_event(\"Preprocessing complete\")\n",
    "        \n",
    "        # Step 2: Select appropriate system prompt based on question type\n",
    "        with tracer.start_as_current_span(\"select_system_prompt\") as prompt_span:\n",
    "            if question_type == \"technical\":\n",
    "                system_prompt = \"You are an expert AI/ML engineer. Provide technical but accessible explanations.\"\n",
    "            else:\n",
    "                system_prompt = \"You are a helpful assistant. Provide clear and friendly explanations.\"\n",
    "            \n",
    "            prompt_span.set_attribute(\"prompt.type\", question_type)\n",
    "            prompt_span.set_attribute(\"prompt.length\", len(system_prompt))\n",
    "        \n",
    "        # Step 3: Make AI API call (automatically traced)\n",
    "        with tracer.start_as_current_span(\"ai_inference\") as inference_span:\n",
    "            inference_span.add_event(\"Starting AI inference\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = openai_client.chat.completions.create(\n",
    "                model=os.getenv('MODEL_DEPLOYMENT_NAME'),\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_question}\n",
    "                ],\n",
    "                max_tokens=200,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # Add inference metrics to span\n",
    "            inference_span.set_attribute(\"inference.duration_ms\", round(inference_time * 1000, 2))\n",
    "            inference_span.set_attribute(\"tokens.prompt\", response.usage.prompt_tokens)\n",
    "            inference_span.set_attribute(\"tokens.completion\", response.usage.completion_tokens) \n",
    "            inference_span.set_attribute(\"tokens.total\", response.usage.total_tokens)\n",
    "            inference_span.set_attribute(\"model.name\", response.model)\n",
    "            \n",
    "            inference_span.add_event(\"AI inference complete\")\n",
    "        \n",
    "        # Step 4: Post-process response\n",
    "        with tracer.start_as_current_span(\"postprocess_response\") as postprocess_span:\n",
    "            ai_response = response.choices[0].message.content\n",
    "            \n",
    "            # Simulate some post-processing\n",
    "            word_count = len(ai_response.split())\n",
    "            has_code = \"```\" in ai_response\n",
    "            \n",
    "            postprocess_span.set_attribute(\"response.word_count\", word_count)\n",
    "            postprocess_span.set_attribute(\"response.has_code\", has_code)\n",
    "            postprocess_span.set_attribute(\"response.length\", len(ai_response))\n",
    "            \n",
    "            time.sleep(0.02)  # Simulate processing time\n",
    "        \n",
    "        # Add final workflow metrics\n",
    "        workflow_span.set_attribute(\"workflow.success\", True)\n",
    "        workflow_span.set_attribute(\"workflow.total_tokens\", response.usage.total_tokens)\n",
    "        workflow_span.add_event(\"Workflow complete\")\n",
    "        \n",
    "        return {\n",
    "            \"response\": ai_response,\n",
    "            \"metadata\": {\n",
    "                \"question_type\": question_type,\n",
    "                \"tokens_used\": response.usage.total_tokens,\n",
    "                \"inference_time_ms\": round(inference_time * 1000, 2),\n",
    "                \"word_count\": word_count\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Test the advanced workflow\n",
    "print(\"üöÄ Testing Advanced AI Workflow with Custom Tracing:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "test_questions = [\n",
    "    \"What is machine learning and how does it work?\",\n",
    "    \"What's the weather like today?\",\n",
    "    \"Explain the difference between supervised and unsupervised learning algorithms.\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{i}Ô∏è‚É£ Question: {question}\")\n",
    "    result = advanced_ai_workflow(question, f\"user_{i}\")\n",
    "    print(f\"üìù Response: {result['response'][:100]}...\")\n",
    "    print(f\"üìä Metadata: {result['metadata']}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 9. Error Handling and Tracing\n",
    "\n",
    "Let's see how to trace errors and exceptions in your AI applications.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Error handling with tracing\n",
    "def trace_with_error_handling():\n",
    "    \"\"\"\n",
    "    Demonstrate how to trace errors and exceptions.\n",
    "    \"\"\"\n",
    "    tracer = trace.get_tracer(__name__)\n",
    "    \n",
    "    print(\"üö® Testing Error Handling with Tracing:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Test 1: API call with invalid parameters\n",
    "    with tracer.start_as_current_span(\"test_invalid_model\") as span:\n",
    "        span.set_attribute(\"test.type\", \"invalid_model\")\n",
    "        \n",
    "        try:\n",
    "            # This should fail due to invalid model name\n",
    "            response = openai_client.chat.completions.create(\n",
    "                model=\"invalid-model-name\",\n",
    "                messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "                max_tokens=50\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Record the error in the span\n",
    "            span.record_exception(e)\n",
    "            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n",
    "            span.set_attribute(\"error.type\", type(e).__name__)\n",
    "            span.set_attribute(\"error.message\", str(e))\n",
    "            \n",
    "            print(f\"‚ùå Expected error captured: {type(e).__name__}\")\n",
    "    \n",
    "    # Test 2: Rate limiting simulation\n",
    "    with tracer.start_as_current_span(\"test_rate_limiting\") as span:\n",
    "        span.set_attribute(\"test.type\", \"rate_limiting_simulation\")\n",
    "        \n",
    "        try:\n",
    "            # Simulate rate limiting by making rapid requests\n",
    "            print(\"üîÑ Simulating multiple rapid requests...\")\n",
    "            \n",
    "            for i in range(3):\n",
    "                with tracer.start_as_current_span(f\"request_{i}\") as req_span:\n",
    "                    req_span.set_attribute(\"request.number\", i)\n",
    "                    \n",
    "                    try:\n",
    "                        response = openai_client.chat.completions.create(\n",
    "                            model=os.getenv('MODEL_DEPLOYMENT_NAME'),\n",
    "                            messages=[{\"role\": \"user\", \"content\": f\"Quick question {i}\"}],\n",
    "                            max_tokens=20\n",
    "                        )\n",
    "                        \n",
    "                        req_span.set_attribute(\"request.success\", True)\n",
    "                        req_span.set_attribute(\"tokens.used\", response.usage.total_tokens)\n",
    "                        print(f\"  ‚úÖ Request {i} succeeded\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        req_span.record_exception(e)\n",
    "                        req_span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n",
    "                        req_span.set_attribute(\"request.success\", False)\n",
    "                        print(f\"  ‚ùå Request {i} failed: {type(e).__name__}\")\n",
    "                    \n",
    "                    time.sleep(0.1)  # Small delay between requests\n",
    "                    \n",
    "        except Exception as e:\n",
    "            span.record_exception(e)\n",
    "            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n",
    "            print(f\"‚ùå Batch request error: {e}\")\n",
    "\n",
    "# Run error handling test\n",
    "trace_with_error_handling()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 10. Performance Analysis with Tracing\n",
    "\n",
    "Let's create a performance analysis example that shows how to use tracing data.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Performance analysis with tracing\n",
    "def performance_analysis_demo():\n",
    "    \"\"\"\n",
    "    Demonstrate performance analysis using tracing data.\n",
    "    \"\"\"\n",
    "    tracer = trace.get_tracer(__name__)\n",
    "    \n",
    "    print(\"üìä Performance Analysis Demo:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Test different temperature values and their impact on response time\n",
    "    temperatures = [0.1, 0.5, 1.0]\n",
    "    results = []\n",
    "    \n",
    "    with tracer.start_as_current_span(\"performance_analysis\") as analysis_span:\n",
    "        analysis_span.set_attribute(\"analysis.type\", \"temperature_impact\")\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            with tracer.start_as_current_span(f\"temperature_test_{temp}\") as temp_span:\n",
    "                temp_span.set_attribute(\"model.temperature\", temp)\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                try:\n",
    "                    response = openai_client.chat.completions.create(\n",
    "                        model=os.getenv('MODEL_DEPLOYMENT_NAME'),\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are a creative writer.\"},\n",
    "                            {\"role\": \"user\", \"content\": \"Write a short poem about AI.\"}\n",
    "                        ],\n",
    "                        max_tokens=100,\n",
    "                        temperature=temp\n",
    "                    )\n",
    "                    \n",
    "                    end_time = time.time()\n",
    "                    duration = round((end_time - start_time) * 1000, 2)\n",
    "                    \n",
    "                    # Add performance metrics to span\n",
    "                    temp_span.set_attribute(\"performance.duration_ms\", duration)\n",
    "                    temp_span.set_attribute(\"performance.tokens_per_second\", \n",
    "                                          round(response.usage.total_tokens / (duration/1000), 2))\n",
    "                    temp_span.set_attribute(\"tokens.total\", response.usage.total_tokens)\n",
    "                    temp_span.set_attribute(\"response.length\", len(response.choices[0].message.content))\n",
    "                    \n",
    "                    results.append({\n",
    "                        'temperature': temp,\n",
    "                        'duration_ms': duration,\n",
    "                        'tokens': response.usage.total_tokens,\n",
    "                        'tokens_per_second': round(response.usage.total_tokens / (duration/1000), 2),\n",
    "                        'response_length': len(response.choices[0].message.content)\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"üå°Ô∏è Temperature {temp}: {duration}ms, {response.usage.total_tokens} tokens\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    temp_span.record_exception(e)\n",
    "                    temp_span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n",
    "                    print(f\"‚ùå Temperature {temp} failed: {e}\")\n",
    "    \n",
    "    # Analyze results\n",
    "    if results:\n",
    "        print(f\"\\nüìà Performance Analysis Results:\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        avg_duration = sum(r['duration_ms'] for r in results) / len(results)\n",
    "        avg_tokens_per_sec = sum(r['tokens_per_second'] for r in results) / len(results)\n",
    "        \n",
    "        print(f\"Average duration: {avg_duration:.2f}ms\")\n",
    "        print(f\"Average tokens/sec: {avg_tokens_per_sec:.2f}\")\n",
    "        \n",
    "        # Find fastest and slowest\n",
    "        fastest = min(results, key=lambda x: x['duration_ms'])\n",
    "        slowest = max(results, key=lambda x: x['duration_ms'])\n",
    "        \n",
    "        print(f\"Fastest: Temperature {fastest['temperature']} ({fastest['duration_ms']}ms)\")\n",
    "        print(f\"Slowest: Temperature {slowest['temperature']} ({slowest['duration_ms']}ms)\")\n",
    "\n",
    "# Run performance analysis\n",
    "performance_analysis_demo()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 11. Viewing Traces in Azure Application Insights\n",
    "\n",
    "If Azure Monitor is configured, your traces are now being sent to Application Insights. Here's how to view them.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Instructions for viewing traces in Azure\n",
    "def show_azure_insights_instructions():\n",
    "    \"\"\"\n",
    "    Provide instructions for viewing traces in Azure Application Insights.\n",
    "    \"\"\"\n",
    "    print(\"‚òÅÔ∏è Viewing Traces in Azure Application Insights:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if azure_monitor_enabled:\n",
    "        print(\"‚úÖ Your traces are being sent to Azure Application Insights!\")\n",
    "        print()\n",
    "        \n",
    "        steps = [\n",
    "            \"1. Open the Azure Portal (portal.azure.com)\",\n",
    "            \"2. Navigate to your AI Foundry project resource\",\n",
    "            \"3. Go to 'Observability' -> 'Tracing' in the left menu\",\n",
    "            \"4. Or directly open Application Insights resource\",\n",
    "            \"5. In Application Insights, go to 'Investigate' -> 'Transaction search'\",\n",
    "            \"6. Look for traces with operation names like:\",\n",
    "            \"   ‚Ä¢ 'ai_workflow' (our custom spans)\",\n",
    "            \"   ‚Ä¢ 'chat/completions' (OpenAI API calls)\",\n",
    "            \"   ‚Ä¢ 'workshop_demo' (our demo spans)\",\n",
    "            \"7. Click on a trace to see the detailed span timeline\",\n",
    "            \"8. Explore the 'Performance' tab for aggregated metrics\",\n",
    "            \"9. Use 'Logs' to write KQL queries for custom analysis\"\n",
    "        ]\n",
    "        \n",
    "        for step in steps:\n",
    "            print(step)\n",
    "        \n",
    "        print(f\"\\nüîç What to Look For:\")\n",
    "        insights = [\n",
    "            \"‚Ä¢ End-to-end trace duration\",\n",
    "            \"‚Ä¢ Individual span timings (preprocessing, inference, postprocessing)\",\n",
    "            \"‚Ä¢ OpenAI API call details and token usage\",\n",
    "            \"‚Ä¢ Custom attributes we added (question_type, user_id, etc.)\",\n",
    "            \"‚Ä¢ Error traces and exception details\",\n",
    "            \"‚Ä¢ Performance patterns across different requests\"\n",
    "        ]\n",
    "        \n",
    "        for insight in insights:\n",
    "            print(insight)\n",
    "        \n",
    "        print(f\"\\nüìä Useful KQL Queries for Application Insights:\")\n",
    "        queries = [\n",
    "            \"// All traces from our workshop\",\n",
    "            \"traces | where customDimensions.['workshop.name'] == 'tracing_demo'\",\n",
    "            \"\",\n",
    "            \"// OpenAI API performance\",\n",
    "            \"requests | where name contains 'chat/completions'\",\n",
    "            \"| summarize avg(duration), count() by bin(timestamp, 5m)\",\n",
    "            \"\",\n",
    "            \"// Token usage over time\", \n",
    "            \"traces | where customDimensions.['tokens.total'] != ''\",\n",
    "            \"| extend tokens = toint(customDimensions.['tokens.total'])\",\n",
    "            \"| summarize avg(tokens), sum(tokens) by bin(timestamp, 1h)\"\n",
    "        ]\n",
    "        \n",
    "        for query in queries:\n",
    "            print(query)\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ùå Azure Monitor not configured\")\n",
    "        print(\"üí° To enable Azure Application Insights:\")\n",
    "        print(\"1. Ensure Application Insights is configured in your AI Foundry project\")\n",
    "        print(\"2. Check the 'Observability' section in Azure AI Foundry portal\")\n",
    "        print(\"3. Verify your connection string is accessible\")\n",
    "\n",
    "show_azure_insights_instructions()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 12. Production Best Practices\n",
    "\n",
    "Let's cover best practices for using tracing in production environments.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Production best practices\n",
    "def production_best_practices():\n",
    "    \"\"\"\n",
    "    Demonstrate production best practices for tracing.\n",
    "    \"\"\"\n",
    "    print(\"üè≠ Production Tracing Best Practices:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    practices = {\n",
    "        \"üîí Security\": [\n",
    "            \"‚Ä¢ Disable content recording in production (set AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED=false)\",\n",
    "            \"‚Ä¢ Use Azure Managed Identity instead of API keys\",\n",
    "            \"‚Ä¢ Be careful with custom attributes - don't include PII\",\n",
    "            \"‚Ä¢ Review trace data retention policies\"\n",
    "        ],\n",
    "        \"‚ö° Performance\": [\n",
    "            \"‚Ä¢ Use sampling to reduce trace volume (start with 1% sampling)\",\n",
    "            \"‚Ä¢ Implement batch span processors instead of simple processors\",\n",
    "            \"‚Ä¢ Set appropriate timeout values for exporters\",\n",
    "            \"‚Ä¢ Monitor the overhead of tracing itself\"\n",
    "        ],\n",
    "        \"üìä Monitoring\": [\n",
    "            \"‚Ä¢ Set up alerts on error rates and high latency\",\n",
    "            \"‚Ä¢ Monitor token usage trends and costs\",\n",
    "            \"‚Ä¢ Track model performance metrics over time\",\n",
    "            \"‚Ä¢ Create dashboards for key business metrics\"\n",
    "        ],\n",
    "        \"üèóÔ∏è Architecture\": [\n",
    "            \"‚Ä¢ Use consistent span names across services\",\n",
    "            \"‚Ä¢ Add business context through custom attributes\",\n",
    "            \"‚Ä¢ Implement correlation IDs for distributed tracing\",\n",
    "            \"‚Ä¢ Document your tracing strategy and naming conventions\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in practices.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for item in items:\n",
    "            print(f\"  {item}\")\n",
    "    \n",
    "    print(f\"\\nüö® Common Pitfalls to Avoid:\")\n",
    "    pitfalls = [\n",
    "        \"‚Ä¢ Tracing sensitive data (passwords, personal info)\",\n",
    "        \"‚Ä¢ Over-instrumenting and creating performance overhead\",\n",
    "        \"‚Ä¢ Not handling tracing failures gracefully\",\n",
    "        \"‚Ä¢ Forgetting to update trace configurations in different environments\",\n",
    "        \"‚Ä¢ Not correlating traces with business metrics\"\n",
    "    ]\n",
    "    \n",
    "    for pitfall in pitfalls:\n",
    "        print(f\"  {pitfall}\")\n",
    "\n",
    "production_best_practices()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 13. Workshop Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed Workshop 2 on Tracing and Observability.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Workshop summary\n",
    "def workshop_summary():\n",
    "    \"\"\"\n",
    "    Summarize what was learned in Workshop 2.\n",
    "    \"\"\"\n",
    "    print(\"üéØ Workshop 2 Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    achievements = [\n",
    "        \"‚úÖ Understood OpenTelemetry concepts (traces, spans, attributes)\",\n",
    "        \"‚úÖ Instrumented Azure OpenAI SDK for automatic tracing\",\n",
    "        \"‚úÖ Set up console tracing for local debugging\",\n",
    "        \"‚úÖ Configured Azure Application Insights integration\",\n",
    "        \"‚úÖ Created custom spans for business logic\",\n",
    "        \"‚úÖ Implemented error handling with tracing\",\n",
    "        \"‚úÖ Performed performance analysis using trace data\",\n",
    "        \"‚úÖ Learned production best practices\"\n",
    "    ]\n",
    "    \n",
    "    for achievement in achievements:\n",
    "        print(achievement)\n",
    "    \n",
    "    print(f\"\\nüîß Technical Skills Gained:\")\n",
    "    skills = [\n",
    "        \"‚Ä¢ OpenTelemetry SDK configuration and usage\",\n",
    "        \"‚Ä¢ Azure Monitor OpenTelemetry integration\",\n",
    "        \"‚Ä¢ Custom span creation and attribute management\",\n",
    "        \"‚Ä¢ Error tracking and exception recording\",\n",
    "        \"‚Ä¢ Performance measurement and analysis\",\n",
    "        \"‚Ä¢ Production monitoring setup\"\n",
    "    ]\n",
    "    \n",
    "    for skill in skills:\n",
    "        print(skill)\n",
    "    \n",
    "    print(f\"\\nüöÄ Next Workshop Preview:\")\n",
    "    print(\"Workshop 3: AI Agents\")\n",
    "    print(\"‚Ä¢ Create intelligent AI agents with tools\")\n",
    "    print(\"‚Ä¢ Implement function calling capabilities\")\n",
    "    print(\"‚Ä¢ Build multi-step reasoning workflows\")\n",
    "    print(\"‚Ä¢ Trace agent interactions and decision-making\")\n",
    "    \n",
    "    print(f\"\\nüí° Homework:\")\n",
    "    print(\"‚Ä¢ Explore your traces in Azure Application Insights\")\n",
    "    print(\"‚Ä¢ Try adding custom attributes to trace business metrics\")\n",
    "    print(\"‚Ä¢ Experiment with different sampling rates\")\n",
    "    print(\"‚Ä¢ Set up a simple alert on token usage or error rates\")\n",
    "\n",
    "# Final cleanup\n",
    "def cleanup_instrumentation():\n",
    "    \"\"\"\n",
    "    Clean up OpenTelemetry instrumentation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        OpenAIInstrumentor().uninstrument()\n",
    "        print(\"üßπ OpenAI instrumentation cleaned up\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: Cleanup not needed or failed: {e}\")\n",
    "\n",
    "workshop_summary()\n",
    "cleanup_instrumentation()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## üîß Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "#### Tracing Not Appearing\n",
    "- **Problem**: No traces visible in console or Azure\n",
    "- **Solution**: \n",
    "  - Check if OpenAI instrumentation is properly set up\n",
    "  - Verify Azure Monitor connection string\n",
    "  - Ensure you're making actual API calls\n",
    "\n",
    "#### Application Insights Connection Issues\n",
    "- **Problem**: \"No connection string found\"\n",
    "- **Solution**:\n",
    "  - Check if Application Insights is configured in your AI Foundry project\n",
    "  - Go to Azure AI Foundry portal ‚Üí Observability ‚Üí Tracing\n",
    "  - Verify your Azure permissions\n",
    "\n",
    "#### Performance Overhead\n",
    "- **Problem**: Tracing is slowing down your application\n",
    "- **Solution**:\n",
    "  - Implement sampling (trace only 1-10% of requests)\n",
    "  - Use batch processors instead of simple processors\n",
    "  - Disable content recording in production\n",
    "\n",
    "#### Missing Trace Data\n",
    "- **Problem**: Some spans or attributes are missing\n",
    "- **Solution**:\n",
    "  - Check for exceptions in span creation\n",
    "  - Verify attribute names don't contain special characters\n",
    "  - Ensure spans are properly closed\n",
    "\n",
    "### üìö Additional Resources\n",
    "\n",
    "- [OpenTelemetry Python Documentation](https://opentelemetry.io/docs/languages/python/)\n",
    "- [Azure Monitor OpenTelemetry Documentation](https://learn.microsoft.com/azure/azure-monitor/app/opentelemetry-enable)\n",
    "- [Azure AI Foundry Tracing Guide](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/trace-application)\n",
    "- [Application Insights KQL Reference](https://learn.microsoft.com/azure/data-explorer/kusto/query/)\n",
    "\n",
    "### üéÆ Try These Extensions\n",
    "\n",
    "1. **Add Custom Metrics**: Implement custom metrics for token costs\n",
    "2. **Correlation IDs**: Add correlation IDs to track requests across services  \n",
    "3. **Sampling**: Implement different sampling strategies\n",
    "4. **Dashboards**: Create custom dashboards in Application Insights\n",
    "</VSCode.Cell>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
