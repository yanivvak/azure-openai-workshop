{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6627c2af",
   "metadata": {},
   "source": [
    "# Azure AI Foundry Agent Evaluators (Preview)\n",
    "\n",
    "This notebook demonstrates how to use Azure AI Foundry's agent-specific evaluators to assess the performance of AI agents. We'll cover three main evaluators:\n",
    "\n",
    "- **Intent Resolution**: Measures how well the system identifies and understands user requests\n",
    "- **Tool Call Accuracy**: Evaluates the accuracy and efficiency of tool calls made by an agent\n",
    "- **Task Adherence**: Assesses how well an agent adheres to assigned tasks\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Azure AI Foundry subscription\n",
    "- Azure OpenAI resource\n",
    "- Required Python packages: `azure-ai-evaluation`, `python-dotenv`\n",
    "\n",
    "**Note**: Agent evaluators are currently in public preview."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1af5d23",
   "metadata": {},
   "source": [
    "## 1. Setup and Authentication\n",
    "\n",
    "First, let's set up our environment variables and authentication. Create a `.env` file with your Azure credentials:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bafdec6",
   "metadata": {},
   "source": [
    "## 2. Import Required Libraries\n",
    "\n",
    "Import all necessary libraries for agent evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4834540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Azure Identity for Entra ID authentication\n",
    "from azure.identity import DefaultAzureCredential, AzureCliCredential\n",
    "\n",
    "# Azure AI Evaluation imports\n",
    "from azure.ai.evaluation import (\n",
    "    AzureOpenAIModelConfiguration,\n",
    "    IntentResolutionEvaluator,\n",
    "    ToolCallAccuracyEvaluator,\n",
    "    TaskAdherenceEvaluator\n",
    ")\n",
    "\n",
    "# Load environment variables (override any existing ones)\n",
    "load_dotenv(override=True)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2717aa2",
   "metadata": {},
   "source": [
    "## 3. Configure Azure AI Foundry Client\n",
    "\n",
    "Set up the model configuration for the LLM-judge used by evaluators.\n",
    "\n",
    "### Authentication Limitations & Workaround ‚ö†Ô∏è\n",
    "\n",
    "**Important**: The current version of `azure.ai.evaluation` library only supports **API key authentication**. However, some Azure OpenAI resources are configured to only allow Entra ID authentication (key-based authentication disabled).\n",
    "\n",
    "### Workaround for Entra ID-Only Resources\n",
    "\n",
    "If your Azure OpenAI resource has API key authentication disabled, we'll use a workaround:\n",
    "\n",
    "1. **Get Entra ID Access Token**: Use Azure CLI to get an access token\n",
    "2. **Token as API Key**: Pass the access token as the `api_key` parameter\n",
    "3. **Automatic Fallback**: The code will automatically detect and use this approach\n",
    "\n",
    "### Required Setup\n",
    "\n",
    "Make sure you have:\n",
    "1. **Azure CLI installed** and logged in (`az login`)\n",
    "2. **Appropriate permissions** (Cognitive Services OpenAI User role)\n",
    "3. **Environment variables** set in your `.env` file\n",
    "\n",
    "### Environment Variables\n",
    "\n",
    "```\n",
    "AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/\n",
    "AZURE_OPENAI_DEPLOYMENT_NAME=your-deployment-name\n",
    "OPENAI_API_VERSION=2024-10-21\n",
    "\n",
    "# Optional: API key (if resource supports key-based auth)\n",
    "AZURE_OPENAI_API_KEY=your-api-key-here\n",
    "```\n",
    "\n",
    "### Future Support\n",
    "\n",
    "Microsoft is working on adding native Entra ID support to the Azure AI Evaluation library. Until then, this workaround provides a secure way to use Entra ID authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc76701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Azure OpenAI model for evaluation\n",
    "print(\"üîß Setting up authentication...\")\n",
    "\n",
    "# Note: The current version of azure.ai.evaluation AzureOpenAIModelConfiguration \n",
    "# only supports API key authentication, not azure_credential/Entra ID\n",
    "# However, this resource has key-based authentication disabled.\n",
    "# We'll use a workaround with Entra ID token\n",
    "\n",
    "try:\n",
    "    # First, try API key authentication if available\n",
    "    if \"AZURE_OPENAI_API_KEY\" in os.environ and os.environ[\"AZURE_OPENAI_API_KEY\"]:\n",
    "        model_config = AzureOpenAIModelConfiguration(\n",
    "            azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "            api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "            azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "            api_version=os.environ[\"OPENAI_API_VERSION\"],\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Model configuration created with API key authentication\")\n",
    "        print(\"   Authentication: API Key\")\n",
    "        \n",
    "    else:\n",
    "        # Fallback: Get Entra ID token and use as API key (workaround)\n",
    "        print(\"üîÑ API key not available, using Entra ID token workaround...\")\n",
    "        \n",
    "        # Get access token using Azure CLI\n",
    "        import subprocess\n",
    "        result = subprocess.run([\n",
    "            \"az\", \"account\", \"get-access-token\", \n",
    "            \"--resource\", \"https://cognitiveservices.azure.com\"\n",
    "        ], capture_output=True, text=True, check=True)\n",
    "        \n",
    "        import json\n",
    "        token_info = json.loads(result.stdout)\n",
    "        access_token = token_info[\"accessToken\"]\n",
    "        \n",
    "        # Use token as API key (this is a workaround)\n",
    "        model_config = AzureOpenAIModelConfiguration(\n",
    "            azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "            api_key=access_token,  # Use Entra ID token as API key\n",
    "            azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "            api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Model configuration created with Entra ID token (workaround)\")\n",
    "        print(\"   Authentication: Entra ID Token (as API Key)\")\n",
    "        print(\"   Note: This is a workaround for the library limitation\")\n",
    "        \n",
    "except KeyError as e:\n",
    "    print(f\"‚ùå Missing environment variable: {e}\")\n",
    "    print(\"   Please ensure all required environment variables are set:\")\n",
    "    print(\"   - AZURE_OPENAI_ENDPOINT\")\n",
    "    print(\"   - AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "    print(\"   - OPENAI_API_VERSION\")\n",
    "    raise\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error setting up authentication: {e}\")\n",
    "    print(\"   Make sure you're logged in with 'az login' and have appropriate permissions\")\n",
    "    raise\n",
    "\n",
    "print(f\"   Endpoint: {os.environ['AZURE_OPENAI_ENDPOINT']}\")\n",
    "print(f\"   Deployment: {os.environ['AZURE_OPENAI_DEPLOYMENT_NAME']}\")\n",
    "print(f\"   API Version: {os.environ['AZURE_OPENAI_API_VERSION']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1ef8fa",
   "metadata": {},
   "source": [
    "## 4. Intent Resolution Evaluator\n",
    "\n",
    "The `IntentResolutionEvaluator` measures how well the system identifies and understands a user's request. It evaluates:\n",
    "- How well it scopes the user's intent\n",
    "- How well it asks clarifying questions\n",
    "- How well it reminds users of its capabilities\n",
    "\n",
    "**Score**: 1-5 (Likert scale), higher is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c5e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Intent Resolution evaluator\n",
    "intent_resolution = IntentResolutionEvaluator(\n",
    "    model_config=model_config, \n",
    "    threshold=3  # Scores >= 3 are considered \"pass\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Intent Resolution Evaluator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b30517",
   "metadata": {},
   "source": [
    "### Example 1: Good Intent Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce30361f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Example of good intent resolution\n",
    "result_good = intent_resolution(\n",
    "    query=\"What are the opening hours of the Eiffel Tower?\",\n",
    "    response=\"The Eiffel Tower is open daily from 9:00 AM to 11:00 PM. Please note that hours may vary during holidays or special events.\"\n",
    ")\n",
    "\n",
    "print(\"=== Good Intent Resolution Example ===\")\n",
    "print(f\"Query: What are the opening hours of the Eiffel Tower?\")\n",
    "print(f\"Response: The Eiffel Tower is open daily from 9:00 AM to 11:00 PM...\")\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(f\"Score: {result_good['intent_resolution']}\")\n",
    "print(f\"Result: {result_good['intent_resolution_result']}\")\n",
    "print(f\"Reason: {result_good['intent_resolution_reason']}\")\n",
    "\n",
    "# Check if additional_details exists before accessing it\n",
    "if 'additional_details' in result_good:\n",
    "    print(f\"Additional Details: {json.dumps(result_good['additional_details'], indent=2)}\")\n",
    "else:\n",
    "    print(\"Additional Details: Not available\")\n",
    "    print(f\"Available keys in result: {list(result_good.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa4d47",
   "metadata": {},
   "source": [
    "### Example 2: Poor Intent Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19647296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Example of poor intent resolution\n",
    "result_poor = intent_resolution(\n",
    "    query=\"What are the opening hours of the Eiffel Tower?\",\n",
    "    response=\"Paris is a beautiful city with many attractions to visit.\"\n",
    ")\n",
    "\n",
    "print(\"=== Poor Intent Resolution Example ===\")\n",
    "print(f\"Query: What are the opening hours of the Eiffel Tower?\")\n",
    "print(f\"Response: Paris is a beautiful city with many attractions to visit.\")\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(f\"Score: {result_poor['intent_resolution']}\")\n",
    "print(f\"Result: {result_poor['intent_resolution_result']}\")\n",
    "print(f\"Reason: {result_poor['intent_resolution_reason']}\")\n",
    "\n",
    "# Check if additional_details exists before accessing it\n",
    "if 'additional_details' in result_poor:\n",
    "    print(f\"Additional Details: {json.dumps(result_poor['additional_details'], indent=2)}\")\n",
    "else:\n",
    "    print(\"Additional Details: Not available\")\n",
    "    print(f\"Available keys in result: {list(result_poor.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca44795",
   "metadata": {},
   "source": [
    "## 5. Tool Call Accuracy Evaluator\n",
    "\n",
    "The `ToolCallAccuracyEvaluator` measures the accuracy and efficiency of tool calls made by an agent. It evaluates:\n",
    "- Relevance and helpfulness of the tool invoked\n",
    "- Correctness of parameters used in tool calls\n",
    "- Counts of missing or excessive calls\n",
    "\n",
    "**Note**: Only supports Azure AI Agent's Function Tool evaluation (not Built-in Tools)\n",
    "**Score**: 1-5 (Likert scale), higher is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83e92a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Tool Call Accuracy evaluator\n",
    "tool_call_accuracy = ToolCallAccuracyEvaluator(\n",
    "    model_config=model_config, \n",
    "    threshold=3\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tool Call Accuracy Evaluator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0f5c7c",
   "metadata": {},
   "source": [
    "### Example: Weather Query Tool Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089a248f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example tool call evaluation\n",
    "tool_result = tool_call_accuracy(\n",
    "    query=\"How is the weather in Seattle?\",\n",
    "    tool_calls=[\n",
    "        {\n",
    "            \"type\": \"tool_call\",\n",
    "            \"tool_call_id\": \"call_CUdbkBfvVBla2YP3p24uhElJ\",\n",
    "            \"name\": \"fetch_weather\",\n",
    "            \"arguments\": {\n",
    "                \"location\": \"Seattle\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    tool_definitions=[\n",
    "        {\n",
    "            \"id\": \"fetch_weather\",\n",
    "            \"name\": \"fetch_weather\",\n",
    "            \"description\": \"Fetches the weather information for the specified location.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The location to fetch weather for.\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=== Tool Call Accuracy Example ===\")\n",
    "print(f\"Query: How is the weather in Seattle?\")\n",
    "print(f\"Tool Called: fetch_weather(location='Seattle')\")\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(f\"Score: {tool_result['tool_call_accuracy']}\")\n",
    "print(f\"Result: {tool_result['tool_call_accuracy_result']}\")\n",
    "print(f\"Details: {json.dumps(tool_result['details'], indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb054ad",
   "metadata": {},
   "source": [
    "### Example: Multiple Tool Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d635cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with multiple tool calls\n",
    "multi_tool_result = tool_call_accuracy(\n",
    "    query=\"What's the weather in New York and what restaurants are nearby?\",\n",
    "    tool_calls=[\n",
    "        {\n",
    "            \"type\": \"tool_call\",\n",
    "            \"tool_call_id\": \"call_weather_001\",\n",
    "            \"name\": \"fetch_weather\",\n",
    "            \"arguments\": {\n",
    "                \"location\": \"New York\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"tool_call\",\n",
    "            \"tool_call_id\": \"call_restaurants_001\",\n",
    "            \"name\": \"find_restaurants\",\n",
    "            \"arguments\": {\n",
    "                \"location\": \"New York\",\n",
    "                \"radius\": \"5km\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    tool_definitions=[\n",
    "        {\n",
    "            \"id\": \"fetch_weather\",\n",
    "            \"name\": \"fetch_weather\",\n",
    "            \"description\": \"Fetches the weather information for the specified location.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The location to fetch weather for.\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"find_restaurants\",\n",
    "            \"name\": \"find_restaurants\",\n",
    "            \"description\": \"Find restaurants near a specified location.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The location to search near.\"\n",
    "                    },\n",
    "                    \"radius\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Search radius (e.g., '5km', '1mile').\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"=== Multiple Tool Calls Example ===\")\n",
    "print(f\"Query: What's the weather in New York and what restaurants are nearby?\")\n",
    "print(f\"Tools Called: fetch_weather + find_restaurants\")\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(f\"Score: {multi_tool_result['tool_call_accuracy']}\")\n",
    "print(f\"Result: {multi_tool_result['tool_call_accuracy_result']}\")\n",
    "print(f\"Total Tools Called: {multi_tool_result['details']['tool_calls_made_by_agent']}\")\n",
    "print(f\"Correct Tools Called: {multi_tool_result['details']['correct_tool_calls_made_by_agent']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f804d",
   "metadata": {},
   "source": [
    "## 6. Task Adherence Evaluator\n",
    "\n",
    "The `TaskAdherenceEvaluator` measures how well an agent's response adheres to their assigned tasks. It evaluates:\n",
    "- Adherence to task instructions (from system message and user query)\n",
    "- Appropriate use of available tools\n",
    "- Staying on track vs. making inefficient or out-of-scope steps\n",
    "\n",
    "**Score**: 1-5 (Likert scale), higher is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12160dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Task Adherence evaluator\n",
    "task_adherence = TaskAdherenceEvaluator(\n",
    "    model_config=model_config, \n",
    "    threshold=3\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Task Adherence Evaluator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f7d38f",
   "metadata": {},
   "source": [
    "### Example 1: Good Task Adherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4171f9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of good task adherence\n",
    "adherence_good = task_adherence(\n",
    "    query=\"What are the best practices for maintaining a healthy rose garden during the summer?\",\n",
    "    response=\"\"\"Here are the key best practices for maintaining a healthy rose garden during summer:\n",
    "\n",
    "1. **Watering**: Water deeply 2-3 times per week, preferably early morning. Aim for 1-1.5 inches per week.\n",
    "2. **Mulching**: Apply 2-3 inches of organic mulch around plants to retain moisture and suppress weeds.\n",
    "3. **Pruning**: Remove spent blooms (deadheading) regularly and prune any diseased or damaged canes.\n",
    "4. **Fertilizing**: Apply balanced fertilizer monthly during growing season.\n",
    "5. **Disease Prevention**: Ensure good air circulation and avoid overhead watering to prevent fungal diseases.\n",
    "6. **Pest Management**: Monitor for aphids, spider mites, and thrips. Use integrated pest management approaches.\n",
    "7. **Heat Protection**: Consider shade cloth during extreme heat waves (above 90¬∞F).\n",
    "\n",
    "Regular monitoring and consistent care are key to healthy roses in summer heat.\"\"\"\n",
    ")\n",
    "\n",
    "print(\"=== Good Task Adherence Example ===\")\n",
    "print(f\"Query: What are the best practices for maintaining a healthy rose garden during the summer?\")\n",
    "print(f\"Response: [Detailed rose care instructions...]\")\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(f\"Score: {adherence_good['task_adherence']}\")\n",
    "print(f\"Result: {adherence_good['task_adherence_result']}\")\n",
    "print(f\"Reason: {adherence_good['task_adherence_reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6ceb82",
   "metadata": {},
   "source": [
    "### Example 2: Poor Task Adherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffeec18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of poor task adherence\n",
    "adherence_poor = task_adherence(\n",
    "    query=\"What are the best practices for maintaining a healthy rose garden during the summer?\",\n",
    "    response=\"Make sure to water your roses regularly and trim them occasionally.\"\n",
    ")\n",
    "\n",
    "print(\"=== Poor Task Adherence Example ===\")\n",
    "print(f\"Query: What are the best practices for maintaining a healthy rose garden during the summer?\")\n",
    "print(f\"Response: Make sure to water your roses regularly and trim them occasionally.\")\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(f\"Score: {adherence_poor['task_adherence']}\")\n",
    "print(f\"Result: {adherence_poor['task_adherence_result']}\")\n",
    "print(f\"Reason: {adherence_poor['task_adherence_reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8be978",
   "metadata": {},
   "source": [
    "## 7. Batch Evaluation of Agent Responses\n",
    "\n",
    "Let's evaluate multiple scenarios in batch to get a comprehensive view of agent performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229973bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define test scenarios\n",
    "test_scenarios = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"query\": \"What time does the library close on Sundays?\",\n",
    "        \"response\": \"The library closes at 6:00 PM on Sundays. Please note that hours may change during holidays.\",\n",
    "        \"category\": \"Information Request\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"query\": \"Can you help me book a flight to Tokyo?\",\n",
    "        \"response\": \"I can't directly book flights, but I can help you find flight options and guide you to booking websites. Would you like me to search for flights to Tokyo?\",\n",
    "        \"category\": \"Service Limitation\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"query\": \"How do I reset my password?\",\n",
    "        \"response\": \"The weather is nice today, isn't it?\",\n",
    "        \"category\": \"Irrelevant Response\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"query\": \"Explain quantum computing in simple terms\",\n",
    "        \"response\": \"Quantum computing uses quantum mechanics principles like superposition and entanglement to process information. Unlike classical bits that are either 0 or 1, quantum bits (qubits) can be in multiple states simultaneously, potentially allowing for much faster computation of certain problems.\",\n",
    "        \"category\": \"Educational Content\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Defined {len(test_scenarios)} test scenarios for batch evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f539b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch evaluation with Intent Resolution\n",
    "batch_results = []\n",
    "\n",
    "print(\"üîÑ Running batch evaluation with Intent Resolution Evaluator...\\n\")\n",
    "\n",
    "for i, scenario in enumerate(test_scenarios):\n",
    "    print(f\"Processing scenario {scenario['id']}/{len(test_scenarios)} ({scenario['category']})...\")\n",
    "    \n",
    "    try:\n",
    "        # Evaluate the scenario\n",
    "        result = intent_resolution(\n",
    "            query=scenario[\"query\"],\n",
    "            response=scenario[\"response\"]\n",
    "        )\n",
    "        \n",
    "        # Safely extract intent_resolved, defaulting to False if not available\n",
    "        intent_resolved = False\n",
    "        if 'additional_details' in result and 'intent_resolved' in result['additional_details']:\n",
    "            intent_resolved = result['additional_details']['intent_resolved']\n",
    "        elif result.get('intent_resolution_result') == 'pass':\n",
    "            intent_resolved = True  # Assume intent was resolved if evaluation passed\n",
    "            \n",
    "        batch_results.append({\n",
    "            \"id\": scenario[\"id\"],\n",
    "            \"category\": scenario[\"category\"],\n",
    "            \"query\": scenario[\"query\"],\n",
    "            \"response\": scenario[\"response\"],\n",
    "            \"intent_resolution_score\": result[\"intent_resolution\"],\n",
    "            \"intent_resolution_result\": result[\"intent_resolution_result\"],\n",
    "            \"intent_resolved\": intent_resolved\n",
    "        })\n",
    "        \n",
    "        print(f\"  ‚úÖ Score: {result['intent_resolution']} | Result: {result['intent_resolution_result']}\")\n",
    "        print(f\"  Intent Resolved: {intent_resolved}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ùå Error evaluating scenario {scenario['id']}: {str(e)}\")\n",
    "        # Add placeholder result for failed evaluation\n",
    "        batch_results.append({\n",
    "            \"id\": scenario[\"id\"],\n",
    "            \"category\": scenario[\"category\"],\n",
    "            \"query\": scenario[\"query\"],\n",
    "            \"response\": scenario[\"response\"],\n",
    "            \"intent_resolution_score\": None,\n",
    "            \"intent_resolution_result\": \"error\",\n",
    "            \"intent_resolved\": False\n",
    "        })\n",
    "    \n",
    "    print()\n",
    "\n",
    "successful_evaluations = len([r for r in batch_results if r[\"intent_resolution_score\"] is not None])\n",
    "print(f\"‚úÖ Batch evaluation completed: {successful_evaluations}/{len(test_scenarios)} successful evaluations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d10096",
   "metadata": {},
   "source": [
    "## 8. Performance Metrics Analysis\n",
    "\n",
    "Let's analyze the batch evaluation results to understand overall agent performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f50123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Convert results to DataFrame for analysis\n",
    "df_results = pd.DataFrame(batch_results)\n",
    "\n",
    "if not df_results.empty:\n",
    "    # Filter out failed evaluations for accurate statistics\n",
    "    df_successful = df_results[df_results['intent_resolution_score'].notna()]\n",
    "    \n",
    "    if not df_successful.empty:\n",
    "        # Calculate summary statistics\n",
    "        avg_score = df_successful['intent_resolution_score'].mean()\n",
    "        pass_rate = (df_successful['intent_resolution_result'] == 'pass').mean() * 100\n",
    "        intent_resolution_rate = df_successful['intent_resolved'].mean() * 100\n",
    "        success_rate = len(df_successful) / len(df_results) * 100\n",
    "        \n",
    "        print(\"=== Performance Metrics Summary ===\")\n",
    "        print(f\"Successful Evaluations: {len(df_successful)}/{len(df_results)} ({success_rate:.1f}%)\")\n",
    "        print(f\"Average Intent Resolution Score: {avg_score:.2f} / 5.0\")\n",
    "        print(f\"Pass Rate (Score >= 3): {pass_rate:.1f}%\")\n",
    "        print(f\"Intent Resolution Rate: {intent_resolution_rate:.1f}%\")\n",
    "        print()\n",
    "        \n",
    "        # Performance by category (successful evaluations only)\n",
    "        print(\"=== Performance by Category ===\")\n",
    "        category_stats = df_successful.groupby('category').agg({\n",
    "            'intent_resolution_score': ['mean', 'count'],\n",
    "            'intent_resolved': 'mean'\n",
    "        }).round(2)\n",
    "        \n",
    "        print(category_stats)\n",
    "        print()\n",
    "        \n",
    "        # Display detailed results\n",
    "        print(\"=== Detailed Results ===\")\n",
    "        for _, row in df_results.iterrows():\n",
    "            status = \"‚úÖ\" if row['intent_resolution_score'] is not None else \"‚ùå\"\n",
    "            print(f\"{status} Scenario {row['id']}: {row['category']}\")\n",
    "            print(f\"  Query: {row['query'][:60]}{'...' if len(row['query']) > 60 else ''}\")\n",
    "            if row['intent_resolution_score'] is not None:\n",
    "                print(f\"  Score: {row['intent_resolution_score']} | Result: {row['intent_resolution_result']}\")\n",
    "            else:\n",
    "                print(f\"  Status: {row['intent_resolution_result']} (evaluation failed)\")\n",
    "            print()\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ùå All evaluations failed.\")\n",
    "        print(\"üí° Check your Azure OpenAI deployment and authentication.\")\n",
    "else:\n",
    "    print(\"No results to analyze. Please run the batch evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e09a986",
   "metadata": {},
   "source": [
    "## 9. Working with Reasoning Models\n",
    "\n",
    "For complex evaluations, Azure AI Foundry supports reasoning models (o-series) which can provide more refined evaluation. Here's how to configure them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5716a73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example configuration for reasoning models (o3-mini, etc.)\n",
    "# Uncomment and modify if you have access to reasoning models\n",
    "\n",
    "# reasoning_model_config = AzureOpenAIModelConfiguration(\n",
    "#     azure_endpoint=os.environ[\"AZURE_ENDPOINT\"],\n",
    "#     api_key=os.environ[\"AZURE_API_KEY\"],\n",
    "#     azure_deployment=os.environ.get(\"AZURE_REASONING_DEPLOYMENT\", \"o3-mini\"),\n",
    "#     api_version=os.environ[\"AZURE_API_VERSION\"],\n",
    "# )\n",
    "\n",
    "# # Initialize evaluators with reasoning model\n",
    "# reasoning_intent_evaluator = IntentResolutionEvaluator(\n",
    "#     model_config=reasoning_model_config,\n",
    "#     is_reasoning_model=True,  # Important: Set this to True for reasoning models\n",
    "#     threshold=3\n",
    "# )\n",
    "\n",
    "print(\"üí° Reasoning model configuration example provided (commented out)\")\n",
    "print(\"   Set is_reasoning_model=True when using o-series models\")\n",
    "print(\"   Reasoning models are recommended for complex evaluations requiring refined reasoning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa83a799",
   "metadata": {},
   "source": [
    "## 10. Evaluation Results Visualization\n",
    "\n",
    "Create visualizations to better understand evaluation results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0522eeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "if not df_results.empty:\n",
    "    # Create subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('Agent Evaluation Results Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Score distribution\n",
    "    axes[0, 0].hist(df_results['intent_resolution_score'], bins=5, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Intent Resolution Score Distribution')\n",
    "    axes[0, 0].set_xlabel('Score')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Pass/Fail by category\n",
    "    category_counts = df_results.groupby(['category', 'intent_resolution_result']).size().unstack(fill_value=0)\n",
    "    category_counts.plot(kind='bar', ax=axes[0, 1], color=['lightcoral', 'lightgreen'])\n",
    "    axes[0, 1].set_title('Pass/Fail by Category')\n",
    "    axes[0, 1].set_xlabel('Category')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[0, 1].legend(['Fail', 'Pass'])\n",
    "    \n",
    "    # 3. Average scores by category\n",
    "    avg_scores = df_results.groupby('category')['intent_resolution_score'].mean()\n",
    "    bars = axes[1, 0].bar(avg_scores.index, avg_scores.values, color='lightblue', alpha=0.8)\n",
    "    axes[1, 0].set_title('Average Intent Resolution Score by Category')\n",
    "    axes[1, 0].set_xlabel('Category')\n",
    "    axes[1, 0].set_ylabel('Average Score')\n",
    "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "                       f'{height:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # 4. Intent resolution success rate\n",
    "    intent_success = df_results.groupby('category')['intent_resolved'].mean() * 100\n",
    "    bars2 = axes[1, 1].bar(intent_success.index, intent_success.values, color='lightgreen', alpha=0.8)\n",
    "    axes[1, 1].set_title('Intent Resolution Success Rate by Category')\n",
    "    axes[1, 1].set_xlabel('Category')\n",
    "    axes[1, 1].set_ylabel('Success Rate (%)')\n",
    "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add percentage labels\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + 1,\n",
    "                       f'{height:.0f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No data available for visualization. Please run the batch evaluation first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6a96d2",
   "metadata": {},
   "source": [
    "## Best Practices and Recommendations\n",
    "\n",
    "### Evaluator Selection Guide\n",
    "\n",
    "| Evaluator | Use Case | Score Range | Key Metrics |\n",
    "|-----------|----------|-------------|-------------|\n",
    "| **Intent Resolution** | Measure understanding of user requests | 1-5 | Intent detection, clarification, scope awareness |\n",
    "| **Tool Call Accuracy** | Evaluate function/tool usage efficiency | 1-5 | Tool relevance, parameter correctness, call efficiency |\n",
    "| **Task Adherence** | Assess adherence to assigned tasks | 1-5 | Task completion, scope adherence, efficiency |\n",
    "\n",
    "### Model Recommendations\n",
    "\n",
    "- **Reasoning Models** (o3-mini, o-series): Use for complex evaluations requiring refined reasoning\n",
    "- **Standard Models** (GPT-4o, GPT-4.1): Use for general evaluation tasks\n",
    "- Set `is_reasoning_model=True` when using o-series models\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "1. **Threshold Setting**: Adjust thresholds based on your quality requirements (default: 3)\n",
    "2. **Batch Processing**: Evaluate multiple scenarios for comprehensive assessment\n",
    "3. **Category Analysis**: Group similar use cases for targeted improvements\n",
    "4. **Continuous Monitoring**: Regularly evaluate agent performance in production\n",
    "\n",
    "### Integration with Azure AI Agent Service\n",
    "\n",
    "For agents built with Azure AI Agent Service, these evaluators provide native integration that directly takes agent messages. See the [end-to-end example](https://aka.ms/e2e-agent-eval-sample) for complete implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cee3038",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated how to use Azure AI Foundry's agent evaluators to assess AI agent performance:\n",
    "\n",
    "‚úÖ **Intent Resolution Evaluator**: Measures understanding of user requests  \n",
    "‚úÖ **Tool Call Accuracy Evaluator**: Evaluates function call efficiency and correctness  \n",
    "‚úÖ **Task Adherence Evaluator**: Assesses adherence to assigned tasks  \n",
    "‚úÖ **Batch Evaluation**: Process multiple scenarios for comprehensive assessment  \n",
    "‚úÖ **Performance Analysis**: Calculate metrics and identify improvement areas  \n",
    "‚úÖ **Visualization**: Create charts to understand evaluation results  \n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Customize Evaluators**: Adjust thresholds and parameters for your specific use case\n",
    "2. **Expand Test Scenarios**: Create more comprehensive test suites\n",
    "3. **Integrate with CI/CD**: Automate evaluation in your development pipeline\n",
    "4. **Monitor Production**: Set up continuous evaluation for deployed agents\n",
    "5. **Explore Additional Evaluators**: Try other quality and safety evaluators available in Azure AI Foundry\n",
    "\n",
    "For more information, visit the [Azure AI Foundry documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-evaluators/agent-evaluators)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-openai-workshop (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
