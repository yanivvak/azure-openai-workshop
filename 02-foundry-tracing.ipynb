{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17b3ab08",
   "metadata": {},
   "source": [
    "# Azure AI Foundry Application Tracing\n",
    "\n",
    "This notebook demonstrates how to implement tracing for AI applications using the OpenAI SDK with OpenTelemetry in Azure AI Foundry. Tracing provides deep visibility into execution of your application by capturing detailed telemetry at each execution step.\n",
    "\n",
    "## Prerequisites\n",
    "- An Azure AI Foundry project created\n",
    "- An AI application that uses OpenAI SDK to make calls to models hosted in Azure AI Foundry\n",
    "- Application Insights resource configured for your Azure AI Foundry project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb0339",
   "metadata": {},
   "source": [
    "## 1. Install Required Packages\n",
    "\n",
    "Install the Azure AI Foundry SDK and other required packages for application tracing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033c2b66",
   "metadata": {},
   "source": [
    "## 2. Import Libraries and Initialize Client\n",
    "\n",
    "Import necessary libraries and initialize the Azure AI Foundry client with proper authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc59e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from opentelemetry.instrumentation.openai_v2 import OpenAIInstrumentor\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fa5933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure your Azure AI Foundry project details\n",
    "# Replace these with your actual values\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "AZURE_AI_PROJECT_ENDPOINT = os.getenv(\"PROJECT_ENDPOINT\")\n",
    "# Alternative: You can also use environment variables\n",
    "# AZURE_AI_PROJECT_ENDPOINT = os.getenv(\"AZURE_AI_PROJECT_ENDPOINT\")\n",
    "deployment_name=os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "# Initialize the AI Project Client\n",
    "try:\n",
    "    project_client = AIProjectClient(\n",
    "        credential=DefaultAzureCredential(),\n",
    "        endpoint=AZURE_AI_PROJECT_ENDPOINT,\n",
    "    )\n",
    "    print(\"AI Project Client initialized successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error initializing client: {e}\")\n",
    "    print(\"Please ensure your Azure credentials are configured and the endpoint is correct.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a710db",
   "metadata": {},
   "source": [
    "## 3. Configure Tracing Settings\n",
    "\n",
    "Set up tracing configuration including project settings and trace collection parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc395ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Instrument the OpenAI SDK\n",
    "OpenAIInstrumentor().instrument()\n",
    "print(\"OpenAI SDK instrumented for tracing!\")\n",
    "\n",
    "# Step 2: Configure environment variable to capture inputs and outputs\n",
    "os.environ[\"OTEL_INSTRUMENTATION_GENAI_CAPTURE_MESSAGE_CONTENT\"] = \"true\"\n",
    "print(\"Message content capture enabled!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c676b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the connection string to Azure Application Insights\n",
    "try:\n",
    "    connection_string = project_client.telemetry.get_application_insights_connection_string()\n",
    "    print(f\"Connection string retrieved: {connection_string[:50]}...\")\n",
    "    \n",
    "    # Configure Azure Monitor for telemetry\n",
    "    configure_azure_monitor(connection_string=connection_string)\n",
    "    print(\"Azure Monitor configured successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error getting connection string: {e}\")\n",
    "    print(\"Make sure Application Insights is configured for your Azure AI Foundry project.\")\n",
    "    \n",
    "    # Fallback: Configure console tracing for testing\n",
    "    print(\"\\nConfiguring console tracing as fallback...\")\n",
    "    span_exporter = ConsoleSpanExporter()\n",
    "    tracer_provider = TracerProvider()\n",
    "    tracer_provider.add_span_processor(SimpleSpanProcessor(span_exporter))\n",
    "    trace.set_tracer_provider(tracer_provider)\n",
    "    print(\"Console tracing configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2f0a7a",
   "metadata": {},
   "source": [
    "## 4. Create a Simple Traced Function\n",
    "\n",
    "Implement a basic function with tracing decorators to demonstrate fundamental tracing concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba605b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the OpenAI client from the project\n",
    "try:\n",
    "    client = project_client.get_openai_client()\n",
    "    print(\"OpenAI client retrieved successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error getting OpenAI client: {e}\")\n",
    "    # For testing purposes, you might want to create a client directly\n",
    "    # from openai import OpenAI\n",
    "    # client = OpenAI(api_key=\"your-api-key\", base_url=\"your-base-url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a73e29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tracer instance\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "# Simple traced function\n",
    "@tracer.start_as_current_span(\"simple_chat_completion\")\n",
    "def simple_chat_completion(message: str, model: str = deployment_name):\n",
    "    \"\"\"\n",
    "    A simple function that makes a chat completion request with tracing.\n",
    "    \"\"\"\n",
    "    current_span = trace.get_current_span()\n",
    "    \n",
    "    # Add custom attributes to the span\n",
    "    current_span.set_attribute(\"user.message_length\", len(message))\n",
    "    current_span.set_attribute(\"user.model_requested\", model)\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": message}\n",
    "            ],\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        # Add response attributes\n",
    "        current_span.set_attribute(\"response.completion_tokens\", response.usage.completion_tokens)\n",
    "        current_span.set_attribute(\"response.prompt_tokens\", response.usage.prompt_tokens)\n",
    "        current_span.set_attribute(\"response.total_tokens\", response.usage.total_tokens)\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        current_span.record_exception(e)\n",
    "        current_span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n",
    "        raise\n",
    "\n",
    "print(\"Simple traced function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f87dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the simple traced function\n",
    "try:\n",
    "    result = simple_chat_completion(\"Write a short poem about tracing in AI applications.\")\n",
    "    print(\"Response received:\")\n",
    "    print(result)\n",
    "except Exception as e:\n",
    "    print(f\"Error in simple chat completion: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92be7bb0",
   "metadata": {},
   "source": [
    "## 5. Trace a Multi-Step Workflow\n",
    "\n",
    "Build a more complex workflow with multiple steps and nested function calls to show hierarchical tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b406915b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt_with_context(claim: str, context: str) -> list:\n",
    "    \"\"\"\n",
    "    Build a prompt for assessing claims based on provided context.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        {\n",
    "            'role': 'system', \n",
    "            'content': \"I will ask you to assess whether a particular scientific claim is supported by the evidence provided. Output only 'True' if the claim is true, 'False' if the claim is false, or 'NEE' if there's not enough evidence.\"\n",
    "        },\n",
    "        {\n",
    "            'role': 'user', \n",
    "            'content': f\"\"\"\n",
    "                The evidence is the following: {context}\n",
    "\n",
    "                Assess the following claim on the basis of the evidence. Output only 'True' if the claim is true, 'False' if the claim is false, or 'NEE' if there's not enough evidence. Do not output any other text.\n",
    "\n",
    "                Claim:\n",
    "                {claim}\n",
    "\n",
    "                Assessment:\n",
    "            \"\"\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "@tracer.start_as_current_span(\"assess_single_claim\")\n",
    "def assess_single_claim(claim: str, context: str, model: str = \"gpt-4\"):\n",
    "    \"\"\"\n",
    "    Assess a single claim against provided context.\n",
    "    \"\"\"\n",
    "    current_span = trace.get_current_span()\n",
    "    current_span.set_attribute(\"claim.length\", len(claim))\n",
    "    current_span.set_attribute(\"context.length\", len(context))\n",
    "    \n",
    "    messages = build_prompt_with_context(claim=claim, context=context)\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        max_tokens=10\n",
    "    )\n",
    "    \n",
    "    result = response.choices[0].message.content.strip('., ')\n",
    "    current_span.set_attribute(\"assessment.result\", result)\n",
    "    \n",
    "    return result\n",
    "\n",
    "@tracer.start_as_current_span(\"assess_claims_with_context\")\n",
    "def assess_claims_with_context(claims: list, contexts: list, model: str = deployment_name):\n",
    "    \"\"\"\n",
    "    Assess multiple claims against their respective contexts.\n",
    "    \"\"\"\n",
    "    current_span = trace.get_current_span()\n",
    "    current_span.set_attribute(\"operation.claims_count\", len(claims))\n",
    "    current_span.set_attribute(\"operation.model\", model)\n",
    "    \n",
    "    responses = []\n",
    "    \n",
    "    for i, (claim, context) in enumerate(zip(claims, contexts)):\n",
    "        # Create a child span for each individual assessment\n",
    "        with tracer.start_as_current_span(f\"assess_claim_{i+1}\"):\n",
    "            result = assess_single_claim(claim, context, model)\n",
    "            responses.append(result)\n",
    "    \n",
    "    current_span.set_attribute(\"operation.completed_assessments\", len(responses))\n",
    "    return responses\n",
    "\n",
    "print(\"Multi-step workflow functions created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf499f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the multi-step workflow\n",
    "sample_claims = [\n",
    "    \"Python is a programming language\",\n",
    "    \"The Earth is flat\",\n",
    "    \"Machine learning requires large datasets\"\n",
    "]\n",
    "\n",
    "sample_contexts = [\n",
    "    \"Python is a high-level, interpreted programming language with dynamic semantics. It was created by Guido van Rossum and first released in 1991.\",\n",
    "    \"The Earth is an oblate spheroid, meaning it's mostly spherical but slightly flattened at the poles due to its rotation. This has been confirmed by satellite imagery and space exploration.\",\n",
    "    \"While machine learning can benefit from large datasets, many techniques work effectively with smaller datasets. Transfer learning, few-shot learning, and data augmentation are examples of approaches that work with limited data.\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    assessments = assess_claims_with_context(sample_claims, sample_contexts)\n",
    "    print(\"Assessment Results:\")\n",
    "    for claim, assessment in zip(sample_claims, assessments):\n",
    "        print(f\"Claim: {claim[:50]}...\")\n",
    "        print(f\"Assessment: {assessment}\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"Error in multi-step workflow: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf22801",
   "metadata": {},
   "source": [
    "## 6. Add Custom Attributes and Events\n",
    "\n",
    "Enhance traces with custom attributes, tags, and events to provide richer debugging information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e297b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "@tracer.start_as_current_span(\"enhanced_chat_with_attributes\")\n",
    "def enhanced_chat_with_attributes(message: str, model: str = deployment_name, temperature: float = 0.7):\n",
    "    \"\"\"\n",
    "    Enhanced chat function with rich tracing attributes and events.\n",
    "    \"\"\"\n",
    "    current_span = trace.get_current_span()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Set initial attributes\n",
    "    current_span.set_attribute(\"user.message\", message[:100])  # Truncate for privacy\n",
    "    current_span.set_attribute(\"model.name\", model)\n",
    "    current_span.set_attribute(\"model.temperature\", temperature)\n",
    "    current_span.set_attribute(\"session.timestamp\", datetime.now().isoformat())\n",
    "    current_span.set_attribute(\"session.user_id\", \"demo_user\")\n",
    "    \n",
    "    # Add an event for the start of processing\n",
    "    current_span.add_event(\n",
    "        \"processing_started\",\n",
    "        {\n",
    "            \"message_length\": len(message),\n",
    "            \"processing_timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Add event before API call\n",
    "        current_span.add_event(\"api_call_initiated\")\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": message}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=300\n",
    "        )\n",
    "        \n",
    "        # Calculate processing time\n",
    "        processing_time = time.time() - start_time\n",
    "        \n",
    "        # Add success attributes\n",
    "        current_span.set_attribute(\"response.success\", True)\n",
    "        current_span.set_attribute(\"response.processing_time_seconds\", processing_time)\n",
    "        current_span.set_attribute(\"response.completion_tokens\", response.usage.completion_tokens)\n",
    "        current_span.set_attribute(\"response.prompt_tokens\", response.usage.prompt_tokens)\n",
    "        current_span.set_attribute(\"response.total_tokens\", response.usage.total_tokens)\n",
    "        current_span.set_attribute(\"response.model_used\", response.model)\n",
    "        \n",
    "        # Add success event\n",
    "        current_span.add_event(\n",
    "            \"response_received\",\n",
    "            {\n",
    "                \"response_length\": len(response.choices[0].message.content),\n",
    "                \"finish_reason\": response.choices[0].finish_reason,\n",
    "                \"processing_time\": processing_time\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Add performance warning if response took too long\n",
    "        if processing_time > 5.0:\n",
    "            current_span.add_event(\n",
    "                \"performance_warning\",\n",
    "                {\"message\": \"Response time exceeded 5 seconds\", \"actual_time\": processing_time}\n",
    "            )\n",
    "        \n",
    "        return {\n",
    "            \"content\": response.choices[0].message.content,\n",
    "            \"processing_time\": processing_time,\n",
    "            \"token_usage\": {\n",
    "                \"prompt_tokens\": response.usage.prompt_tokens,\n",
    "                \"completion_tokens\": response.usage.completion_tokens,\n",
    "                \"total_tokens\": response.usage.total_tokens\n",
    "            }\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Add error attributes and events\n",
    "        current_span.set_attribute(\"response.success\", False)\n",
    "        current_span.set_attribute(\"error.type\", type(e).__name__)\n",
    "        current_span.set_attribute(\"error.message\", str(e))\n",
    "        \n",
    "        current_span.add_event(\n",
    "            \"error_occurred\",\n",
    "            {\n",
    "                \"error_type\": type(e).__name__,\n",
    "                \"error_message\": str(e),\n",
    "                \"processing_time_at_error\": time.time() - start_time\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Record the exception in the span\n",
    "        current_span.record_exception(e)\n",
    "        current_span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n",
    "        \n",
    "        raise\n",
    "\n",
    "print(\"Enhanced function with custom attributes and events created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e45e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the enhanced function\n",
    "try:\n",
    "    result = enhanced_chat_with_attributes(\n",
    "        \"Explain the benefits of application tracing in AI systems.\",\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(\"Enhanced Response:\")\n",
    "    print(result[\"content\"])\n",
    "    print(f\"\\nProcessing time: {result['processing_time']:.2f} seconds\")\n",
    "    print(f\"Token usage: {result['token_usage']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in enhanced chat: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffae83dd",
   "metadata": {},
   "source": [
    "## 7. Trace LLM Calls\n",
    "\n",
    "Implement tracing for Large Language Model calls, including input/output logging and performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709ac92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.start_as_current_span(\"llm_conversation_chain\")\n",
    "def llm_conversation_chain(initial_prompt: str, follow_up_questions: list, model: str = deployment_name):\n",
    "    \"\"\"\n",
    "    Simulate a conversation chain with multiple LLM calls and comprehensive tracing.\n",
    "    \"\"\"\n",
    "    current_span = trace.get_current_span()\n",
    "    current_span.set_attribute(\"conversation.initial_prompt\", initial_prompt[:200])\n",
    "    current_span.set_attribute(\"conversation.follow_up_count\", len(follow_up_questions))\n",
    "    current_span.set_attribute(\"conversation.model\", model)\n",
    "    \n",
    "    conversation_history = []\n",
    "    total_tokens = 0\n",
    "    \n",
    "    # Initial call\n",
    "    with tracer.start_as_current_span(\"initial_llm_call\") as initial_span:\n",
    "        initial_span.set_attribute(\"call.type\", \"initial\")\n",
    "        initial_span.set_attribute(\"call.prompt\", initial_prompt[:200])\n",
    "        \n",
    "        messages = [{\"role\": \"user\", \"content\": initial_prompt}]\n",
    "        \n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_tokens=200\n",
    "        )\n",
    "        \n",
    "        initial_response = response.choices[0].message.content\n",
    "        conversation_history.append({\"role\": \"user\", \"content\": initial_prompt})\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": initial_response})\n",
    "        \n",
    "        initial_tokens = response.usage.total_tokens\n",
    "        total_tokens += initial_tokens\n",
    "        \n",
    "        initial_span.set_attribute(\"call.tokens_used\", initial_tokens)\n",
    "        initial_span.set_attribute(\"call.response_length\", len(initial_response))\n",
    "    \n",
    "    # Follow-up calls\n",
    "    for i, follow_up in enumerate(follow_up_questions):\n",
    "        with tracer.start_as_current_span(f\"follow_up_call_{i+1}\") as follow_up_span:\n",
    "            follow_up_span.set_attribute(\"call.type\", \"follow_up\")\n",
    "            follow_up_span.set_attribute(\"call.index\", i + 1)\n",
    "            follow_up_span.set_attribute(\"call.question\", follow_up[:200])\n",
    "            follow_up_span.set_attribute(\"call.conversation_length\", len(conversation_history))\n",
    "            \n",
    "            # Add the follow-up question to conversation\n",
    "            conversation_history.append({\"role\": \"user\", \"content\": follow_up})\n",
    "            \n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=conversation_history,\n",
    "                max_tokens=200\n",
    "            )\n",
    "            \n",
    "            follow_up_response = response.choices[0].message.content\n",
    "            conversation_history.append({\"role\": \"assistant\", \"content\": follow_up_response})\n",
    "            \n",
    "            follow_up_tokens = response.usage.total_tokens\n",
    "            total_tokens += follow_up_tokens\n",
    "            \n",
    "            follow_up_span.set_attribute(\"call.tokens_used\", follow_up_tokens)\n",
    "            follow_up_span.set_attribute(\"call.response_length\", len(follow_up_response))\n",
    "            follow_up_span.set_attribute(\"call.cumulative_tokens\", total_tokens)\n",
    "    \n",
    "    # Final span attributes\n",
    "    current_span.set_attribute(\"conversation.total_tokens\", total_tokens)\n",
    "    current_span.set_attribute(\"conversation.total_exchanges\", len(conversation_history) // 2)\n",
    "    current_span.set_attribute(\"conversation.final_length\", len(conversation_history))\n",
    "    \n",
    "    # Add final event\n",
    "    current_span.add_event(\n",
    "        \"conversation_completed\",\n",
    "        {\n",
    "            \"total_exchanges\": len(conversation_history) // 2,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"conversation_length\": len(conversation_history)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"conversation\": conversation_history,\n",
    "        \"total_tokens\": total_tokens,\n",
    "        \"exchanges\": len(conversation_history) // 2\n",
    "    }\n",
    "\n",
    "print(\"LLM conversation chain function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ac71d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the LLM conversation chain\n",
    "try:\n",
    "    result = llm_conversation_chain(\n",
    "        initial_prompt=\"What is machine learning?\",\n",
    "        follow_up_questions=[\n",
    "            \"What are the main types of machine learning?\",\n",
    "            \"Give me an example of supervised learning.\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    print(\"Conversation Chain Results:\")\n",
    "    print(f\"Total exchanges: {result['exchanges']}\")\n",
    "    print(f\"Total tokens used: {result['total_tokens']}\")\n",
    "    print(\"\\nConversation:\")\n",
    "    \n",
    "    for i, message in enumerate(result['conversation']):\n",
    "        role = message['role'].capitalize()\n",
    "        content = message['content'][:100] + \"...\" if len(message['content']) > 100 else message['content']\n",
    "        print(f\"{i+1}. {role}: {content}\\n\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error in conversation chain: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb69f465",
   "metadata": {},
   "source": [
    "## 8. View and Analyze Traces\n",
    "\n",
    "Access and examine collected traces through the Azure AI Foundry portal and programmatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f02241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about viewing traces\n",
    "print(\"\"\"Viewing and Analyzing Traces:\n",
    "\n",
    "1. Azure AI Foundry Portal:\n",
    "   - Go to https://ai.azure.com\n",
    "   - Navigate to your project\n",
    "   - Click on \"Tracing\" in the side navigation\n",
    "   - View traces in real-time or filter by time range\n",
    "\n",
    "2. Application Insights:\n",
    "   - Access your Application Insights resource in Azure Portal\n",
    "   - Use \"Transaction search\" to find specific traces\n",
    "   - Use \"Application map\" to see component interactions\n",
    "   - Create custom queries using KQL (Kusto Query Language)\n",
    "\n",
    "3. Programmatic Access:\n",
    "   - Use Azure Monitor Query API\n",
    "   - Export traces for analysis\n",
    "   - Create dashboards and alerts\n",
    "\n",
    "Key Trace Information Available:\n",
    "- Execution time and performance metrics\n",
    "- Input/output data (when enabled)\n",
    "- Error details and stack traces\n",
    "- Custom attributes and events\n",
    "- Token usage and costs\n",
    "- Request/response patterns\n",
    "\"\"\")\n",
    "\n",
    "# Display current project endpoint for reference\n",
    "if 'AZURE_AI_PROJECT_ENDPOINT' in locals():\n",
    "    print(f\"\\nYour project endpoint: {AZURE_AI_PROJECT_ENDPOINT}\")\n",
    "    print(\"Visit the tracing section in Azure AI Foundry to see your traces!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f345005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of a function that helps with trace analysis\n",
    "@tracer.start_as_current_span(\"trace_analysis_example\")\n",
    "def analyze_model_performance(test_prompts: list, model: str = deployment_name):\n",
    "    \"\"\"\n",
    "    Analyze model performance across multiple prompts with detailed tracing.\n",
    "    \"\"\"\n",
    "    current_span = trace.get_current_span()\n",
    "    current_span.set_attribute(\"analysis.prompt_count\", len(test_prompts))\n",
    "    current_span.set_attribute(\"analysis.model\", model)\n",
    "    \n",
    "    results = []\n",
    "    total_time = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts):\n",
    "        with tracer.start_as_current_span(f\"test_prompt_{i+1}\") as prompt_span:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            prompt_span.set_attribute(\"prompt.index\", i + 1)\n",
    "            prompt_span.set_attribute(\"prompt.content\", prompt[:100])\n",
    "            prompt_span.set_attribute(\"prompt.length\", len(prompt))\n",
    "            \n",
    "            try:\n",
    "                response = client.chat.completions.create(\n",
    "                    model=model,\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    max_tokens=150\n",
    "                )\n",
    "                \n",
    "                elapsed_time = time.time() - start_time\n",
    "                tokens_used = response.usage.total_tokens\n",
    "                \n",
    "                total_time += elapsed_time\n",
    "                total_tokens += tokens_used\n",
    "                \n",
    "                prompt_span.set_attribute(\"response.time_seconds\", elapsed_time)\n",
    "                prompt_span.set_attribute(\"response.tokens\", tokens_used)\n",
    "                prompt_span.set_attribute(\"response.success\", True)\n",
    "                \n",
    "                results.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"response\": response.choices[0].message.content,\n",
    "                    \"time\": elapsed_time,\n",
    "                    \"tokens\": tokens_used,\n",
    "                    \"success\": True\n",
    "                })\n",
    "                \n",
    "            except Exception as e:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                \n",
    "                prompt_span.set_attribute(\"response.time_seconds\", elapsed_time)\n",
    "                prompt_span.set_attribute(\"response.success\", False)\n",
    "                prompt_span.set_attribute(\"error.type\", type(e).__name__)\n",
    "                prompt_span.record_exception(e)\n",
    "                \n",
    "                results.append({\n",
    "                    \"prompt\": prompt,\n",
    "                    \"error\": str(e),\n",
    "                    \"time\": elapsed_time,\n",
    "                    \"success\": False\n",
    "                })\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    successful_calls = [r for r in results if r[\"success\"]]\n",
    "    avg_time = total_time / len(test_prompts) if test_prompts else 0\n",
    "    success_rate = len(successful_calls) / len(test_prompts) if test_prompts else 0\n",
    "    \n",
    "    current_span.set_attribute(\"analysis.total_time\", total_time)\n",
    "    current_span.set_attribute(\"analysis.average_time\", avg_time)\n",
    "    current_span.set_attribute(\"analysis.total_tokens\", total_tokens)\n",
    "    current_span.set_attribute(\"analysis.success_rate\", success_rate)\n",
    "    current_span.set_attribute(\"analysis.successful_calls\", len(successful_calls))\n",
    "    \n",
    "    current_span.add_event(\n",
    "        \"analysis_completed\",\n",
    "        {\n",
    "            \"total_prompts\": len(test_prompts),\n",
    "            \"successful_calls\": len(successful_calls),\n",
    "            \"success_rate\": success_rate,\n",
    "            \"total_time\": total_time,\n",
    "            \"total_tokens\": total_tokens\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"results\": results,\n",
    "        \"summary\": {\n",
    "            \"total_prompts\": len(test_prompts),\n",
    "            \"successful_calls\": len(successful_calls),\n",
    "            \"success_rate\": success_rate,\n",
    "            \"total_time\": total_time,\n",
    "            \"average_time\": avg_time,\n",
    "            \"total_tokens\": total_tokens\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(\"Model performance analysis function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4dc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the performance analysis\n",
    "test_prompts = [\n",
    "    \"What is Python?\",\n",
    "    \"Explain machine learning in one sentence.\",\n",
    "    \"What are the benefits of cloud computing?\",\n",
    "    \"How does artificial intelligence work?\"\n",
    "]\n",
    "\n",
    "try:\n",
    "    analysis_result = analyze_model_performance(test_prompts)\n",
    "    \n",
    "    print(\"Performance Analysis Results:\")\n",
    "    print(f\"Total prompts: {analysis_result['summary']['total_prompts']}\")\n",
    "    print(f\"Successful calls: {analysis_result['summary']['successful_calls']}\")\n",
    "    print(f\"Success rate: {analysis_result['summary']['success_rate']:.2%}\")\n",
    "    print(f\"Total time: {analysis_result['summary']['total_time']:.2f} seconds\")\n",
    "    print(f\"Average time per call: {analysis_result['summary']['average_time']:.2f} seconds\")\n",
    "    print(f\"Total tokens used: {analysis_result['summary']['total_tokens']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in performance analysis: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478995bf",
   "metadata": {},
   "source": [
    "## 9. Error Handling and Debugging with Traces\n",
    "\n",
    "Demonstrate how to use traces for debugging errors and monitoring application performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727b934d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tracer.start_as_current_span(\"robust_ai_function\")\n",
    "def robust_ai_function(user_input: str, max_retries: int = 3, timeout: float = 30.0):\n",
    "    \"\"\"\n",
    "    A robust AI function with comprehensive error handling and tracing.\n",
    "    \"\"\"\n",
    "    current_span = trace.get_current_span()\n",
    "    current_span.set_attribute(\"function.max_retries\", max_retries)\n",
    "    current_span.set_attribute(\"function.timeout\", timeout)\n",
    "    current_span.set_attribute(\"function.input_length\", len(user_input))\n",
    "    \n",
    "    # Input validation\n",
    "    if not user_input or len(user_input.strip()) == 0:\n",
    "        error_msg = \"Empty input provided\"\n",
    "        current_span.set_attribute(\"error.type\", \"ValidationError\")\n",
    "        current_span.set_attribute(\"error.message\", error_msg)\n",
    "        current_span.add_event(\"validation_failed\", {\"reason\": \"empty_input\"})\n",
    "        current_span.set_status(trace.Status(trace.StatusCode.ERROR, error_msg))\n",
    "        raise ValueError(error_msg)\n",
    "    \n",
    "    if len(user_input) > 4000:\n",
    "        error_msg = \"Input too long (max 4000 characters)\"\n",
    "        current_span.set_attribute(\"error.type\", \"ValidationError\")\n",
    "        current_span.set_attribute(\"error.message\", error_msg)\n",
    "        current_span.add_event(\"validation_failed\", {\"reason\": \"input_too_long\", \"length\": len(user_input)})\n",
    "        current_span.set_status(trace.Status(trace.StatusCode.ERROR, error_msg))\n",
    "        raise ValueError(error_msg)\n",
    "    \n",
    "    current_span.add_event(\"validation_passed\")\n",
    "    \n",
    "    # Retry logic with tracing\n",
    "    for attempt in range(max_retries):\n",
    "        with tracer.start_as_current_span(f\"attempt_{attempt + 1}\") as attempt_span:\n",
    "            attempt_span.set_attribute(\"retry.attempt_number\", attempt + 1)\n",
    "            attempt_span.set_attribute(\"retry.max_attempts\", max_retries)\n",
    "            \n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                # Simulate potential issues for demonstration\n",
    "                if attempt == 0 and \"error\" in user_input.lower():\n",
    "                    # Simulate a transient error on first attempt\n",
    "                    attempt_span.add_event(\"simulated_transient_error\")\n",
    "                    raise Exception(\"Simulated transient network error\")\n",
    "                \n",
    "                response = client.chat.completions.create(\n",
    "                    model=\"gpt-4\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": user_input}],\n",
    "                    max_tokens=300,\n",
    "                    timeout=timeout\n",
    "                )\n",
    "                \n",
    "                elapsed_time = time.time() - start_time\n",
    "                \n",
    "                # Success - add attributes and return\n",
    "                attempt_span.set_attribute(\"attempt.success\", True)\n",
    "                attempt_span.set_attribute(\"attempt.response_time\", elapsed_time)\n",
    "                attempt_span.set_attribute(\"attempt.tokens_used\", response.usage.total_tokens)\n",
    "                \n",
    "                current_span.set_attribute(\"function.successful_attempt\", attempt + 1)\n",
    "                current_span.set_attribute(\"function.total_time\", elapsed_time)\n",
    "                current_span.set_attribute(\"function.success\", True)\n",
    "                \n",
    "                current_span.add_event(\n",
    "                    \"function_completed_successfully\",\n",
    "                    {\n",
    "                        \"attempt\": attempt + 1,\n",
    "                        \"response_time\": elapsed_time,\n",
    "                        \"tokens_used\": response.usage.total_tokens\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                return {\n",
    "                    \"content\": response.choices[0].message.content,\n",
    "                    \"attempt\": attempt + 1,\n",
    "                    \"response_time\": elapsed_time,\n",
    "                    \"tokens_used\": response.usage.total_tokens\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                elapsed_time = time.time() - start_time\n",
    "                \n",
    "                attempt_span.set_attribute(\"attempt.success\", False)\n",
    "                attempt_span.set_attribute(\"attempt.error_type\", type(e).__name__)\n",
    "                attempt_span.set_attribute(\"attempt.error_message\", str(e))\n",
    "                attempt_span.set_attribute(\"attempt.time_to_failure\", elapsed_time)\n",
    "                \n",
    "                attempt_span.add_event(\n",
    "                    \"attempt_failed\",\n",
    "                    {\n",
    "                        \"error_type\": type(e).__name__,\n",
    "                        \"error_message\": str(e),\n",
    "                        \"time_to_failure\": elapsed_time\n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                attempt_span.record_exception(e)\n",
    "                \n",
    "                if attempt == max_retries - 1:\n",
    "                    # Final attempt failed\n",
    "                    current_span.set_attribute(\"function.success\", False)\n",
    "                    current_span.set_attribute(\"function.final_error_type\", type(e).__name__)\n",
    "                    current_span.set_attribute(\"function.final_error_message\", str(e))\n",
    "                    current_span.set_attribute(\"function.attempts_exhausted\", True)\n",
    "                    \n",
    "                    current_span.add_event(\n",
    "                        \"all_attempts_failed\",\n",
    "                        {\n",
    "                            \"total_attempts\": max_retries,\n",
    "                            \"final_error\": str(e)\n",
    "                        }\n",
    "                    )\n",
    "                    \n",
    "                    current_span.record_exception(e)\n",
    "                    current_span.set_status(trace.Status(trace.StatusCode.ERROR, f\"Failed after {max_retries} attempts: {str(e)}\"))\n",
    "                    raise\n",
    "                else:\n",
    "                    # Wait before retry\n",
    "                    wait_time = 2 ** attempt  # Exponential backoff\n",
    "                    attempt_span.add_event(\"waiting_before_retry\", {\"wait_seconds\": wait_time})\n",
    "                    time.sleep(wait_time)\n",
    "\n",
    "print(\"Robust AI function with error handling and tracing created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569fef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test successful case\n",
    "print(\"Testing successful case:\")\n",
    "try:\n",
    "    result = robust_ai_function(\"What are the main benefits of distributed tracing?\")\n",
    "    print(f\"Success! Attempt: {result['attempt']}, Time: {result['response_time']:.2f}s\")\n",
    "    print(f\"Response: {result['content'][:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test retry scenario (simulated)\n",
    "print(\"Testing retry scenario (contains 'error' to trigger simulation):\")\n",
    "try:\n",
    "    result = robust_ai_function(\"Explain how error handling works in distributed systems.\")\n",
    "    print(f\"Success after retries! Attempt: {result['attempt']}, Time: {result['response_time']:.2f}s\")\n",
    "    print(f\"Response: {result['content'][:100]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed after all retries: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Test validation error\n",
    "print(\"Testing validation error:\")\n",
    "try:\n",
    "    result = robust_ai_function(\"\")  # Empty input\n",
    "    print(f\"Unexpected success: {result}\")\n",
    "except ValueError as e:\n",
    "    print(f\"Expected validation error: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea67dca1",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "This notebook demonstrated comprehensive tracing capabilities for AI applications using Azure AI Foundry. Here are the key takeaways:\n",
    "\n",
    "### Key Features Demonstrated:\n",
    "1. **Basic Tracing Setup** - Instrumentation and configuration\n",
    "2. **Custom Spans** - Creating meaningful trace boundaries\n",
    "3. **Attributes and Events** - Adding rich metadata to traces\n",
    "4. **Error Handling** - Capturing and tracing errors effectively\n",
    "5. **Performance Monitoring** - Tracking timing and resource usage\n",
    "6. **Multi-step Workflows** - Tracing complex application flows\n",
    "\n",
    "### Best Practices:\n",
    "- Always instrument your OpenAI SDK calls\n",
    "- Use meaningful span names that describe the operation\n",
    "- Add relevant attributes for debugging and analysis\n",
    "- Include error handling with proper exception recording\n",
    "- Use events to mark important milestones\n",
    "- Consider privacy when logging user inputs\n",
    "- Monitor performance metrics like token usage and response times\n",
    "\n",
    "### Next Steps:\n",
    "- Set up dashboards in Application Insights\n",
    "- Create alerts for performance thresholds\n",
    "- Implement trace sampling for high-volume applications\n",
    "- Use trace data for performance optimization\n",
    "- Integrate with CI/CD pipelines for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccbfef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up and final notes\n",
    "print(\"Notebook completed successfully!\")\n",
    "print(\"\"\"\\nTo view your traces:\n",
    "1. Go to Azure AI Foundry portal (https://ai.azure.com)\n",
    "2. Navigate to your project\n",
    "3. Click on \"Tracing\" in the navigation menu\n",
    "4. Explore the traces generated by this notebook\n",
    "\n",
    "For production use:\n",
    "- Configure appropriate sampling rates\n",
    "- Set up proper authentication\n",
    "- Monitor trace volume and costs\n",
    "- Create custom dashboards and alerts\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-openai-workshop (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
