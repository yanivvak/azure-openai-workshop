{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get configuration settings \n",
    "load_dotenv()\n",
    "azure_oai_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "azure_oai_key = os.getenv(\"AZURE_OPENAI_KEY\")\n",
    "azure_oai_model = os.getenv(\"AZURE_OPENAI_MODEL\")\n",
    "api_version = os.getenv(\"API_VERSION\")\n",
    "# Read text from file\n",
    "text = open(file=\"./Data/sample-text.txt\", encoding=\"utf8\").read()\n",
    "#print(text)\n",
    "\n",
    "# Initialize the Azure OpenAI client\n",
    "client1 = AzureOpenAI(\n",
    "        azure_endpoint = azure_oai_endpoint, \n",
    "        api_key=azure_oai_key,  \n",
    "        api_version=api_version\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0325818061828613\n",
      "2.283216953277588\n",
      "0.7178621292114258\n",
      "1.7439689636230469\n",
      "0.8159308433532715\n",
      "2.1498610973358154\n",
      "0.9185888767242432\n",
      "5.0157630443573\n",
      "0.9244880676269531\n",
      "0.6138207912445068\n",
      "Average latency for Deployment A: 1621.61 ms\n"
     ]
    }
   ],
   "source": [
    "# Send request to Azure OpenAI model\n",
    "import time\n",
    "\n",
    "num_requests = 10\n",
    "\n",
    "def measure_latency(client):\n",
    "    latencies = []\n",
    "    for _ in range(num_requests):\n",
    "        start_time = time.time()\n",
    "        response = client.chat.completions.create(\n",
    "        model=azure_oai_model,\n",
    "        temperature=0.2,\n",
    "        max_tokens=120,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Summarize the following text in 20 words or less:\\n\" + text}\n",
    "        ]\n",
    "    )\n",
    "        end_time = time.time()\n",
    "        print(end_time - start_time)\n",
    "        latencies.append(end_time - start_time)\n",
    "    average_latency = sum(latencies) / num_requests\n",
    "\n",
    "    return average_latency\n",
    "\n",
    "latency = measure_latency(client1)\n",
    "\n",
    "\n",
    "print(f\"Average latency for Deployment A: {latency * 1000:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3115320205688477\n",
      "1.3618879318237305\n",
      "0.62801194190979\n",
      "2.9135918617248535\n",
      "2.560758113861084\n",
      "0.7163572311401367\n",
      "0.808535099029541\n",
      "4.293958902359009\n",
      "0.7054388523101807\n",
      "0.8038730621337891\n",
      "Average latency for Deployment A: 1610.39 ms\n"
     ]
    }
   ],
   "source": [
    "client2 = AzureOpenAI(\n",
    "        azure_endpoint = \"https://scdemo.openai.azure.com/\", \n",
    "        api_key=\"add your key here\",   #add your key here\n",
    "        api_version=api_version\n",
    "        )\n",
    "latency = measure_latency(client2)\n",
    "\n",
    "\n",
    "print(f\"Average latency for Deployment B: {latency * 1000:.2f} ms\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
