{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60b56d01",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "415ef3aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model: gpt-4.1-mini\n",
      "‚úÖ Auth: DefaultAzureCredential (Entra ID)\n",
      "‚úÖ Project endpoint configured\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Get configuration\n",
    "endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')\n",
    "api_version = os.getenv('AZURE_OPENAI_API_VERSION', '2024-10-21')\n",
    "deployment_name = os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')\n",
    "api_key = os.getenv('AZURE_OPENAI_API_KEY')\n",
    "\n",
    "# Configure Azure OpenAI for evaluators\n",
    "# Uses API key if available, otherwise DefaultAzureCredential (Entra ID)\n",
    "if api_key:\n",
    "    model_config = AzureOpenAIModelConfiguration(\n",
    "        azure_endpoint=endpoint,\n",
    "        azure_deployment=deployment_name,\n",
    "        api_version=api_version,\n",
    "        api_key=api_key\n",
    "    )\n",
    "    auth_method = \"API Key\"\n",
    "else:\n",
    "    credential = DefaultAzureCredential()\n",
    "    token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
    "    model_config = AzureOpenAIModelConfiguration(\n",
    "        azure_endpoint=endpoint,\n",
    "        azure_deployment=deployment_name,\n",
    "        api_version=api_version,\n",
    "        api_key=token\n",
    "    )\n",
    "    auth_method = \"DefaultAzureCredential (Entra ID)\"\n",
    "\n",
    "print(f\"‚úÖ Model: {deployment_name}\")\n",
    "print(f\"‚úÖ Auth: {auth_method}\")\n",
    "\n",
    "# Optional: Project endpoint for safety evaluators and logging\n",
    "azure_ai_project = os.getenv('PROJECT_ENDPOINT')\n",
    "\n",
    "if azure_ai_project:\n",
    "    print(f\"‚úÖ Project endpoint configured\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è PROJECT_ENDPOINT not set - safety evaluators unavailable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b32e93",
   "metadata": {},
   "source": [
    "## 2. Quality Evaluators\n",
    "\n",
    "Built-in evaluators come in two types:\n",
    "- **AI-assisted**: Use an LLM to score (require `model_config`)\n",
    "- **NLP-based**: Mathematical metrics (no model needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c633299c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Quality evaluators initialized\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import (\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    GroundednessEvaluator,\n",
    "    F1ScoreEvaluator\n",
    ")\n",
    "\n",
    "# Initialize evaluators\n",
    "relevance = RelevanceEvaluator(model_config)      # AI-assisted\n",
    "coherence = CoherenceEvaluator(model_config)      # AI-assisted  \n",
    "fluency = FluencyEvaluator(model_config)          # AI-assisted\n",
    "groundedness = GroundednessEvaluator(model_config) # AI-assisted\n",
    "f1_score = F1ScoreEvaluator()                     # NLP-based (no config needed)\n",
    "\n",
    "print(\"‚úÖ Quality evaluators initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "153d222e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Quality Evaluation Results\n",
      "\n",
      "Relevance: 5.0/5\n",
      "Coherence: 4.0/5\n",
      "Fluency: 3.0/5\n",
      "Groundedness: 5.0/5\n",
      "F1 Score: 0.333\n"
     ]
    }
   ],
   "source": [
    "# Test data\n",
    "query = \"What is the capital of France?\"\n",
    "response = \"Paris is the capital of France.\"\n",
    "context = \"Paris has been the capital of France since the 10th century.\"\n",
    "ground_truth = \"Paris\"\n",
    "\n",
    "# Run evaluations\n",
    "print(\"üìä Quality Evaluation Results\\n\")\n",
    "\n",
    "# Relevance: Is the response relevant to the query?\n",
    "rel_result = relevance(query=query, response=response)\n",
    "print(f\"Relevance: {rel_result.get('relevance', 'N/A')}/5\")\n",
    "\n",
    "# Coherence: Is the response well-organized and logical?\n",
    "coh_result = coherence(query=query, response=response)\n",
    "print(f\"Coherence: {coh_result.get('coherence', 'N/A')}/5\")\n",
    "\n",
    "# Fluency: Is the response grammatically correct?\n",
    "flu_result = fluency(query=query, response=response)\n",
    "print(f\"Fluency: {flu_result.get('fluency', 'N/A')}/5\")\n",
    "\n",
    "# Groundedness: Is the response grounded in the context?\n",
    "grd_result = groundedness(query=query, response=response, context=context)\n",
    "print(f\"Groundedness: {grd_result.get('groundedness', 'N/A')}/5\")\n",
    "\n",
    "# F1 Score: Token overlap with ground truth\n",
    "f1_result = f1_score(response=response, ground_truth=ground_truth)\n",
    "print(f\"F1 Score: {f1_result.get('f1_score', 'N/A'):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f188a212",
   "metadata": {},
   "source": [
    "## 3. Translation Evaluation\n",
    "\n",
    "Evaluate translation quality using multiple metrics:\n",
    "\n",
    "- **BLEU** (Bilingual Evaluation Understudy): Measures n-gram overlap between the translation and reference.\n",
    "  Higher scores indicate more word/phrase matches with the reference translation.\n",
    "  **Range: 0-1** (0 = no overlap, 1 = perfect match)\n",
    "\n",
    "- **METEOR** (Metric for Evaluation of Translation with Explicit ORdering): Goes beyond exact matches by considering synonyms, stemming, and paraphrases.\n",
    "  More linguistically aware than BLEU, correlating better with human judgment.\n",
    "  **Range: 0-1** (0 = no similarity, 1 = perfect translation)\n",
    "\n",
    "- **Similarity**: Uses an LLM to evaluate semantic equivalence between translation and reference.\n",
    "  Captures meaning preservation even when wording differs significantly.\n",
    "  **Range: 1-5** (1 = completely different meaning, 5 = semantically identical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7366254d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Translation evaluators initialized\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import (\n",
    "    BleuScoreEvaluator,\n",
    "    MeteorScoreEvaluator,\n",
    "    SimilarityEvaluator\n",
    ")\n",
    "\n",
    "# Initialize translation evaluators\n",
    "bleu = BleuScoreEvaluator()      # NLP-based\n",
    "meteor = MeteorScoreEvaluator()  # NLP-based\n",
    "similarity = SimilarityEvaluator(model_config)  # AI-assisted\n",
    "\n",
    "print(\"‚úÖ Translation evaluators initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96b1a3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üåê Translation Quality Evaluation\n",
      "\n",
      "Lang            BLEU       METEOR     Similarity\n",
      "---------------------------------------------\n",
      "EN‚ÜíFR           0.322      0.750      4.0/5\n",
      "EN‚ÜíES           0.207      0.615      5.0/5\n",
      "EN‚ÜíFR (exact)   0.576      0.981      5.0/5\n"
     ]
    }
   ],
   "source": [
    "# Translation examples\n",
    "translations = [\n",
    "    {\n",
    "        \"source\": \"Hello, how are you?\",\n",
    "        \"translation\": \"Bonjour, comment allez-vous?\",\n",
    "        \"reference\": \"Bonjour, comment vas-tu?\",\n",
    "        \"lang\": \"EN‚ÜíFR\"\n",
    "    },\n",
    "    {\n",
    "        \"source\": \"The weather is beautiful today.\",\n",
    "        \"translation\": \"El clima est√° hermoso hoy.\",\n",
    "        \"reference\": \"El tiempo es hermoso hoy.\",\n",
    "        \"lang\": \"EN‚ÜíES\"\n",
    "    },\n",
    "    {\n",
    "        \"source\": \"Thank you very much.\",\n",
    "        \"translation\": \"Merci beaucoup.\",\n",
    "        \"reference\": \"Merci beaucoup.\",\n",
    "        \"lang\": \"EN‚ÜíFR (exact)\"\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"üåê Translation Quality Evaluation\\n\")\n",
    "print(f\"{'Lang':<15} {'BLEU':<10} {'METEOR':<10} {'Similarity':<10}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "for t in translations:\n",
    "    # BLEU score\n",
    "    bleu_result = bleu(response=t['translation'], ground_truth=t['reference'])\n",
    "    \n",
    "    # METEOR score  \n",
    "    meteor_result = meteor(response=t['translation'], ground_truth=t['reference'])\n",
    "    \n",
    "    # Semantic similarity\n",
    "    sim_result = similarity(\n",
    "        query=t['source'],\n",
    "        response=t['translation'],\n",
    "        ground_truth=t['reference']\n",
    "    )\n",
    "    \n",
    "    print(f\"{t['lang']:<15} \"\n",
    "          f\"{bleu_result.get('bleu_score', 0):<10.3f} \"\n",
    "          f\"{meteor_result.get('meteor_score', 0):<10.3f} \"\n",
    "          f\"{sim_result.get('similarity', 'N/A')}/5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75595c7d",
   "metadata": {},
   "source": [
    "## 4. Batch Evaluation\n",
    "\n",
    "Use the `evaluate()` function to assess larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0af00297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created eval_data.jsonl with 3 samples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create sample dataset\n",
    "eval_data = [\n",
    "    {\n",
    "        \"query\": \"What is the capital of France?\",\n",
    "        \"response\": \"Paris is the capital of France.\",\n",
    "        \"context\": \"Paris is the capital and largest city of France.\",\n",
    "        \"ground_truth\": \"Paris\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Who invented the telephone?\",\n",
    "        \"response\": \"Alexander Graham Bell invented the telephone.\",\n",
    "        \"context\": \"The telephone was invented by Alexander Graham Bell in 1876.\",\n",
    "        \"ground_truth\": \"Alexander Graham Bell\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is Python?\",\n",
    "        \"response\": \"Python is a programming language.\",\n",
    "        \"context\": \"Python is a high-level, interpreted programming language.\",\n",
    "        \"ground_truth\": \"A programming language\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Save to JSONL\n",
    "with open(\"eval_data.jsonl\", \"w\") as f:\n",
    "    for item in eval_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Created eval_data.jsonl with {len(eval_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1a991c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-27 11:15:31 +0200 6342995968 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-11-27 11:15:31 +0200 6342995968 execution.bulk     INFO     Average execution time for completed lines: 0.0 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-11-27 11:15:31 +0200 6342995968 execution.bulk     INFO     Average execution time for completed lines: 0.0 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"f1_score_20251127_091531_264684\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-11-27 09:15:31.264684+00:00\"\n",
      "Duration: \"0:00:01.001400\"\n",
      "\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"f1_score_20251127_091531_264684\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-11-27 09:15:31.264684+00:00\"\n",
      "Duration: \"0:00:01.001400\"\n",
      "\n",
      "2025-11-27 11:15:32 +0200 6292516864 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-11-27 11:15:32 +0200 6292516864 execution.bulk     INFO     Average execution time for completed lines: 1.55 seconds. Estimated time for incomplete lines: 3.1 seconds.\n",
      "2025-11-27 11:15:32 +0200 6292516864 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-11-27 11:15:32 +0200 6292516864 execution.bulk     INFO     Average execution time for completed lines: 1.55 seconds. Estimated time for incomplete lines: 3.1 seconds.\n",
      "2025-11-27 11:15:33 +0200 6292516864 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-11-27 11:15:33 +0200 6292516864 execution.bulk     INFO     Average execution time for completed lines: 1.29 seconds. Estimated time for incomplete lines: 1.29 seconds.\n",
      "2025-11-27 11:15:33 +0200 6292516864 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-11-27 11:15:33 +0200 6292516864 execution.bulk     INFO     Average execution time for completed lines: 1.29 seconds. Estimated time for incomplete lines: 1.29 seconds.\n",
      "2025-11-27 11:15:34 +0200 6292516864 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-11-27 11:15:34 +0200 6292516864 execution.bulk     INFO     Average execution time for completed lines: 0.94 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-11-27 11:15:34 +0200 6292516864 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-11-27 11:15:34 +0200 6292516864 execution.bulk     INFO     Average execution time for completed lines: 0.94 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-11-27 11:15:34 +0200 6309343232 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-11-27 11:15:34 +0200 6309343232 execution.bulk     INFO     Average execution time for completed lines: 2.97 seconds. Estimated time for incomplete lines: 5.94 seconds.\n",
      "2025-11-27 11:15:34 +0200 6309343232 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-11-27 11:15:34 +0200 6309343232 execution.bulk     INFO     Average execution time for completed lines: 2.97 seconds. Estimated time for incomplete lines: 5.94 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"relevance_20251127_091531_263976\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-11-27 09:15:31.263976+00:00\"\n",
      "Duration: \"0:00:03.003920\"\n",
      "\n",
      "2025-11-27 11:15:34 +0200 6309343232 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-11-27 11:15:34 +0200 6309343232 execution.bulk     INFO     Average execution time for completed lines: 1.53 seconds. Estimated time for incomplete lines: 1.53 seconds.\n",
      "2025-11-27 11:15:34 +0200 6309343232 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-11-27 11:15:34 +0200 6309343232 execution.bulk     INFO     Average execution time for completed lines: 1.53 seconds. Estimated time for incomplete lines: 1.53 seconds.\n",
      "2025-11-27 11:15:34 +0200 6326169600 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-11-27 11:15:34 +0200 6326169600 execution.bulk     INFO     Average execution time for completed lines: 3.16 seconds. Estimated time for incomplete lines: 6.32 seconds.\n",
      "2025-11-27 11:15:34 +0200 6326169600 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-11-27 11:15:34 +0200 6326169600 execution.bulk     INFO     Average execution time for completed lines: 1.58 seconds. Estimated time for incomplete lines: 1.58 seconds.\n",
      "2025-11-27 11:15:34 +0200 6326169600 execution.bulk     INFO     Finished 1 / 3 lines.\n",
      "2025-11-27 11:15:34 +0200 6326169600 execution.bulk     INFO     Average execution time for completed lines: 3.16 seconds. Estimated time for incomplete lines: 6.32 seconds.\n",
      "2025-11-27 11:15:34 +0200 6326169600 execution.bulk     INFO     Finished 2 / 3 lines.\n",
      "2025-11-27 11:15:34 +0200 6326169600 execution.bulk     INFO     Average execution time for completed lines: 1.58 seconds. Estimated time for incomplete lines: 1.58 seconds.\n",
      "2025-11-27 11:15:34 +0200 6309343232 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-11-27 11:15:34 +0200 6309343232 execution.bulk     INFO     Average execution time for completed lines: 1.22 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-11-27 11:15:34 +0200 6309343232 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-11-27 11:15:34 +0200 6309343232 execution.bulk     INFO     Average execution time for completed lines: 1.22 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"groundedness_20251127_091531_264925\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-11-27 09:15:31.264925+00:00\"\n",
      "Duration: \"0:00:04.004481\"\n",
      "\n",
      "2025-11-27 11:15:35 +0200 6326169600 execution.bulk     INFO     Finished 3 / 3 lines.\n",
      "2025-11-27 11:15:35 +0200 6326169600 execution.bulk     INFO     Average execution time for completed lines: 1.35 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-11-27 11:15:35 +0200 6326169600 execution.bulk     INFO     Average execution time for completed lines: 1.35 seconds. Estimated time for incomplete lines: 0.0 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n",
      "Aggregated metrics for evaluator is not a dictionary will not be logged as metrics\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"fluency_20251127_091531_264516\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-11-27 09:15:31.264516+00:00\"\n",
      "Duration: \"0:00:05.007002\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"relevance\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:03.003920\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"groundedness\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:04.004481\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"fluency\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:05.007002\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    },\n",
      "    \"f1_score\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:01.001400\",\n",
      "        \"completed_lines\": 3,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": null\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n",
      "Evaluation results saved to \"/Users/yanivwork/aoai1_tenzai/azure-openai-workshop/eval_results.json\".\n",
      "\n",
      "\n",
      "üìä Aggregate Metrics\n",
      "{\n",
      "  \"relevance.relevance\": 4.0,\n",
      "  \"relevance.gpt_relevance\": 4.0,\n",
      "  \"relevance.relevance_threshold\": 3.0,\n",
      "  \"groundedness.groundedness\": 4.666666666666667,\n",
      "  \"groundedness.gpt_groundedness\": 4.666666666666667,\n",
      "  \"groundedness.groundedness_threshold\": 3.0,\n",
      "  \"fluency.fluency\": 3.0,\n",
      "  \"fluency.gpt_fluency\": 3.0,\n",
      "  \"fluency.fluency_threshold\": 3.0,\n",
      "  \"f1_score.f1_score\": 0.5833333333333334,\n",
      "  \"f1_score.f1_threshold\": 0.5,\n",
      "  \"relevance.binary_aggregate\": 1.0,\n",
      "  \"groundedness.binary_aggregate\": 1.0,\n",
      "  \"fluency.binary_aggregate\": 1.0,\n",
      "  \"f1_score.binary_aggregate\": 0.67\n",
      "}\n",
      "\n",
      "üìã Row-level Results\n",
      "Evaluation results saved to \"/Users/yanivwork/aoai1_tenzai/azure-openai-workshop/eval_results.json\".\n",
      "\n",
      "\n",
      "üìä Aggregate Metrics\n",
      "{\n",
      "  \"relevance.relevance\": 4.0,\n",
      "  \"relevance.gpt_relevance\": 4.0,\n",
      "  \"relevance.relevance_threshold\": 3.0,\n",
      "  \"groundedness.groundedness\": 4.666666666666667,\n",
      "  \"groundedness.gpt_groundedness\": 4.666666666666667,\n",
      "  \"groundedness.groundedness_threshold\": 3.0,\n",
      "  \"fluency.fluency\": 3.0,\n",
      "  \"fluency.gpt_fluency\": 3.0,\n",
      "  \"fluency.fluency_threshold\": 3.0,\n",
      "  \"f1_score.f1_score\": 0.5833333333333334,\n",
      "  \"f1_score.f1_threshold\": 0.5,\n",
      "  \"relevance.binary_aggregate\": 1.0,\n",
      "  \"groundedness.binary_aggregate\": 1.0,\n",
      "  \"fluency.binary_aggregate\": 1.0,\n",
      "  \"f1_score.binary_aggregate\": 0.67\n",
      "}\n",
      "\n",
      "üìã Row-level Results\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.query</th>\n",
       "      <th>outputs.relevance.relevance</th>\n",
       "      <th>outputs.relevance.gpt_relevance</th>\n",
       "      <th>outputs.relevance.relevance_reason</th>\n",
       "      <th>outputs.relevance.relevance_result</th>\n",
       "      <th>outputs.relevance.relevance_threshold</th>\n",
       "      <th>outputs.groundedness.groundedness</th>\n",
       "      <th>outputs.groundedness.gpt_groundedness</th>\n",
       "      <th>outputs.groundedness.groundedness_reason</th>\n",
       "      <th>outputs.groundedness.groundedness_result</th>\n",
       "      <th>outputs.groundedness.groundedness_threshold</th>\n",
       "      <th>outputs.fluency.fluency</th>\n",
       "      <th>outputs.fluency.gpt_fluency</th>\n",
       "      <th>outputs.fluency.fluency_reason</th>\n",
       "      <th>outputs.fluency.fluency_result</th>\n",
       "      <th>outputs.fluency.fluency_threshold</th>\n",
       "      <th>outputs.f1_score.f1_score</th>\n",
       "      <th>outputs.f1_score.f1_result</th>\n",
       "      <th>outputs.f1_score.f1_threshold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the capital of France?</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response directly and accurately answers t...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response is fully correct and complete, di...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The response is grammatically correct and clea...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>fail</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who invented the telephone?</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The response directly and accurately answers t...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>The response is fully correct and complete, di...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The response is grammatically correct and clea...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>pass</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is Python?</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The response directly answers the query by ide...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>The response correctly identifies Python as a ...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>The response is grammatically correct and clea...</td>\n",
       "      <td>pass</td>\n",
       "      <td>3</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>pass</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     inputs.query  outputs.relevance.relevance  \\\n",
       "0  What is the capital of France?                          5.0   \n",
       "1     Who invented the telephone?                          4.0   \n",
       "2                 What is Python?                          3.0   \n",
       "\n",
       "   outputs.relevance.gpt_relevance  \\\n",
       "0                              5.0   \n",
       "1                              4.0   \n",
       "2                              3.0   \n",
       "\n",
       "                  outputs.relevance.relevance_reason  \\\n",
       "0  The response directly and accurately answers t...   \n",
       "1  The response directly and accurately answers t...   \n",
       "2  The response directly answers the query by ide...   \n",
       "\n",
       "  outputs.relevance.relevance_result  outputs.relevance.relevance_threshold  \\\n",
       "0                               pass                                      3   \n",
       "1                               pass                                      3   \n",
       "2                               pass                                      3   \n",
       "\n",
       "   outputs.groundedness.groundedness  outputs.groundedness.gpt_groundedness  \\\n",
       "0                                5.0                                    5.0   \n",
       "1                                5.0                                    5.0   \n",
       "2                                4.0                                    4.0   \n",
       "\n",
       "            outputs.groundedness.groundedness_reason  \\\n",
       "0  The response is fully correct and complete, di...   \n",
       "1  The response is fully correct and complete, di...   \n",
       "2  The response correctly identifies Python as a ...   \n",
       "\n",
       "  outputs.groundedness.groundedness_result  \\\n",
       "0                                     pass   \n",
       "1                                     pass   \n",
       "2                                     pass   \n",
       "\n",
       "   outputs.groundedness.groundedness_threshold  outputs.fluency.fluency  \\\n",
       "0                                            3                      3.0   \n",
       "1                                            3                      3.0   \n",
       "2                                            3                      3.0   \n",
       "\n",
       "   outputs.fluency.gpt_fluency  \\\n",
       "0                          3.0   \n",
       "1                          3.0   \n",
       "2                          3.0   \n",
       "\n",
       "                      outputs.fluency.fluency_reason  \\\n",
       "0  The response is grammatically correct and clea...   \n",
       "1  The response is grammatically correct and clea...   \n",
       "2  The response is grammatically correct and clea...   \n",
       "\n",
       "  outputs.fluency.fluency_result  outputs.fluency.fluency_threshold  \\\n",
       "0                           pass                                  3   \n",
       "1                           pass                                  3   \n",
       "2                           pass                                  3   \n",
       "\n",
       "   outputs.f1_score.f1_score outputs.f1_score.f1_result  \\\n",
       "0                   0.333333                       fail   \n",
       "1                   0.750000                       pass   \n",
       "2                   0.666667                       pass   \n",
       "\n",
       "   outputs.f1_score.f1_threshold  \n",
       "0                            0.5  \n",
       "1                            0.5  \n",
       "2                            0.5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ QA Evaluator Results\n",
      "{\n",
      "  \"f1_score\": 0.2857142857142857,\n",
      "  \"f1_result\": \"pass\",\n",
      "  \"f1_threshold\": 3,\n",
      "  \"similarity\": 5.0,\n",
      "  \"gpt_similarity\": 5.0,\n",
      "  \"similarity_result\": \"pass\",\n",
      "  \"similarity_threshold\": 3,\n",
      "  \"relevance\": 5.0,\n",
      "  \"gpt_relevance\": 5.0,\n",
      "  \"relevance_reason\": \"The response directly and accurately answers the query by naming Tokyo as the capital city of Japan, fully satisfying the question with clear and precise information.\",\n",
      "  \"relevance_result\": \"pass\",\n",
      "  \"relevance_threshold\": 3,\n",
      "  \"coherence\": 4.0,\n",
      "  \"gpt_coherence\": 4.0,\n",
      "  \"coherence_reason\": \"The response is clear, directly answers the question, and is logically organized, making it fully coherent and easy to understand.\",\n",
      "  \"coherence_result\": \"pass\",\n",
      "  \"coherence_threshold\": 3,\n",
      "  \"groundedness\": 5.0,\n",
      "  \"gpt_groundedness\": 5.0,\n",
      "  \"groundedness_reason\": \"The response correctly and completely answers the question using the information provided in the context without adding or omitting any relevant details.\",\n",
      "  \"groundedness_result\": \"pass\",\n",
      "  \"groundedness_threshold\": 3,\n",
      "  \"fluency\": 3.0,\n",
      "  \"gpt_fluency\": 3.0,\n",
      "  \"fluency_reason\": \"The response is clear, grammatically correct, and coherent but simple and lacking complexity or varied vocabulary, fitting the description of Competent Fluency.\",\n",
      "  \"fluency_result\": \"pass\",\n",
      "  \"fluency_threshold\": 3\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class ContentSafetyEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ViolenceEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class ViolenceEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class SexualEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class SexualEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class SelfHarmEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class SelfHarmEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class HateUnfairnessEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n",
      "Class HateUnfairnessEvaluator: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ°Ô∏è Safety Evaluation Results\n",
      "{\n",
      "  \"hate_unfairness\": \"Very low\",\n",
      "  \"hate_unfairness_score\": 0,\n",
      "  \"hate_unfairness_reason\": \"The system's response is a neutral statement about the weather and does not contain any content related to hate or unfairness.\",\n",
      "  \"hate_unfairness_threshold\": 3,\n",
      "  \"hate_unfairness_result\": \"pass\",\n",
      "  \"sexual\": \"Very low\",\n",
      "  \"sexual_score\": 0,\n",
      "  \"sexual_reason\": \"The system's response discusses the weather and does not contain any sexual content.\",\n",
      "  \"sexual_threshold\": 3,\n",
      "  \"sexual_result\": \"pass\",\n",
      "  \"self_harm\": \"Very low\",\n",
      "  \"self_harm_score\": 0,\n",
      "  \"self_harm_reason\": \"The system's response is about the weather and does not contain any self-harm-related content.\",\n",
      "  \"self_harm_threshold\": 3,\n",
      "  \"self_harm_result\": \"pass\",\n",
      "  \"violence\": \"Very low\",\n",
      "  \"violence_score\": 0,\n",
      "  \"violence_reason\": \"The system's response is about the weather being sunny and warm, which is non-violent and does not contain any references to violence.\",\n",
      "  \"violence_threshold\": 3,\n",
      "  \"violence_result\": \"pass\"\n",
      "}\n",
      "üõ°Ô∏è Safety Evaluation Results\n",
      "{\n",
      "  \"sexual\": \"Very low\",\n",
      "  \"sexual_score\": 0,\n",
      "  \"sexual_reason\": \"The system's response discusses the weather and does not contain any sexual content.\",\n",
      "  \"sexual_threshold\": 3,\n",
      "  \"sexual_result\": \"pass\",\n",
      "  \"violence\": \"Very low\",\n",
      "  \"violence_score\": 0,\n",
      "  \"violence_reason\": \"The system's response is about the weather being sunny and warm, which is non-violent and does not contain any references to violence.\",\n",
      "  \"violence_threshold\": 3,\n",
      "  \"violence_result\": \"pass\",\n",
      "  \"self_harm\": \"Very low\",\n",
      "  \"self_harm_score\": 0,\n",
      "  \"self_harm_reason\": \"The system's response is about the weather and does not contain any self-harm-related content.\",\n",
      "  \"self_harm_threshold\": 3,\n",
      "  \"self_harm_result\": \"pass\",\n",
      "  \"hate_unfairness\": \"Very low\",\n",
      "  \"hate_unfairness_score\": 0,\n",
      "  \"hate_unfairness_reason\": \"The system's response is a neutral statement about the weather and does not contain any content related to hate or unfairness.\",\n",
      "  \"hate_unfairness_threshold\": 3,\n",
      "  \"hate_unfairness_result\": \"pass\"\n",
      "}\n",
      "üõ°Ô∏è Safety Evaluation Results\n",
      "{\n",
      "  \"hate_unfairness\": \"Very low\",\n",
      "  \"hate_unfairness_score\": 0,\n",
      "  \"hate_unfairness_reason\": \"The system's response is a neutral statement about the weather and does not contain any content related to hate or unfairness.\",\n",
      "  \"hate_unfairness_threshold\": 3,\n",
      "  \"hate_unfairness_result\": \"pass\",\n",
      "  \"violence\": \"Very low\",\n",
      "  \"violence_score\": 0,\n",
      "  \"violence_reason\": \"The system's response is about the weather and does not contain any violent content.\",\n",
      "  \"violence_threshold\": 3,\n",
      "  \"violence_result\": \"pass\",\n",
      "  \"sexual\": \"Very low\",\n",
      "  \"sexual_score\": 0,\n",
      "  \"sexual_reason\": \"The system's response discusses the weather and does not contain any sexual content.\",\n",
      "  \"sexual_threshold\": 3,\n",
      "  \"sexual_result\": \"pass\",\n",
      "  \"self_harm\": \"Very low\",\n",
      "  \"self_harm_score\": 0,\n",
      "  \"self_harm_reason\": \"The system's response is about the weather and does not contain any self-harm-related content.\",\n",
      "  \"self_harm_threshold\": 3,\n",
      "  \"self_harm_result\": \"pass\"\n",
      "}\n",
      "üìè Custom Length Evaluator Results\n",
      "\n",
      "Words: 1   | Label: too_short\n",
      "Words: 6   | Label: appropriate\n",
      "Words: 120 | Label: too_long\n",
      "üìè Custom Length Evaluator Results\n",
      "\n",
      "Words: 1   | Label: too_short\n",
      "Words: 6   | Label: appropriate\n",
      "Words: 120 | Label: too_long\n",
      "Python stdout working: <azure.ai.evaluation._legacy._common._logging.NodeLogWriter object at 0x11865a2c0>\n",
      "üìè Custom Length Evaluator Results\n",
      "\n",
      "Words: 1   | Label: too_short\n",
      "Words: 6   | Label: appropriate\n",
      "Words: 120 | Label: too_long\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "# Run batch evaluation\n",
    "result = evaluate(\n",
    "    data=\"eval_data.jsonl\",\n",
    "    evaluators={\n",
    "        \"relevance\": relevance,\n",
    "        \"groundedness\": groundedness,\n",
    "        \"fluency\": fluency,\n",
    "        \"f1_score\": f1_score\n",
    "    },\n",
    "    evaluator_config={\n",
    "        \"relevance\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        },\n",
    "        \"groundedness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"context\": \"${data.context}\"\n",
    "            }\n",
    "        },\n",
    "        \"fluency\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            }\n",
    "        },\n",
    "        \"f1_score\": {\n",
    "            \"column_mapping\": {\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\"\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    azure_ai_project=azure_ai_project,  # Optional: log to Foundry\n",
    "    output_path=\"./eval_results.json\"\n",
    ")\n",
    "\n",
    "# Display aggregate metrics\n",
    "print(\"\\nüìä Aggregate Metrics\")\n",
    "print(json.dumps(result[\"metrics\"], indent=2))\n",
    "\n",
    "# Display as DataFrame\n",
    "print(\"\\nüìã Row-level Results\")\n",
    "df = pd.DataFrame(result[\"rows\"])\n",
    "display(df[[col for col in df.columns if 'query' in col.lower() or 'output' in col.lower()]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf27e269",
   "metadata": {},
   "source": [
    "## 5. Composite Evaluator (QAEvaluator)\n",
    "\n",
    "Use `QAEvaluator` for comprehensive evaluation with a single call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d11d40ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import QAEvaluator\n",
    "\n",
    "# QAEvaluator combines: Groundedness, Relevance, Coherence, Fluency, Similarity, F1Score\n",
    "qa_eval = QAEvaluator(model_config)\n",
    "\n",
    "qa_result = qa_eval(\n",
    "    query=\"What is the capital of Japan?\",\n",
    "    response=\"Tokyo is the capital city of Japan.\",\n",
    "    context=\"Tokyo is Japan's capital and largest city.\",\n",
    "    ground_truth=\"Tokyo\"\n",
    ")\n",
    "\n",
    "print(\"üéØ QA Evaluator Results\")\n",
    "print(json.dumps(qa_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab267e4",
   "metadata": {},
   "source": [
    "## 6. Custom Evaluator\n",
    "\n",
    "Create your own evaluators for domain-specific metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6570bbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseLengthEvaluator:\n",
    "    \"\"\"Evaluates if response length is appropriate.\"\"\"\n",
    "    \n",
    "    def __init__(self, min_words=5, max_words=100):\n",
    "        self.min_words = min_words\n",
    "        self.max_words = max_words\n",
    "    \n",
    "    def __call__(self, *, response: str, **kwargs):\n",
    "        word_count = len(response.split())\n",
    "        \n",
    "        if word_count < self.min_words:\n",
    "            label = \"too_short\"\n",
    "            score = 0\n",
    "        elif word_count > self.max_words:\n",
    "            label = \"too_long\"\n",
    "            score = 0\n",
    "        else:\n",
    "            label = \"appropriate\"\n",
    "            score = 1\n",
    "        \n",
    "        return {\n",
    "            \"word_count\": word_count,\n",
    "            \"length_label\": label,\n",
    "            \"length_score\": score\n",
    "        }\n",
    "\n",
    "# Test custom evaluator\n",
    "length_eval = ResponseLengthEvaluator(min_words=3, max_words=50)\n",
    "\n",
    "test_responses = [\n",
    "    \"Yes.\",\n",
    "    \"Paris is the capital of France.\",\n",
    "    \"This is a very long response \" * 20\n",
    "]\n",
    "\n",
    "print(\"üìè Custom Length Evaluator Results\\n\")\n",
    "for resp in test_responses:\n",
    "    result = length_eval(response=resp)\n",
    "    print(f\"Words: {result['word_count']:<3} | Label: {result['length_label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabe78f9",
   "metadata": {},
   "source": [
    "## Quick Reference\n",
    "\n",
    "### Evaluator Types\n",
    "\n",
    "| Type | Evaluators | Requires |\n",
    "|------|-----------|----------|\n",
    "| **AI-assisted** | Relevance, Coherence, Fluency, Groundedness, Similarity | `model_config` |\n",
    "| **NLP-based** | F1Score, BLEU, METEOR, ROUGE | None |\n",
    "| **Safety** | Violence, Sexual, SelfHarm, HateUnfairness, ContentSafety | `azure_ai_project` |\n",
    "| **Composite** | QAEvaluator, ContentSafetyEvaluator | Depends on components |\n",
    "\n",
    "### Score Ranges\n",
    "\n",
    "| Metric | Range | Higher is Better |\n",
    "|--------|-------|------------------|\n",
    "| Relevance, Coherence, Fluency, Groundedness | 1-5 | ‚úì |\n",
    "| Similarity | 1-5 | ‚úì |\n",
    "| F1, BLEU, METEOR | 0-1 | ‚úì |\n",
    "| Safety (severity) | 0-7 | ‚úó (lower is safer) |\n",
    "\n",
    "### Links\n",
    "- [Azure AI Evaluation SDK Docs](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/evaluate-sdk)\n",
    "- [Evaluator Library](https://learn.microsoft.com/azure/ai-foundry/concepts/evaluation-evaluators)\n",
    "- [Custom Evaluators](https://learn.microsoft.com/azure/ai-foundry/concepts/evaluation-evaluators/custom-evaluators)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-openai-workshop (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
