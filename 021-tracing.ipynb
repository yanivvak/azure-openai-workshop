{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "232269ce",
   "metadata": {},
   "source": [
    "# Workshop 2: Tracing and Observability\n",
    "\n",
    "Welcome to Workshop 2! In this notebook, you'll learn how to add comprehensive observability to your Azure OpenAI applications using OpenTelemetry and Azure Monitor.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **OpenTelemetry Fundamentals** - Understanding traces, spans, and telemetry\n",
    "2. **Instrument Azure OpenAI calls** - Automatic tracing of API calls\n",
    "3. **Azure Application Insights Integration** - Send traces to Azure Monitor\n",
    "4. **Custom Instrumentation** - Add your own traces and metrics\n",
    "5. **Analyze Performance** - Use traces to debug and optimize\n",
    "6. **Production Monitoring** - Set up alerts and dashboards\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Workshop 1 (Deploy Your First Model)\n",
    "- Azure AI Foundry project with Application Insights configured\n",
    "- Environment variables set up correctly\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this workshop, you will:\n",
    "- Understand distributed tracing concepts\n",
    "- Instrument Azure OpenAI applications with OpenTelemetry\n",
    "- Analyze traces in Azure Application Insights\n",
    "- Create custom spans for business logic\n",
    "- Set up monitoring for production applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d57330e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/yanivwork/foundry_workshop/azure-openai-workshop/.venv/bin/python: No module named pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for tracing\n",
    "%pip install opentelemetry-sdk opentelemetry-instrumentation-openai-v2 azure-monitor-opentelemetry azure-core-tracing-opentelemetry opentelemetry-exporter-otlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62beccf8",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports\n",
    "\n",
    "Let's set up our environment and import the necessary libraries for tracing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0af67c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Tracing Workshop Environment Check:\n",
      "----------------------------------------\n",
      "‚úÖ PROJECT_ENDPOINT: Set\n",
      "‚úÖ AZURE_AI_FOUNDRY_RESOURCE_NAME: Set\n",
      "‚ùå MODEL_DEPLOYMENT_NAME: Not set\n",
      "‚ùå APPLICATION_INSIGHTS_CONNECTION_STRING: Not set\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# OpenTelemetry imports\n",
    "from opentelemetry import trace, metrics\n",
    "from opentelemetry.instrumentation.openai_v2 import OpenAIInstrumentor\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanProcessor\n",
    "from opentelemetry.sdk.metrics import MeterProvider\n",
    "from opentelemetry.sdk.metrics.export import ConsoleMetricExporter, PeriodicExportingMetricReader\n",
    "\n",
    "# Azure Monitor integration\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "from azure.core.settings import settings\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"üîß Tracing Workshop Environment Check:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Check required environment variables\n",
    "required_vars = [\n",
    "    'PROJECT_ENDPOINT',\n",
    "    'AZURE_AI_FOUNDRY_RESOURCE_NAME', \n",
    "    'MODEL_DEPLOYMENT_NAME',\n",
    "    'APPLICATION_INSIGHTS_CONNECTION_STRING'\n",
    "]\n",
    "\n",
    "for var in required_vars:\n",
    "    value = os.getenv(var)\n",
    "    status = \"‚úÖ\" if value else \"‚ùå\"\n",
    "    print(f\"{status} {var}: {'Set' if value else 'Not set'}\")\n",
    "\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a305877",
   "metadata": {},
   "source": [
    "## 2. Understanding OpenTelemetry Concepts\n",
    "\n",
    "Before we start instrumenting, let's understand the key concepts of observability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2fdeeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö OpenTelemetry Concepts:\n",
      "==================================================\n",
      "\n",
      "üîç Traces\n",
      "  üìñ Definition: End-to-end journey of a request through your application\n",
      "  üí° Example: User question ‚Üí API call ‚Üí Model inference ‚Üí Response\n",
      "  üéØ Use Case: Understanding request flow and finding bottlenecks\n",
      "\n",
      "‚è±Ô∏è Spans\n",
      "  üìñ Definition: Individual operations within a trace (building blocks)\n",
      "  üí° Example: Database query, HTTP request, function execution\n",
      "  üéØ Use Case: Measuring duration and recording operation details\n",
      "\n",
      "üè∑Ô∏è Attributes\n",
      "  üìñ Definition: Key-value pairs that provide context to spans\n",
      "  üí° Example: model_name=gpt-4o, user_id=123, temperature=0.7\n",
      "  üéØ Use Case: Filtering and analyzing traces by specific criteria\n",
      "\n",
      "üìä Metrics\n",
      "  üìñ Definition: Numerical measurements aggregated over time\n",
      "  üí° Example: Request count, response latency, token usage\n",
      "  üéØ Use Case: Monitoring performance trends and alerting\n",
      "\n",
      "üìù Logs\n",
      "  üìñ Definition: Structured or unstructured text records of events\n",
      "  üí° Example: Error messages, debug information, business events\n",
      "  üéØ Use Case: Debugging issues and understanding application behavior\n"
     ]
    }
   ],
   "source": [
    "def explain_telemetry_concepts():\n",
    "    \"\"\"\n",
    "    Explain key OpenTelemetry concepts with examples.\n",
    "    \"\"\"\n",
    "    print(\"üìö OpenTelemetry Concepts:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    concepts = {\n",
    "        \"üîç Traces\": {\n",
    "            \"definition\": \"End-to-end journey of a request through your application\",\n",
    "            \"example\": \"User question ‚Üí API call ‚Üí Model inference ‚Üí Response\",\n",
    "            \"use_case\": \"Understanding request flow and finding bottlenecks\"\n",
    "        },\n",
    "        \"‚è±Ô∏è Spans\": {\n",
    "            \"definition\": \"Individual operations within a trace (building blocks)\",\n",
    "            \"example\": \"Database query, HTTP request, function execution\",\n",
    "            \"use_case\": \"Measuring duration and recording operation details\"\n",
    "        },\n",
    "        \"üè∑Ô∏è Attributes\": {\n",
    "            \"definition\": \"Key-value pairs that provide context to spans\",\n",
    "            \"example\": \"model_name=gpt-4o, user_id=123, temperature=0.7\",\n",
    "            \"use_case\": \"Filtering and analyzing traces by specific criteria\"\n",
    "        },\n",
    "        \"üìä Metrics\": {\n",
    "            \"definition\": \"Numerical measurements aggregated over time\",\n",
    "            \"example\": \"Request count, response latency, token usage\",\n",
    "            \"use_case\": \"Monitoring performance trends and alerting\"\n",
    "        },\n",
    "        \"üìù Logs\": {\n",
    "            \"definition\": \"Structured or unstructured text records of events\",\n",
    "            \"example\": \"Error messages, debug information, business events\",\n",
    "            \"use_case\": \"Debugging issues and understanding application behavior\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for concept, details in concepts.items():\n",
    "        print(f\"\\n{concept}\")\n",
    "        print(f\"  üìñ Definition: {details['definition']}\")\n",
    "        print(f\"  üí° Example: {details['example']}\")\n",
    "        print(f\"  üéØ Use Case: {details['use_case']}\")\n",
    "\n",
    "explain_telemetry_concepts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac7b0dc",
   "metadata": {},
   "source": [
    "## 3. Setting Up Local Console Tracing\n",
    "\n",
    "Let's start with console tracing to see traces locally before sending them to Azure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a37ed5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Setting up Console Tracing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Overriding of current MeterProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Console tracing configured\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"validate_input\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x9b5a0649d707b94de0bdbfb5b45c1136\",\n",
      "        \"span_id\": \"0x2d25ab06d48bcf34\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0x596f60e0bcd3f6c9\",\n",
      "    \"start_time\": \"2025-08-10T14:16:14.182921Z\",\n",
      "    \"end_time\": \"2025-08-10T14:16:14.286711Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"input.length\": 42\n",
      "    },\n",
      "    \"events\": [\n",
      "        {\n",
      "            \"name\": \"Starting input validation\",\n",
      "            \"timestamp\": \"2025-08-10T14:16:14.182931Z\",\n",
      "            \"attributes\": {}\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Input validation completed\",\n",
      "            \"timestamp\": \"2025-08-10T14:16:14.286690Z\",\n",
      "            \"attributes\": {}\n",
      "        }\n",
      "    ],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"telemetry.sdk.language\": \"python\",\n",
      "            \"telemetry.sdk.name\": \"opentelemetry\",\n",
      "            \"telemetry.sdk.version\": \"1.36.0\",\n",
      "            \"service.name\": \"unknown_service\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"database_query\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x9b5a0649d707b94de0bdbfb5b45c1136\",\n",
      "        \"span_id\": \"0x7e2d143fc6c54b80\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0x1027cb7788ff2948\",\n",
      "    \"start_time\": \"2025-08-10T14:16:14.492271Z\",\n",
      "    \"end_time\": \"2025-08-10T14:16:14.545561Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"db.operation\": \"select\",\n",
      "        \"db.table\": \"user_preferences\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"telemetry.sdk.language\": \"python\",\n",
      "            \"telemetry.sdk.name\": \"opentelemetry\",\n",
      "            \"telemetry.sdk.version\": \"1.36.0\",\n",
      "            \"service.name\": \"unknown_service\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"business_logic\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x9b5a0649d707b94de0bdbfb5b45c1136\",\n",
      "        \"span_id\": \"0x1027cb7788ff2948\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0x596f60e0bcd3f6c9\",\n",
      "    \"start_time\": \"2025-08-10T14:16:14.287019Z\",\n",
      "    \"end_time\": \"2025-08-10T14:16:14.545696Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"operation\": \"prepare_prompt\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"telemetry.sdk.language\": \"python\",\n",
      "            \"telemetry.sdk.name\": \"opentelemetry\",\n",
      "            \"telemetry.sdk.version\": \"1.36.0\",\n",
      "            \"service.name\": \"unknown_service\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"format_response\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x9b5a0649d707b94de0bdbfb5b45c1136\",\n",
      "        \"span_id\": \"0x6d66b1085f646b50\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": \"0x596f60e0bcd3f6c9\",\n",
      "    \"start_time\": \"2025-08-10T14:16:14.545897Z\",\n",
      "    \"end_time\": \"2025-08-10T14:16:14.597224Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"format.type\": \"json\"\n",
      "    },\n",
      "    \"events\": [],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"telemetry.sdk.language\": \"python\",\n",
      "            \"telemetry.sdk.name\": \"opentelemetry\",\n",
      "            \"telemetry.sdk.version\": \"1.36.0\",\n",
      "            \"service.name\": \"unknown_service\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n",
      "{\n",
      "    \"name\": \"process_user_request\",\n",
      "    \"context\": {\n",
      "        \"trace_id\": \"0x9b5a0649d707b94de0bdbfb5b45c1136\",\n",
      "        \"span_id\": \"0x596f60e0bcd3f6c9\",\n",
      "        \"trace_state\": \"[]\"\n",
      "    },\n",
      "    \"kind\": \"SpanKind.INTERNAL\",\n",
      "    \"parent_id\": null,\n",
      "    \"start_time\": \"2025-08-10T14:16:14.182478Z\",\n",
      "    \"end_time\": \"2025-08-10T14:16:14.597707Z\",\n",
      "    \"status\": {\n",
      "        \"status_code\": \"UNSET\"\n",
      "    },\n",
      "    \"attributes\": {\n",
      "        \"user.id\": \"user123\",\n",
      "        \"request.type\": \"chat_completion\"\n",
      "    },\n",
      "    \"events\": [\n",
      "        {\n",
      "            \"name\": \"Request processing completed\",\n",
      "            \"timestamp\": \"2025-08-10T14:16:14.597406Z\",\n",
      "            \"attributes\": {}\n",
      "        }\n",
      "    ],\n",
      "    \"links\": [],\n",
      "    \"resource\": {\n",
      "        \"attributes\": {\n",
      "            \"telemetry.sdk.language\": \"python\",\n",
      "            \"telemetry.sdk.name\": \"opentelemetry\",\n",
      "            \"telemetry.sdk.version\": \"1.36.0\",\n",
      "            \"service.name\": \"unknown_service\"\n",
      "        },\n",
      "        \"schema_url\": \"\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Configure local console tracing\n",
    "def setup_console_tracing():\n",
    "    \"\"\"\n",
    "    Set up OpenTelemetry with console output for local debugging.\n",
    "    \"\"\"\n",
    "    print(\"üñ•Ô∏è Setting up Console Tracing...\")\n",
    "    \n",
    "    # Configure tracing\n",
    "    trace.set_tracer_provider(TracerProvider())\n",
    "    tracer = trace.get_tracer(__name__)\n",
    "    \n",
    "    # Add console exporter to see traces locally\n",
    "    console_exporter = ConsoleSpanExporter()\n",
    "    span_processor = BatchSpanProcessor(console_exporter)\n",
    "    trace.get_tracer_provider().add_span_processor(span_processor)\n",
    "    \n",
    "    # Configure metrics\n",
    "    console_reader = PeriodicExportingMetricReader(\n",
    "        ConsoleMetricExporter(),\n",
    "        export_interval_millis=5000  # Export every 5 seconds\n",
    "    )\n",
    "    metrics.set_meter_provider(MeterProvider(metric_readers=[console_reader]))\n",
    "    \n",
    "    print(\"‚úÖ Console tracing configured\")\n",
    "    return tracer\n",
    "\n",
    "# Set up console tracing\n",
    "tracer = setup_console_tracing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f111ad",
   "metadata": {},
   "source": [
    "## 4. Manual Tracing Example\n",
    "\n",
    "Let's create some manual traces to understand how spans work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67dff51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Creating Manual Traces...\n",
      "‚úÖ Manual traces created - check console output above\n"
     ]
    }
   ],
   "source": [
    "def demonstrate_manual_tracing():\n",
    "    \"\"\"\n",
    "    Create manual traces to demonstrate span concepts.\n",
    "    \"\"\"\n",
    "    print(\"üîç Creating Manual Traces...\")\n",
    "    \n",
    "    # Create a root span for the entire operation\n",
    "    with tracer.start_as_current_span(\"process_user_request\") as root_span:\n",
    "        # Add attributes to the root span\n",
    "        root_span.set_attribute(\"user.id\", \"user123\")\n",
    "        root_span.set_attribute(\"request.type\", \"chat_completion\")\n",
    "        \n",
    "        # Simulate input validation\n",
    "        with tracer.start_as_current_span(\"validate_input\") as validation_span:\n",
    "            validation_span.set_attribute(\"input.length\", 42)\n",
    "            validation_span.add_event(\"Starting input validation\")\n",
    "            time.sleep(0.1)  # Simulate processing time\n",
    "            validation_span.add_event(\"Input validation completed\")\n",
    "        \n",
    "        # Simulate business logic\n",
    "        with tracer.start_as_current_span(\"business_logic\") as logic_span:\n",
    "            logic_span.set_attribute(\"operation\", \"prepare_prompt\")\n",
    "            time.sleep(0.2)  # Simulate processing time\n",
    "            \n",
    "            # Nested span for database operation\n",
    "            with tracer.start_as_current_span(\"database_query\") as db_span:\n",
    "                db_span.set_attribute(\"db.operation\", \"select\")\n",
    "                db_span.set_attribute(\"db.table\", \"user_preferences\")\n",
    "                time.sleep(0.05)  # Simulate DB query\n",
    "        \n",
    "        # Simulate response formatting\n",
    "        with tracer.start_as_current_span(\"format_response\") as format_span:\n",
    "            format_span.set_attribute(\"format.type\", \"json\")\n",
    "            time.sleep(0.05)  # Simulate formatting time\n",
    "        \n",
    "        root_span.add_event(\"Request processing completed\")\n",
    "        print(\"‚úÖ Manual traces created - check console output above\")\n",
    "\n",
    "demonstrate_manual_tracing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e174c",
   "metadata": {},
   "source": [
    "## 5. Configure Azure Application Insights\n",
    "\n",
    "Now let's configure Azure Monitor to send traces to Application Insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "373566d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå APPLICATION_INSIGHTS_CONNECTION_STRING not found\n",
      "Please ensure you have deployed the infrastructure and set the environment variable.\n"
     ]
    }
   ],
   "source": [
    "def setup_azure_monitor():\n",
    "    \"\"\"\n",
    "    Configure Azure Monitor integration for sending telemetry to Application Insights.\n",
    "    \"\"\"\n",
    "    connection_string = os.getenv('APPLICATION_INSIGHTS_CONNECTION_STRING')\n",
    "    \n",
    "    if not connection_string:\n",
    "        print(\"‚ùå APPLICATION_INSIGHTS_CONNECTION_STRING not found\")\n",
    "        print(\"Please ensure you have deployed the infrastructure and set the environment variable.\")\n",
    "        return False\n",
    "    \n",
    "    print(\"‚òÅÔ∏è Configuring Azure Monitor...\")\n",
    "    \n",
    "    try:\n",
    "        # Configure Azure Monitor with the connection string\n",
    "        configure_azure_monitor(\n",
    "            connection_string=connection_string,\n",
    "            # Add custom configuration\n",
    "            logging_config={\n",
    "                \"level\": \"INFO\",\n",
    "                \"format\": \"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ Azure Monitor configured successfully\")\n",
    "        print(f\"üìä Traces will be sent to Application Insights\")\n",
    "        print(f\"üîó Connection string configured (ending with: ...{connection_string[-10:]})\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to configure Azure Monitor: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Configure Azure Monitor\n",
    "azure_monitor_enabled = setup_azure_monitor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90d47e5",
   "metadata": {},
   "source": [
    "## 6. Instrument Azure OpenAI Client\n",
    "\n",
    "Let's set up automatic instrumentation for Azure OpenAI API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d639c75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Setting up OpenAI Client with Tracing...\n"
     ]
    },
    {
     "ename": "ServiceRequestError",
     "evalue": "<urllib3.connection.HTTPSConnection object at 0x119eddc40>: Failed to resolve 'your-foundry-resource.services.ai.azure.com' ([Errno 8] nodename nor servname provided, or not known)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mServiceRequestError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m openai_client, project_client\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# Set up the instrumented OpenAI client\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m openai_client, project_client = \u001b[43msetup_openai_client_with_tracing\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36msetup_openai_client_with_tracing\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     11\u001b[39m project_client = AIProjectClient(\n\u001b[32m     12\u001b[39m     endpoint=os.getenv(\u001b[33m'\u001b[39m\u001b[33mPROJECT_ENDPOINT\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     13\u001b[39m     credential=DefaultAzureCredential()\n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Get connection info for Azure OpenAI\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m connection = \u001b[43mproject_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnections\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mAzureOpenAI\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_credentials\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Create instrumented Azure OpenAI client\u001b[39;00m\n\u001b[32m     20\u001b[39m openai_client = AzureOpenAI(\n\u001b[32m     21\u001b[39m     azure_endpoint=connection.properties[\u001b[33m\"\u001b[39m\u001b[33mendpoint\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     22\u001b[39m     api_key=connection.credentials[\u001b[33m\"\u001b[39m\u001b[33mkey\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     23\u001b[39m     api_version=\u001b[33m\"\u001b[39m\u001b[33m2024-09-01-preview\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     24\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foundry_workshop/azure-openai-workshop/.venv/lib/python3.12/site-packages/azure/core/tracing/decorator.py:119\u001b[39m, in \u001b[36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# If tracing is disabled globally and user didn't explicitly enable it, don't trace.\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m tracing_enabled \u001b[38;5;129;01mand\u001b[39;00m user_enabled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foundry_workshop/azure-openai-workshop/.venv/lib/python3.12/site-packages/azure/ai/projects/operations/_patch_connections.py:60\u001b[39m, in \u001b[36mConnectionsOperations.get_default\u001b[39m\u001b[34m(self, connection_type, include_credentials, **kwargs)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get the default connection for a given connection type.\u001b[39;00m\n\u001b[32m     49\u001b[39m \n\u001b[32m     50\u001b[39m \u001b[33;03m:param connection_type: The type of the connection. Required.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     57\u001b[39m \u001b[33;03m:raises ~azure.core.exceptions.HttpResponseError:\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     59\u001b[39m connections = \u001b[38;5;28msuper\u001b[39m().list(connection_type=connection_type, default_connection=\u001b[38;5;28;01mTrue\u001b[39;00m, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconnections\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minclude_credentials\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_with_credentials\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foundry_workshop/azure-openai-workshop/.venv/lib/python3.12/site-packages/azure/core/paging.py:136\u001b[39m, in \u001b[36mItemPaged.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._page_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    135\u001b[39m     \u001b[38;5;28mself\u001b[39m._page_iterator = itertools.chain.from_iterable(\u001b[38;5;28mself\u001b[39m.by_page())\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_page_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foundry_workshop/azure-openai-workshop/.venv/lib/python3.12/site-packages/azure/core/paging.py:82\u001b[39m, in \u001b[36mPageIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mEnd of paging\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28mself\u001b[39m._response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_next\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcontinuation_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m AzureError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m     84\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m error.continuation_token:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foundry_workshop/azure-openai-workshop/.venv/lib/python3.12/site-packages/azure/ai/projects/operations/_operations.py:905\u001b[39m, in \u001b[36mConnectionsOperations.list.<locals>.get_next\u001b[39m\u001b[34m(next_link)\u001b[39m\n\u001b[32m    902\u001b[39m _request = prepare_request(next_link)\n\u001b[32m    904\u001b[39m _stream = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m pipeline_response: PipelineResponse = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    906\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    908\u001b[39m response = pipeline_response.http_response\n\u001b[32m    910\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m200\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foundry_workshop/azure-openai-workshop/.venv/lib/python3.12/site-packages/azure/core/pipeline/_base.py:242\u001b[39m, in \u001b[36mPipeline.run\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    240\u001b[39m pipeline_request: PipelineRequest[HTTPRequestType] = PipelineRequest(request, context)\n\u001b[32m    241\u001b[39m first_node = \u001b[38;5;28mself\u001b[39m._impl_policies[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._impl_policies \u001b[38;5;28;01melse\u001b[39;00m _TransportRunner(\u001b[38;5;28mself\u001b[39m._transport)\n\u001b[32m--> \u001b[39m\u001b[32m242\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfirst_node\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpipeline_request\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foundry_workshop/azure-openai-workshop/.venv/lib/python3.12/site-packages/azure/core/pipeline/_base.py:98\u001b[39m, in \u001b[36m_SansIOHTTPPolicyRunner.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     96\u001b[39m _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_request, request)\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    100\u001b[39m     _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_exception, request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foundry_workshop/azure-openai-workshop/.venv/lib/python3.12/site-packages/azure/core/pipeline/_base.py:98\u001b[39m, in \u001b[36m_SansIOHTTPPolicyRunner.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     96\u001b[39m _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_request, request)\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    100\u001b[39m     _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_exception, request)\n",
      "    \u001b[31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 98 (2 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foundry_workshop/azure-openai-workshop/.venv/lib/python3.12/site-packages/azure/core/pipeline/_base.py:98\u001b[39m, in \u001b[36m_SansIOHTTPPolicyRunner.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     96\u001b[39m _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_request, request)\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    100\u001b[39m     _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_exception, request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foundry_workshop/azure-openai-workshop/.venv/lib/python3.12/site-packages/azure/core/pipeline/policies/_redirect.py:205\u001b[39m, in \u001b[36mRedirectPolicy.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    203\u001b[39m original_domain = get_domain(request.http_request.url) \u001b[38;5;28;01mif\u001b[39;00m redirect_settings[\u001b[33m\"\u001b[39m\u001b[33mallow\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m retryable:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     redirect_location = \u001b[38;5;28mself\u001b[39m.get_redirect_location(response)\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m redirect_location \u001b[38;5;129;01mand\u001b[39;00m redirect_settings[\u001b[33m\"\u001b[39m\u001b[33mallow\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foundry_workshop/azure-openai-workshop/.venv/lib/python3.12/site-packages/azure/core/pipeline/policies/_retry.py:567\u001b[39m, in \u001b[36mRetryPolicy.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    565\u001b[39m                 is_response_error = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    566\u001b[39m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m567\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    568\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    569\u001b[39m     end_time = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foundry_workshop/azure-openai-workshop/.venv/lib/python3.12/site-packages/azure/core/pipeline/policies/_retry.py:545\u001b[39m, in \u001b[36mRetryPolicy.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    543\u001b[39m \u001b[38;5;28mself\u001b[39m._configure_timeout(request, absolute_timeout, is_response_error)\n\u001b[32m    544\u001b[39m request.context[\u001b[33m\"\u001b[39m\u001b[33mretry_count\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mlen\u001b[39m(retry_settings[\u001b[33m\"\u001b[39m\u001b[33mhistory\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    546\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_retry(retry_settings, response):\n\u001b[32m    547\u001b[39m     retry_active = \u001b[38;5;28mself\u001b[39m.increment(retry_settings, response=response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foundry_workshop/azure-openai-workshop/.venv/lib/python3.12/site-packages/azure/core/pipeline/policies/_authentication.py:159\u001b[39m, in \u001b[36mBearerTokenCredentialPolicy.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    157\u001b[39m \u001b[38;5;28mself\u001b[39m.on_request(request)\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    161\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_exception(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foundry_workshop/azure-openai-workshop/.venv/lib/python3.12/site-packages/azure/core/pipeline/_base.py:98\u001b[39m, in \u001b[36m_SansIOHTTPPolicyRunner.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     96\u001b[39m _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_request, request)\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    100\u001b[39m     _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_exception, request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foundry_workshop/azure-openai-workshop/.venv/lib/python3.12/site-packages/azure/core/pipeline/_base.py:98\u001b[39m, in \u001b[36m_SansIOHTTPPolicyRunner.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     96\u001b[39m _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_request, request)\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    100\u001b[39m     _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_exception, request)\n",
      "    \u001b[31m[... skipping similar frames: _SansIOHTTPPolicyRunner.send at line 98 (2 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foundry_workshop/azure-openai-workshop/.venv/lib/python3.12/site-packages/azure/core/pipeline/_base.py:98\u001b[39m, in \u001b[36m_SansIOHTTPPolicyRunner.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     96\u001b[39m _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_request, request)\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    100\u001b[39m     _await_result(\u001b[38;5;28mself\u001b[39m._policy.on_exception, request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foundry_workshop/azure-openai-workshop/.venv/lib/python3.12/site-packages/azure/core/pipeline/_base.py:130\u001b[39m, in \u001b[36m_TransportRunner.send\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"HTTP transport send method.\u001b[39;00m\n\u001b[32m    121\u001b[39m \n\u001b[32m    122\u001b[39m \u001b[33;03m:param request: The PipelineRequest object.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    125\u001b[39m \u001b[33;03m:rtype: ~azure.core.pipeline.PipelineResponse\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    127\u001b[39m cleanup_kwargs_for_transport(request.context.options)\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m PipelineResponse(\n\u001b[32m    129\u001b[39m     request.http_request,\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sender\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    131\u001b[39m     context=request.context,\n\u001b[32m    132\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/foundry_workshop/azure-openai-workshop/.venv/lib/python3.12/site-packages/azure/core/pipeline/transport/_requests_basic.py:411\u001b[39m, in \u001b[36mRequestsTransport.send\u001b[39m\u001b[34m(self, request, proxies, **kwargs)\u001b[39m\n\u001b[32m    408\u001b[39m     error = ServiceRequestError(err, error=err)\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_rest(request):\n\u001b[32m    413\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mazure\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrest\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_requests_basic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RestRequestsTransportResponse\n",
      "\u001b[31mServiceRequestError\u001b[39m: <urllib3.connection.HTTPSConnection object at 0x119eddc40>: Failed to resolve 'your-foundry-resource.services.ai.azure.com' ([Errno 8] nodename nor servname provided, or not known)"
     ]
    }
   ],
   "source": [
    "def setup_openai_client_with_tracing():\n",
    "    \"\"\"\n",
    "    Set up Azure OpenAI client with automatic tracing.\n",
    "    \"\"\"\n",
    "    print(\"ü§ñ Setting up OpenAI Client with Tracing...\")\n",
    "    \n",
    "    # Instrument OpenAI API calls\n",
    "    OpenAIInstrumentor().instrument()\n",
    "    \n",
    "    # Initialize AI Project Client\n",
    "    project_client = AIProjectClient(\n",
    "        endpoint=os.getenv('PROJECT_ENDPOINT'),\n",
    "        credential=DefaultAzureCredential()\n",
    "    )\n",
    "    \n",
    "    # Get connection info for Azure OpenAI\n",
    "    connection = project_client.connections.get_default(connection_type=\"AzureOpenAI\", include_credentials=True)\n",
    "    \n",
    "    # Create instrumented Azure OpenAI client\n",
    "    openai_client = AzureOpenAI(\n",
    "        azure_endpoint=connection.properties[\"endpoint\"],\n",
    "        api_key=connection.credentials[\"key\"],\n",
    "        api_version=\"2024-09-01-preview\"\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ OpenAI client configured with automatic tracing\")\n",
    "    print(\"üìà All OpenAI API calls will now be automatically traced\")\n",
    "    \n",
    "    return openai_client, project_client\n",
    "\n",
    "# Set up the instrumented OpenAI client\n",
    "openai_client, project_client = setup_openai_client_with_tracing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0a14f7",
   "metadata": {},
   "source": [
    "## 7. Make Traced API Calls\n",
    "\n",
    "Let's make some API calls and see the traces in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9192f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_traced_completion():\n",
    "    \"\"\"\n",
    "    Make an Azure OpenAI completion call with comprehensive tracing.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Making Traced API Call...\")\n",
    "    \n",
    "    # Create a custom span for the entire conversation\n",
    "    with tracer.start_as_current_span(\"chat_conversation\") as conversation_span:\n",
    "        # Add conversation metadata\n",
    "        conversation_span.set_attribute(\"conversation.type\", \"single_turn\")\n",
    "        conversation_span.set_attribute(\"user.session_id\", \"session_123\")\n",
    "        \n",
    "        # Prepare the prompt\n",
    "        with tracer.start_as_current_span(\"prepare_prompt\") as prompt_span:\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"You are a helpful AI assistant specialized in explaining complex technical concepts in simple terms.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": \"Explain what distributed tracing is and why it's important for modern applications.\"\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            prompt_span.set_attribute(\"prompt.message_count\", len(messages))\n",
    "            prompt_span.set_attribute(\"prompt.user_message_length\", len(messages[1][\"content\"]))\n",
    "        \n",
    "        # Make the API call (this will be automatically traced by OpenTelemetry)\n",
    "        try:\n",
    "            response = openai_client.chat.completions.create(\n",
    "                model=os.getenv('MODEL_DEPLOYMENT_NAME'),\n",
    "                messages=messages,\n",
    "                temperature=0.7,\n",
    "                max_tokens=500\n",
    "            )\n",
    "            \n",
    "            # Process the response\n",
    "            with tracer.start_as_current_span(\"process_response\") as response_span:\n",
    "                response_text = response.choices[0].message.content\n",
    "                \n",
    "                # Add response metadata to span\n",
    "                response_span.set_attribute(\"response.length\", len(response_text))\n",
    "                response_span.set_attribute(\"response.finish_reason\", response.choices[0].finish_reason)\n",
    "                \n",
    "                if hasattr(response, 'usage'):\n",
    "                    response_span.set_attribute(\"tokens.prompt\", response.usage.prompt_tokens)\n",
    "                    response_span.set_attribute(\"tokens.completion\", response.usage.completion_tokens)\n",
    "                    response_span.set_attribute(\"tokens.total\", response.usage.total_tokens)\n",
    "            \n",
    "            conversation_span.add_event(\"API call completed successfully\")\n",
    "            conversation_span.set_attribute(\"conversation.status\", \"success\")\n",
    "            \n",
    "            print(\"‚úÖ API call completed successfully\")\n",
    "            print(f\"üìù Response: {response_text[:200]}...\")\n",
    "            \n",
    "            if hasattr(response, 'usage'):\n",
    "                print(f\"üéØ Token usage: {response.usage.total_tokens} total ({response.usage.prompt_tokens} prompt + {response.usage.completion_tokens} completion)\")\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            conversation_span.record_exception(e)\n",
    "            conversation_span.set_attribute(\"conversation.status\", \"error\")\n",
    "            print(f\"‚ùå API call failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "# Make the traced API call\n",
    "response = make_traced_completion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddaf0d08",
   "metadata": {},
   "source": [
    "## 8. Custom Metrics and Business Logic Tracing\n",
    "\n",
    "Let's add custom metrics and trace business logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a19920d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_metrics_and_traces():\n",
    "    \"\"\"\n",
    "    Demonstrate custom metrics and business logic tracing.\n",
    "    \"\"\"\n",
    "    print(\"üìä Creating Custom Metrics and Traces...\")\n",
    "    \n",
    "    # Get a meter for custom metrics\n",
    "    meter = metrics.get_meter(__name__)\n",
    "    \n",
    "    # Create custom metrics\n",
    "    request_counter = meter.create_counter(\n",
    "        name=\"openai_requests_total\",\n",
    "        description=\"Total number of OpenAI API requests\",\n",
    "        unit=\"requests\"\n",
    "    )\n",
    "    \n",
    "    response_time_histogram = meter.create_histogram(\n",
    "        name=\"openai_response_duration\",\n",
    "        description=\"OpenAI API response time distribution\",\n",
    "        unit=\"seconds\"\n",
    "    )\n",
    "    \n",
    "    token_usage_histogram = meter.create_histogram(\n",
    "        name=\"openai_token_usage\",\n",
    "        description=\"Token usage per request\",\n",
    "        unit=\"tokens\"\n",
    "    )\n",
    "    \n",
    "    # Create a complex business operation with multiple traces\n",
    "    with tracer.start_as_current_span(\"business_operation\") as operation_span:\n",
    "        operation_span.set_attribute(\"operation.type\", \"content_analysis\")\n",
    "        \n",
    "        # Simulate multiple API calls with metrics\n",
    "        for i in range(3):\n",
    "            with tracer.start_as_current_span(f\"api_call_{i+1}\") as call_span:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                call_span.set_attribute(\"call.iteration\", i + 1)\n",
    "                call_span.set_attribute(\"call.type\", \"content_generation\")\n",
    "                \n",
    "                try:\n",
    "                    # Make API call\n",
    "                    response = openai_client.chat.completions.create(\n",
    "                        model=os.getenv('MODEL_DEPLOYMENT_NAME'),\n",
    "                        messages=[\n",
    "                            {\"role\": \"user\", \"content\": f\"Give me a short fun fact about the number {i+1}\"}\n",
    "                        ],\n",
    "                        temperature=0.8,\n",
    "                        max_tokens=100\n",
    "                    )\n",
    "                    \n",
    "                    # Calculate response time\n",
    "                    response_time = time.time() - start_time\n",
    "                    \n",
    "                    # Record metrics\n",
    "                    request_counter.add(1, {\n",
    "                        \"model\": os.getenv('MODEL_DEPLOYMENT_NAME'),\n",
    "                        \"status\": \"success\",\n",
    "                        \"iteration\": str(i + 1)\n",
    "                    })\n",
    "                    \n",
    "                    response_time_histogram.record(response_time, {\n",
    "                        \"model\": os.getenv('MODEL_DEPLOYMENT_NAME'),\n",
    "                        \"status\": \"success\"\n",
    "                    })\n",
    "                    \n",
    "                    if hasattr(response, 'usage'):\n",
    "                        token_usage_histogram.record(response.usage.total_tokens, {\n",
    "                            \"model\": os.getenv('MODEL_DEPLOYMENT_NAME'),\n",
    "                            \"type\": \"total\"\n",
    "                        })\n",
    "                        \n",
    "                        # Add token usage to span\n",
    "                        call_span.set_attribute(\"tokens.total\", response.usage.total_tokens)\n",
    "                    \n",
    "                    call_span.set_attribute(\"response.time_seconds\", response_time)\n",
    "                    call_span.add_event(f\"Fun fact {i+1} generated successfully\")\n",
    "                    \n",
    "                    print(f\"‚úÖ Call {i+1}: {response.choices[0].message.content[:100]}...\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    # Record error metrics\n",
    "                    request_counter.add(1, {\n",
    "                        \"model\": os.getenv('MODEL_DEPLOYMENT_NAME'),\n",
    "                        \"status\": \"error\",\n",
    "                        \"iteration\": str(i + 1)\n",
    "                    })\n",
    "                    \n",
    "                    call_span.record_exception(e)\n",
    "                    call_span.set_attribute(\"error\", True)\n",
    "                    print(f\"‚ùå Call {i+1} failed: {str(e)}\")\n",
    "                \n",
    "                # Add a small delay between calls\n",
    "                time.sleep(0.5)\n",
    "        \n",
    "        operation_span.add_event(\"Business operation completed\")\n",
    "    \n",
    "    print(\"üìà Custom metrics and traces created\")\n",
    "    print(\"üí° Check Application Insights for the custom metrics and detailed traces\")\n",
    "\n",
    "# Create custom metrics and traces\n",
    "create_custom_metrics_and_traces()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c948c223",
   "metadata": {},
   "source": [
    "## 9. Production Monitoring Best Practices\n",
    "\n",
    "Let's implement production-ready monitoring patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d909486",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_production_monitoring():\n",
    "    \"\"\"\n",
    "    Demonstrate production monitoring patterns and best practices.\n",
    "    \"\"\"\n",
    "    print(\"üè≠ Production Monitoring Best Practices:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    best_practices = {\n",
    "        \"üéØ Key Metrics to Track\": [\n",
    "            \"Request count and rate\",\n",
    "            \"Response latency (p50, p95, p99)\",\n",
    "            \"Error rate and types\",\n",
    "            \"Token usage and costs\",\n",
    "            \"Model performance metrics\"\n",
    "        ],\n",
    "        \"üîç Essential Traces\": [\n",
    "            \"End-to-end user request flows\",\n",
    "            \"Database query performance\",\n",
    "            \"External API call latency\",\n",
    "            \"Business logic execution time\",\n",
    "            \"Error propagation paths\"\n",
    "        ],\n",
    "        \"üö® Alerting Strategy\": [\n",
    "            \"High error rates (>5%)\",\n",
    "            \"Slow response times (>95th percentile)\",\n",
    "            \"Token usage spikes\",\n",
    "            \"API quota approaching limits\",\n",
    "            \"Infrastructure health issues\"\n",
    "        ],\n",
    "        \"üìä Dashboard Elements\": [\n",
    "            \"Request volume over time\",\n",
    "            \"Response time trends\",\n",
    "            \"Error rate by endpoint\",\n",
    "            \"Token usage by model\",\n",
    "            \"User journey funnels\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in best_practices.items():\n",
    "        print(f\"\\n{category}\")\n",
    "        for item in items:\n",
    "            print(f\"  ‚Ä¢ {item}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üí° Production Tips:\")\n",
    "    print(\"  ‚Ä¢ Use sampling for high-volume applications\")\n",
    "    print(\"  ‚Ä¢ Set up proper log levels (ERROR, WARN, INFO)\")\n",
    "    print(\"  ‚Ä¢ Monitor business metrics alongside technical metrics\")\n",
    "    print(\"  ‚Ä¢ Implement proper error handling and retries\")\n",
    "    print(\"  ‚Ä¢ Use correlation IDs for request tracing\")\n",
    "    print(\"  ‚Ä¢ Set up cost monitoring for API usage\")\n",
    "\n",
    "demonstrate_production_monitoring()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555e493",
   "metadata": {},
   "source": [
    "## 10. Viewing Traces in Azure Application Insights\n",
    "\n",
    "Now let's check how to view your traces in Azure Application Insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c865d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_application_insights_instructions():\n",
    "    \"\"\"\n",
    "    Provide instructions for viewing traces in Azure Application Insights.\n",
    "    \"\"\"\n",
    "    print(\"üîç Viewing Traces in Azure Application Insights:\")\n",
    "    print(\"=\" * 55)\n",
    "    \n",
    "    resource_name = os.getenv('AZURE_AI_FOUNDRY_RESOURCE_NAME', 'your-ai-foundry-resource')\n",
    "    \n",
    "    instructions = [\n",
    "        \"1. üåê Open Azure Portal (portal.azure.com)\",\n",
    "        f\"2. üîç Search for and open your AI Foundry resource: '{resource_name}'\",\n",
    "        \"3. üìä In the left menu, find 'Application Insights' under Monitoring\",\n",
    "        \"4. üéØ Click on the Application Insights resource link\",\n",
    "        \"5. üìà Explore the following sections:\"\n",
    "    ]\n",
    "    \n",
    "    sections = {\n",
    "        \"üîç Transaction Search\": \"Search for individual traces and requests\",\n",
    "        \"üìä Application Map\": \"Visual representation of your application components\",\n",
    "        \"‚ö° Performance\": \"Response times and operation performance\",\n",
    "        \"‚ùå Failures\": \"Error rates and exception details\",\n",
    "        \"üìà Metrics\": \"Custom metrics and counters you created\",\n",
    "        \"üìù Logs\": \"Query traces using KQL (Kusto Query Language)\"\n",
    "    }\n",
    "    \n",
    "    for instruction in instructions:\n",
    "        print(f\"   {instruction}\")\n",
    "    \n",
    "    print(\"\\n   üìã Key Application Insights Sections:\")\n",
    "    for section, description in sections.items():\n",
    "        print(f\"      {section}: {description}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 55)\n",
    "    print(\"üîß Useful KQL Queries to Try:\")\n",
    "    print(\"\\nüìä View all traces from the last hour:\")\n",
    "    print(\"   traces | where timestamp > ago(1h) | order by timestamp desc\")\n",
    "    \n",
    "    print(\"\\nü§ñ Find OpenAI API calls:\")\n",
    "    print(\"   dependencies | where name contains 'openai' | order by timestamp desc\")\n",
    "    \n",
    "    print(\"\\n‚è±Ô∏è Analyze response times:\")\n",
    "    print(\"   requests | summarize avg(duration), max(duration) by bin(timestamp, 5m)\")\n",
    "    \n",
    "    print(\"\\n‚ùå Check for errors:\")\n",
    "    print(\"   exceptions | where timestamp > ago(1h) | order by timestamp desc\")\n",
    "    \n",
    "    print(\"\\nüí∞ Token usage analysis:\")\n",
    "    print(\"   customMetrics | where name == 'openai_token_usage' | order by timestamp desc\")\n",
    "\n",
    "show_application_insights_instructions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88165afa",
   "metadata": {},
   "source": [
    "## üéØ Workshop Summary\n",
    "\n",
    "Congratulations! You've successfully completed the Tracing and Observability workshop. Here's what you've learned:\n",
    "\n",
    "### ‚úÖ What You Accomplished\n",
    "\n",
    "1. **OpenTelemetry Fundamentals** - Understood traces, spans, and telemetry concepts\n",
    "2. **Automatic Instrumentation** - Set up automatic tracing for Azure OpenAI calls\n",
    "3. **Azure Monitor Integration** - Connected your application to Application Insights\n",
    "4. **Custom Instrumentation** - Created custom spans and metrics for business logic\n",
    "5. **Production Monitoring** - Learned best practices for production observability\n",
    "\n",
    "### üöÄ Next Steps\n",
    "\n",
    "- **Explore Application Insights**: Use the provided KQL queries to analyze your traces\n",
    "- **Set Up Alerts**: Create alerts for error rates and performance issues\n",
    "- **Create Dashboards**: Build custom dashboards for your specific use cases\n",
    "- **Implement Sampling**: For high-volume applications, configure trace sampling\n",
    "- **Monitor Costs**: Track token usage and associated costs\n",
    "\n",
    "### üìö Key Takeaways\n",
    "\n",
    "- **Observability is critical** for production AI applications\n",
    "- **OpenTelemetry provides** standardized instrumentation across languages\n",
    "- **Azure Application Insights** offers powerful analysis and alerting capabilities\n",
    "- **Custom metrics and traces** help monitor business-specific scenarios\n",
    "- **Proactive monitoring** helps prevent issues before they impact users\n",
    "\n",
    "Ready for the next workshop? Proceed to **Workshop 3: AI Agents** to learn about building intelligent agent systems!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbc5eab",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 5) (1590454517.py, line 5)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mWelcome to Workshop 2! In this notebook, you'll learn how to add comprehensive observability to your Azure OpenAI applications using OpenTelemetry and Azure Monitor.\u001b[39m\n                                                ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 5)\n"
     ]
    }
   ],
   "source": [
    "```xml\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "# Workshop 2: Tracing and Observability\n",
    "\n",
    "Welcome to Workshop 2! In this notebook, you'll learn how to add comprehensive observability to your Azure OpenAI applications using OpenTelemetry and Azure Monitor.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **OpenTelemetry Fundamentals** - Understanding traces, spans, and telemetry\n",
    "2. **Instrument Azure OpenAI calls** - Automatic tracing of API calls\n",
    "3. **Azure Application Insights Integration** - Send traces to Azure Monitor\n",
    "4. **Custom Instrumentation** - Add your own traces and metrics\n",
    "5. **Analyze Performance** - Use traces to debug and optimize\n",
    "6. **Production Monitoring** - Set up alerts and dashboards\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Completed Workshop 1 (Deploy Your First Model)\n",
    "- Azure AI Foundry project with Application Insights configured\n",
    "- Environment variables set up correctly\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this workshop, you will:\n",
    "- Understand distributed tracing concepts\n",
    "- Instrument Azure OpenAI applications with OpenTelemetry\n",
    "- Analyze traces in Azure Application Insights\n",
    "- Create custom spans for business logic\n",
    "- Set up monitoring for production applications\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Install required packages for tracing\n",
    "%pip install opentelemetry-sdk opentelemetry-instrumentation-openai-v2 azure-monitor-opentelemetry azure-core-tracing-opentelemetry opentelemetry-exporter-otlp\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 1. Environment Setup and Imports\n",
    "\n",
    "Let's set up our environment and import the necessary libraries for tracing.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# OpenTelemetry imports\n",
    "from opentelemetry import trace, metrics\n",
    "from opentelemetry.instrumentation.openai_v2 import OpenAIInstrumentor\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import ConsoleSpanExporter, BatchSpanProcessor\n",
    "from opentelemetry.sdk.metrics import MeterProvider\n",
    "from opentelemetry.sdk.metrics.export import ConsoleMetricExporter, PeriodicExportingMetricReader\n",
    "\n",
    "# Azure Monitor integration\n",
    "from azure.monitor.opentelemetry import configure_azure_monitor\n",
    "from azure.core.settings import settings\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"üîß Tracing Workshop Environment Check:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Check required environment variables\n",
    "required_vars = [\n",
    "    'PROJECT_ENDPOINT',\n",
    "    'AZURE_AI_FOUNDRY_RESOURCE_NAME', \n",
    "    'MODEL_DEPLOYMENT_NAME'\n",
    "]\n",
    "\n",
    "for var in required_vars:\n",
    "    value = os.getenv(var)\n",
    "    status = \"‚úÖ\" if value else \"‚ùå\"\n",
    "    print(f\"{status} {var}: {'Set' if value else 'Not set'}\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 2. Understanding OpenTelemetry Concepts\n",
    "\n",
    "Before we start instrumenting, let's understand the key concepts of observability.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "def explain_telemetry_concepts():\n",
    "    \"\"\"\n",
    "    Explain key OpenTelemetry concepts with examples.\n",
    "    \"\"\"\n",
    "    print(\"üìö OpenTelemetry Concepts:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    concepts = {\n",
    "        \"üîç Traces\": {\n",
    "            \"definition\": \"End-to-end journey of a request through your application\",\n",
    "            \"example\": \"User question ‚Üí API call ‚Üí Model inference ‚Üí Response\",\n",
    "            \"use_case\": \"Understanding request flow and finding bottlenecks\"\n",
    "        },\n",
    "        \"‚è±Ô∏è Spans\": {\n",
    "            \"definition\": \"Individual operations within a trace (building blocks)\",\n",
    "            \"example\": \"Database query, HTTP request, function execution\",\n",
    "            \"use_case\": \"Measuring duration and recording operation details\"\n",
    "        },\n",
    "        \"üè∑Ô∏è Attributes\": {\n",
    "            \"definition\": \"Key-value pairs that provide context to spans\",\n",
    "            \"example\": \"model_name=gpt-4o, user_id=123, temperature=0.7\",\n",
    "            \"use_case\": \"Filtering and analyzing traces by specific criteria\"\n",
    "        },\n",
    "        \"üìä Metrics\": {\n",
    "            \"definition\": \"Numerical measurements aggregated over time\",\n",
    "            \"example\": \"Request count, response latency, token usage\",\n",
    "            \"use_case\": \"Monitoring performance trends and alerting\"\n",
    "        },\n",
    "        \"üìù Logs\": {\n",
    "            \"definition\": \"Structured or unstructured text records of events\",\n",
    "            \"example\": \"Error messages, debug information, business events\",\n",
    "            \"use_case\": \"Debugging issues and understanding application behavior\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for concept, details in concepts.items():\n",
    "        print(f\"\\n{concept}\")\n",
    "        print(f\"  üìñ Definition: {details['definition']}\")\n",
    "        print(f\"  üí° Example: {details['example']}\")\n",
    "        print(f\"  üéØ Use Case: {details['use_case']}\")\n",
    "\n",
    "explain_telemetry_concepts()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 3. Setting Up Local Console Tracing\n",
    "\n",
    "Let's start with console tracing to see traces locally before sending them to Azure.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Configure local console tracing\n",
    "def setup_console_tracing():\n",
    "    \"\"\"\n",
    "    Set up OpenTelemetry with console output for local debugging.\n",
    "    \"\"\"\n",
    "    print(\"üñ•Ô∏è Setting up Console Tracing...\")\n",
    "    \n",
    "    # Configure tracing\n",
    "    trace.set_tracer_provider(TracerProvider())\n",
    "    tracer = trace.get_tracer(__name__)\n",
    "    \n",
    "    # Add console exporter for local viewing\n",
    "    console_exporter = ConsoleSpanExporter()\n",
    "    span_processor = BatchSpanProcessor(console_exporter)\n",
    "    trace.get_tracer_provider().add_span_processor(span_processor)\n",
    "    \n",
    "    # Configure Azure SDK tracing\n",
    "    settings.tracing_implementation = \"opentelemetry\"\n",
    "    \n",
    "    print(\"‚úÖ Console tracing configured\")\n",
    "    return tracer\n",
    "\n",
    "# Set up console tracing\n",
    "console_tracer = setup_console_tracing()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 4. Instrument Azure OpenAI SDK\n",
    "\n",
    "Now let's instrument the OpenAI SDK to automatically trace all API calls.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Instrument the OpenAI SDK\n",
    "def setup_openai_instrumentation():\n",
    "    \"\"\"\n",
    "    Instrument the OpenAI SDK for automatic tracing.\n",
    "    \"\"\"\n",
    "    print(\"üîß Instrumenting OpenAI SDK...\")\n",
    "    \n",
    "    # Instrument OpenAI SDK\n",
    "    OpenAIInstrumentor().instrument()\n",
    "    \n",
    "    # Optionally enable content recording (contains sensitive data)\n",
    "    content_recording = os.getenv('AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED', 'false').lower() == 'true'\n",
    "    if content_recording:\n",
    "        os.environ[\"AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED\"] = \"true\"\n",
    "        print(\"‚ö†Ô∏è Content recording enabled - traces will include prompts and responses\")\n",
    "    else:\n",
    "        print(\"üîí Content recording disabled - traces will not include sensitive content\")\n",
    "    \n",
    "    print(\"‚úÖ OpenAI SDK instrumentation complete\")\n",
    "\n",
    "setup_openai_instrumentation()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 5. Connect to Azure AI Foundry and Create Client\n",
    "\n",
    "Let's connect to our AI Foundry project and create an instrumented OpenAI client.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Connect to Azure AI Foundry with tracing\n",
    "try:\n",
    "    print(\"üîó Connecting to Azure AI Foundry...\")\n",
    "    \n",
    "    # Initialize project client\n",
    "    credential = DefaultAzureCredential()\n",
    "    project_client = AIProjectClient(\n",
    "        endpoint=os.getenv('PROJECT_ENDPOINT'),\n",
    "        credential=credential\n",
    "    )\n",
    "    \n",
    "    # Get OpenAI client (now instrumented)\n",
    "    openai_client = project_client.get_openai_client()\n",
    "    \n",
    "    print(\"‚úÖ Connected to Azure AI Foundry with tracing enabled\")\n",
    "    print(f\"üìç Project: {os.getenv('PROJECT_ENDPOINT')}\")\n",
    "    print(f\"ü§ñ Model: {os.getenv('MODEL_DEPLOYMENT_NAME')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Connection failed: {e}\")\n",
    "    exit()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 6. Test Basic Tracing with Console Output\n",
    "\n",
    "Let's make some API calls and observe the traces in the console.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Test basic tracing with console output\n",
    "def test_console_tracing():\n",
    "    \"\"\"\n",
    "    Test OpenAI API calls with console tracing to see spans locally.\n",
    "    \"\"\"\n",
    "    print(\"üß™ Testing Console Tracing:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Create a custom span for our business logic\n",
    "    with console_tracer.start_as_current_span(\"workshop_demo\") as demo_span:\n",
    "        # Add attributes to our custom span\n",
    "        demo_span.set_attribute(\"workshop.name\", \"tracing_demo\")\n",
    "        demo_span.set_attribute(\"user.type\", \"workshop_participant\")\n",
    "        \n",
    "        print(\"Making API call with tracing...\")\n",
    "        \n",
    "        # This API call will be automatically traced by OpenAI instrumentation\n",
    "        response = openai_client.chat.completions.create(\n",
    "            model=os.getenv('MODEL_DEPLOYMENT_NAME'),\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant explaining AI concepts.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Explain what observability means in AI applications in 2 sentences.\"}\n",
    "            ],\n",
    "            max_tokens=100,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Add response details to our span\n",
    "        demo_span.set_attribute(\"response.tokens\", response.usage.total_tokens)\n",
    "        demo_span.set_attribute(\"response.model\", response.model)\n",
    "        \n",
    "        print(f\"\\nüìù Response: {response.choices[0].message.content}\")\n",
    "        print(f\"üìä Tokens used: {response.usage.total_tokens}\")\n",
    "        \n",
    "        # Simulate some processing time\n",
    "        time.sleep(0.1)\n",
    "        demo_span.add_event(\"Processing complete\")\n",
    "\n",
    "# Run the test\n",
    "test_console_tracing()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 7. Azure Application Insights Integration\n",
    "\n",
    "Now let's configure Azure Application Insights to send traces to the cloud for persistent storage and analysis.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Set up Azure Application Insights integration\n",
    "def setup_azure_monitor():\n",
    "    \"\"\"\n",
    "    Configure Azure Monitor to send traces to Application Insights.\n",
    "    \"\"\"\n",
    "    print(\"‚òÅÔ∏è Setting up Azure Monitor Integration...\")\n",
    "    \n",
    "    try:\n",
    "        # Get Application Insights connection string from the project\n",
    "        connection_string = project_client.telemetry.get_application_insights_connection_string()\n",
    "        \n",
    "        if connection_string:\n",
    "            print(\"‚úÖ Retrieved Application Insights connection string from project\")\n",
    "            print(f\"üìç Connection: {connection_string[:50]}...\")\n",
    "            \n",
    "            # Configure Azure Monitor\n",
    "            configure_azure_monitor(connection_string=connection_string)\n",
    "            print(\"‚úÖ Azure Monitor configured successfully\")\n",
    "            \n",
    "            return True\n",
    "        else:\n",
    "            print(\"‚ùå No Application Insights connection string found\")\n",
    "            print(\"üí° Make sure Application Insights is configured in your AI Foundry project\")\n",
    "            return False\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Azure Monitor setup failed: {e}\")\n",
    "        print(\"üí° You can still use console tracing for this workshop\")\n",
    "        return False\n",
    "\n",
    "# Configure Azure Monitor\n",
    "azure_monitor_enabled = setup_azure_monitor()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 8. Advanced Tracing with Custom Spans\n",
    "\n",
    "Let's create a more complex example with custom spans to trace business logic.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Advanced tracing example with custom spans\n",
    "def advanced_ai_workflow(user_question: str, user_id: str = \"workshop_user\"):\n",
    "    \"\"\"\n",
    "    Example of a complex AI workflow with custom tracing.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get tracer for creating custom spans\n",
    "    tracer = trace.get_tracer(__name__)\n",
    "    \n",
    "    with tracer.start_as_current_span(\"ai_workflow\") as workflow_span:\n",
    "        # Add workflow-level attributes\n",
    "        workflow_span.set_attribute(\"user.id\", user_id)\n",
    "        workflow_span.set_attribute(\"workflow.type\", \"question_answering\")\n",
    "        workflow_span.set_attribute(\"input.question_length\", len(user_question))\n",
    "        \n",
    "        # Step 1: Question preprocessing\n",
    "        with tracer.start_as_current_span(\"preprocess_question\") as preprocess_span:\n",
    "            preprocess_span.add_event(\"Starting question preprocessing\")\n",
    "            \n",
    "            # Simulate preprocessing\n",
    "            cleaned_question = user_question.strip().lower()\n",
    "            question_type = \"technical\" if any(word in cleaned_question for word in [\"ai\", \"ml\", \"algorithm\", \"model\"]) else \"general\"\n",
    "            \n",
    "            preprocess_span.set_attribute(\"question.type\", question_type)\n",
    "            preprocess_span.set_attribute(\"question.cleaned_length\", len(cleaned_question))\n",
    "            \n",
    "            time.sleep(0.05)  # Simulate processing time\n",
    "            preprocess_span.add_event(\"Preprocessing complete\")\n",
    "        \n",
    "        # Step 2: Select appropriate system prompt based on question type\n",
    "        with tracer.start_as_current_span(\"select_system_prompt\") as prompt_span:\n",
    "            if question_type == \"technical\":\n",
    "                system_prompt = \"You are an expert AI/ML engineer. Provide technical but accessible explanations.\"\n",
    "            else:\n",
    "                system_prompt = \"You are a helpful assistant. Provide clear and friendly explanations.\"\n",
    "            \n",
    "            prompt_span.set_attribute(\"prompt.type\", question_type)\n",
    "            prompt_span.set_attribute(\"prompt.length\", len(system_prompt))\n",
    "        \n",
    "        # Step 3: Make AI API call (automatically traced)\n",
    "        with tracer.start_as_current_span(\"ai_inference\") as inference_span:\n",
    "            inference_span.add_event(\"Starting AI inference\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            response = openai_client.chat.completions.create(\n",
    "                model=os.getenv('MODEL_DEPLOYMENT_NAME'),\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_prompt},\n",
    "                    {\"role\": \"user\", \"content\": user_question}\n",
    "                ],\n",
    "                max_tokens=200,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            \n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            # Add inference metrics to span\n",
    "            inference_span.set_attribute(\"inference.duration_ms\", round(inference_time * 1000, 2))\n",
    "            inference_span.set_attribute(\"tokens.prompt\", response.usage.prompt_tokens)\n",
    "            inference_span.set_attribute(\"tokens.completion\", response.usage.completion_tokens) \n",
    "            inference_span.set_attribute(\"tokens.total\", response.usage.total_tokens)\n",
    "            inference_span.set_attribute(\"model.name\", response.model)\n",
    "            \n",
    "            inference_span.add_event(\"AI inference complete\")\n",
    "        \n",
    "        # Step 4: Post-process response\n",
    "        with tracer.start_as_current_span(\"postprocess_response\") as postprocess_span:\n",
    "            ai_response = response.choices[0].message.content\n",
    "            \n",
    "            # Simulate some post-processing\n",
    "            word_count = len(ai_response.split())\n",
    "            has_code = \"```\" in ai_response\n",
    "            \n",
    "            postprocess_span.set_attribute(\"response.word_count\", word_count)\n",
    "            postprocess_span.set_attribute(\"response.has_code\", has_code)\n",
    "            postprocess_span.set_attribute(\"response.length\", len(ai_response))\n",
    "            \n",
    "            time.sleep(0.02)  # Simulate processing time\n",
    "        \n",
    "        # Add final workflow metrics\n",
    "        workflow_span.set_attribute(\"workflow.success\", True)\n",
    "        workflow_span.set_attribute(\"workflow.total_tokens\", response.usage.total_tokens)\n",
    "        workflow_span.add_event(\"Workflow complete\")\n",
    "        \n",
    "        return {\n",
    "            \"response\": ai_response,\n",
    "            \"metadata\": {\n",
    "                \"question_type\": question_type,\n",
    "                \"tokens_used\": response.usage.total_tokens,\n",
    "                \"inference_time_ms\": round(inference_time * 1000, 2),\n",
    "                \"word_count\": word_count\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Test the advanced workflow\n",
    "print(\"üöÄ Testing Advanced AI Workflow with Custom Tracing:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "test_questions = [\n",
    "    \"What is machine learning and how does it work?\",\n",
    "    \"What's the weather like today?\",\n",
    "    \"Explain the difference between supervised and unsupervised learning algorithms.\"\n",
    "]\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{i}Ô∏è‚É£ Question: {question}\")\n",
    "    result = advanced_ai_workflow(question, f\"user_{i}\")\n",
    "    print(f\"üìù Response: {result['response'][:100]}...\")\n",
    "    print(f\"üìä Metadata: {result['metadata']}\")\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 9. Error Handling and Tracing\n",
    "\n",
    "Let's see how to trace errors and exceptions in your AI applications.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Error handling with tracing\n",
    "def trace_with_error_handling():\n",
    "    \"\"\"\n",
    "    Demonstrate how to trace errors and exceptions.\n",
    "    \"\"\"\n",
    "    tracer = trace.get_tracer(__name__)\n",
    "    \n",
    "    print(\"üö® Testing Error Handling with Tracing:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Test 1: API call with invalid parameters\n",
    "    with tracer.start_as_current_span(\"test_invalid_model\") as span:\n",
    "        span.set_attribute(\"test.type\", \"invalid_model\")\n",
    "        \n",
    "        try:\n",
    "            # This should fail due to invalid model name\n",
    "            response = openai_client.chat.completions.create(\n",
    "                model=\"invalid-model-name\",\n",
    "                messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n",
    "                max_tokens=50\n",
    "            )\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Record the error in the span\n",
    "            span.record_exception(e)\n",
    "            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n",
    "            span.set_attribute(\"error.type\", type(e).__name__)\n",
    "            span.set_attribute(\"error.message\", str(e))\n",
    "            \n",
    "            print(f\"‚ùå Expected error captured: {type(e).__name__}\")\n",
    "    \n",
    "    # Test 2: Rate limiting simulation\n",
    "    with tracer.start_as_current_span(\"test_rate_limiting\") as span:\n",
    "        span.set_attribute(\"test.type\", \"rate_limiting_simulation\")\n",
    "        \n",
    "        try:\n",
    "            # Simulate rate limiting by making rapid requests\n",
    "            print(\"üîÑ Simulating multiple rapid requests...\")\n",
    "            \n",
    "            for i in range(3):\n",
    "                with tracer.start_as_current_span(f\"request_{i}\") as req_span:\n",
    "                    req_span.set_attribute(\"request.number\", i)\n",
    "                    \n",
    "                    try:\n",
    "                        response = openai_client.chat.completions.create(\n",
    "                            model=os.getenv('MODEL_DEPLOYMENT_NAME'),\n",
    "                            messages=[{\"role\": \"user\", \"content\": f\"Quick question {i}\"}],\n",
    "                            max_tokens=20\n",
    "                        )\n",
    "                        \n",
    "                        req_span.set_attribute(\"request.success\", True)\n",
    "                        req_span.set_attribute(\"tokens.used\", response.usage.total_tokens)\n",
    "                        print(f\"  ‚úÖ Request {i} succeeded\")\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        req_span.record_exception(e)\n",
    "                        req_span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n",
    "                        req_span.set_attribute(\"request.success\", False)\n",
    "                        print(f\"  ‚ùå Request {i} failed: {type(e).__name__}\")\n",
    "                    \n",
    "                    time.sleep(0.1)  # Small delay between requests\n",
    "                    \n",
    "        except Exception as e:\n",
    "            span.record_exception(e)\n",
    "            span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n",
    "            print(f\"‚ùå Batch request error: {e}\")\n",
    "\n",
    "# Run error handling test\n",
    "trace_with_error_handling()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 10. Performance Analysis with Tracing\n",
    "\n",
    "Let's create a performance analysis example that shows how to use tracing data.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Performance analysis with tracing\n",
    "def performance_analysis_demo():\n",
    "    \"\"\"\n",
    "    Demonstrate performance analysis using tracing data.\n",
    "    \"\"\"\n",
    "    tracer = trace.get_tracer(__name__)\n",
    "    \n",
    "    print(\"üìä Performance Analysis Demo:\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    # Test different temperature values and their impact on response time\n",
    "    temperatures = [0.1, 0.5, 1.0]\n",
    "    results = []\n",
    "    \n",
    "    with tracer.start_as_current_span(\"performance_analysis\") as analysis_span:\n",
    "        analysis_span.set_attribute(\"analysis.type\", \"temperature_impact\")\n",
    "        \n",
    "        for temp in temperatures:\n",
    "            with tracer.start_as_current_span(f\"temperature_test_{temp}\") as temp_span:\n",
    "                temp_span.set_attribute(\"model.temperature\", temp)\n",
    "                \n",
    "                start_time = time.time()\n",
    "                \n",
    "                try:\n",
    "                    response = openai_client.chat.completions.create(\n",
    "                        model=os.getenv('MODEL_DEPLOYMENT_NAME'),\n",
    "                        messages=[\n",
    "                            {\"role\": \"system\", \"content\": \"You are a creative writer.\"},\n",
    "                            {\"role\": \"user\", \"content\": \"Write a short poem about AI.\"}\n",
    "                        ],\n",
    "                        max_tokens=100,\n",
    "                        temperature=temp\n",
    "                    )\n",
    "                    \n",
    "                    end_time = time.time()\n",
    "                    duration = round((end_time - start_time) * 1000, 2)\n",
    "                    \n",
    "                    # Add performance metrics to span\n",
    "                    temp_span.set_attribute(\"performance.duration_ms\", duration)\n",
    "                    temp_span.set_attribute(\"performance.tokens_per_second\", \n",
    "                                          round(response.usage.total_tokens / (duration/1000), 2))\n",
    "                    temp_span.set_attribute(\"tokens.total\", response.usage.total_tokens)\n",
    "                    temp_span.set_attribute(\"response.length\", len(response.choices[0].message.content))\n",
    "                    \n",
    "                    results.append({\n",
    "                        'temperature': temp,\n",
    "                        'duration_ms': duration,\n",
    "                        'tokens': response.usage.total_tokens,\n",
    "                        'tokens_per_second': round(response.usage.total_tokens / (duration/1000), 2),\n",
    "                        'response_length': len(response.choices[0].message.content)\n",
    "                    })\n",
    "                    \n",
    "                    print(f\"üå°Ô∏è Temperature {temp}: {duration}ms, {response.usage.total_tokens} tokens\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    temp_span.record_exception(e)\n",
    "                    temp_span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n",
    "                    print(f\"‚ùå Temperature {temp} failed: {e}\")\n",
    "    \n",
    "    # Analyze results\n",
    "    if results:\n",
    "        print(f\"\\nüìà Performance Analysis Results:\")\n",
    "        print(\"-\" * 35)\n",
    "        \n",
    "        avg_duration = sum(r['duration_ms'] for r in results) / len(results)\n",
    "        avg_tokens_per_sec = sum(r['tokens_per_second'] for r in results) / len(results)\n",
    "        \n",
    "        print(f\"Average duration: {avg_duration:.2f}ms\")\n",
    "        print(f\"Average tokens/sec: {avg_tokens_per_sec:.2f}\")\n",
    "        \n",
    "        # Find fastest and slowest\n",
    "        fastest = min(results, key=lambda x: x['duration_ms'])\n",
    "        slowest = max(results, key=lambda x: x['duration_ms'])\n",
    "        \n",
    "        print(f\"Fastest: Temperature {fastest['temperature']} ({fastest['duration_ms']}ms)\")\n",
    "        print(f\"Slowest: Temperature {slowest['temperature']} ({slowest['duration_ms']}ms)\")\n",
    "\n",
    "# Run performance analysis\n",
    "performance_analysis_demo()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 11. Viewing Traces in Azure Application Insights\n",
    "\n",
    "If Azure Monitor is configured, your traces are now being sent to Application Insights. Here's how to view them.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Instructions for viewing traces in Azure\n",
    "def show_azure_insights_instructions():\n",
    "    \"\"\"\n",
    "    Provide instructions for viewing traces in Azure Application Insights.\n",
    "    \"\"\"\n",
    "    print(\"‚òÅÔ∏è Viewing Traces in Azure Application Insights:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if azure_monitor_enabled:\n",
    "        print(\"‚úÖ Your traces are being sent to Azure Application Insights!\")\n",
    "        print()\n",
    "        \n",
    "        steps = [\n",
    "            \"1. Open the Azure Portal (portal.azure.com)\",\n",
    "            \"2. Navigate to your AI Foundry project resource\",\n",
    "            \"3. Go to 'Observability' -> 'Tracing' in the left menu\",\n",
    "            \"4. Or directly open Application Insights resource\",\n",
    "            \"5. In Application Insights, go to 'Investigate' -> 'Transaction search'\",\n",
    "            \"6. Look for traces with operation names like:\",\n",
    "            \"   ‚Ä¢ 'ai_workflow' (our custom spans)\",\n",
    "            \"   ‚Ä¢ 'chat/completions' (OpenAI API calls)\",\n",
    "            \"   ‚Ä¢ 'workshop_demo' (our demo spans)\",\n",
    "            \"7. Click on a trace to see the detailed span timeline\",\n",
    "            \"8. Explore the 'Performance' tab for aggregated metrics\",\n",
    "            \"9. Use 'Logs' to write KQL queries for custom analysis\"\n",
    "        ]\n",
    "        \n",
    "        for step in steps:\n",
    "            print(step)\n",
    "        \n",
    "        print(f\"\\nüîç What to Look For:\")\n",
    "        insights = [\n",
    "            \"‚Ä¢ End-to-end trace duration\",\n",
    "            \"‚Ä¢ Individual span timings (preprocessing, inference, postprocessing)\",\n",
    "            \"‚Ä¢ OpenAI API call details and token usage\",\n",
    "            \"‚Ä¢ Custom attributes we added (question_type, user_id, etc.)\",\n",
    "            \"‚Ä¢ Error traces and exception details\",\n",
    "            \"‚Ä¢ Performance patterns across different requests\"\n",
    "        ]\n",
    "        \n",
    "        for insight in insights:\n",
    "            print(insight)\n",
    "        \n",
    "        print(f\"\\nüìä Useful KQL Queries for Application Insights:\")\n",
    "        queries = [\n",
    "            \"// All traces from our workshop\",\n",
    "            \"traces | where customDimensions.['workshop.name'] == 'tracing_demo'\",\n",
    "            \"\",\n",
    "            \"// OpenAI API performance\",\n",
    "            \"requests | where name contains 'chat/completions'\",\n",
    "            \"| summarize avg(duration), count() by bin(timestamp, 5m)\",\n",
    "            \"\",\n",
    "            \"// Token usage over time\", \n",
    "            \"traces | where customDimensions.['tokens.total'] != ''\",\n",
    "            \"| extend tokens = toint(customDimensions.['tokens.total'])\",\n",
    "            \"| summarize avg(tokens), sum(tokens) by bin(timestamp, 1h)\"\n",
    "        ]\n",
    "        \n",
    "        for query in queries:\n",
    "            print(query)\n",
    "            \n",
    "    else:\n",
    "        print(\"‚ùå Azure Monitor not configured\")\n",
    "        print(\"üí° To enable Azure Application Insights:\")\n",
    "        print(\"1. Ensure Application Insights is configured in your AI Foundry project\")\n",
    "        print(\"2. Check the 'Observability' section in Azure AI Foundry portal\")\n",
    "        print(\"3. Verify your connection string is accessible\")\n",
    "\n",
    "show_azure_insights_instructions()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 12. Production Best Practices\n",
    "\n",
    "Let's cover best practices for using tracing in production environments.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Production best practices\n",
    "def production_best_practices():\n",
    "    \"\"\"\n",
    "    Demonstrate production best practices for tracing.\n",
    "    \"\"\"\n",
    "    print(\"üè≠ Production Tracing Best Practices:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    practices = {\n",
    "        \"üîí Security\": [\n",
    "            \"‚Ä¢ Disable content recording in production (set AZURE_TRACING_GEN_AI_CONTENT_RECORDING_ENABLED=false)\",\n",
    "            \"‚Ä¢ Use Azure Managed Identity instead of API keys\",\n",
    "            \"‚Ä¢ Be careful with custom attributes - don't include PII\",\n",
    "            \"‚Ä¢ Review trace data retention policies\"\n",
    "        ],\n",
    "        \"‚ö° Performance\": [\n",
    "            \"‚Ä¢ Use sampling to reduce trace volume (start with 1% sampling)\",\n",
    "            \"‚Ä¢ Implement batch span processors instead of simple processors\",\n",
    "            \"‚Ä¢ Set appropriate timeout values for exporters\",\n",
    "            \"‚Ä¢ Monitor the overhead of tracing itself\"\n",
    "        ],\n",
    "        \"üìä Monitoring\": [\n",
    "            \"‚Ä¢ Set up alerts on error rates and high latency\",\n",
    "            \"‚Ä¢ Monitor token usage trends and costs\",\n",
    "            \"‚Ä¢ Track model performance metrics over time\",\n",
    "            \"‚Ä¢ Create dashboards for key business metrics\"\n",
    "        ],\n",
    "        \"üèóÔ∏è Architecture\": [\n",
    "            \"‚Ä¢ Use consistent span names across services\",\n",
    "            \"‚Ä¢ Add business context through custom attributes\",\n",
    "            \"‚Ä¢ Implement correlation IDs for distributed tracing\",\n",
    "            \"‚Ä¢ Document your tracing strategy and naming conventions\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, items in practices.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        for item in items:\n",
    "            print(f\"  {item}\")\n",
    "    \n",
    "    print(f\"\\nüö® Common Pitfalls to Avoid:\")\n",
    "    pitfalls = [\n",
    "        \"‚Ä¢ Tracing sensitive data (passwords, personal info)\",\n",
    "        \"‚Ä¢ Over-instrumenting and creating performance overhead\",\n",
    "        \"‚Ä¢ Not handling tracing failures gracefully\",\n",
    "        \"‚Ä¢ Forgetting to update trace configurations in different environments\",\n",
    "        \"‚Ä¢ Not correlating traces with business metrics\"\n",
    "    ]\n",
    "    \n",
    "    for pitfall in pitfalls:\n",
    "        print(f\"  {pitfall}\")\n",
    "\n",
    "production_best_practices()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## 13. Workshop Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed Workshop 2 on Tracing and Observability.\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Workshop summary\n",
    "def workshop_summary():\n",
    "    \"\"\"\n",
    "    Summarize what was learned in Workshop 2.\n",
    "    \"\"\"\n",
    "    print(\"üéØ Workshop 2 Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    achievements = [\n",
    "        \"‚úÖ Understood OpenTelemetry concepts (traces, spans, attributes)\",\n",
    "        \"‚úÖ Instrumented Azure OpenAI SDK for automatic tracing\",\n",
    "        \"‚úÖ Set up console tracing for local debugging\",\n",
    "        \"‚úÖ Configured Azure Application Insights integration\",\n",
    "        \"‚úÖ Created custom spans for business logic\",\n",
    "        \"‚úÖ Implemented error handling with tracing\",\n",
    "        \"‚úÖ Performed performance analysis using trace data\",\n",
    "        \"‚úÖ Learned production best practices\"\n",
    "    ]\n",
    "    \n",
    "    for achievement in achievements:\n",
    "        print(achievement)\n",
    "    \n",
    "    print(f\"\\nüîß Technical Skills Gained:\")\n",
    "    skills = [\n",
    "        \"‚Ä¢ OpenTelemetry SDK configuration and usage\",\n",
    "        \"‚Ä¢ Azure Monitor OpenTelemetry integration\",\n",
    "        \"‚Ä¢ Custom span creation and attribute management\",\n",
    "        \"‚Ä¢ Error tracking and exception recording\",\n",
    "        \"‚Ä¢ Performance measurement and analysis\",\n",
    "        \"‚Ä¢ Production monitoring setup\"\n",
    "    ]\n",
    "    \n",
    "    for skill in skills:\n",
    "        print(skill)\n",
    "    \n",
    "    print(f\"\\nüöÄ Next Workshop Preview:\")\n",
    "    print(\"Workshop 3: AI Agents\")\n",
    "    print(\"‚Ä¢ Create intelligent AI agents with tools\")\n",
    "    print(\"‚Ä¢ Implement function calling capabilities\")\n",
    "    print(\"‚Ä¢ Build multi-step reasoning workflows\")\n",
    "    print(\"‚Ä¢ Trace agent interactions and decision-making\")\n",
    "    \n",
    "    print(f\"\\nüí° Homework:\")\n",
    "    print(\"‚Ä¢ Explore your traces in Azure Application Insights\")\n",
    "    print(\"‚Ä¢ Try adding custom attributes to trace business metrics\")\n",
    "    print(\"‚Ä¢ Experiment with different sampling rates\")\n",
    "    print(\"‚Ä¢ Set up a simple alert on token usage or error rates\")\n",
    "\n",
    "# Final cleanup\n",
    "def cleanup_instrumentation():\n",
    "    \"\"\"\n",
    "    Clean up OpenTelemetry instrumentation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        OpenAIInstrumentor().uninstrument()\n",
    "        print(\"üßπ OpenAI instrumentation cleaned up\")\n",
    "    except Exception as e:\n",
    "        print(f\"Note: Cleanup not needed or failed: {e}\")\n",
    "\n",
    "workshop_summary()\n",
    "cleanup_instrumentation()\n",
    "</VSCode.Cell>\n",
    "\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## üîß Troubleshooting Guide\n",
    "\n",
    "### Common Issues and Solutions\n",
    "\n",
    "#### Tracing Not Appearing\n",
    "- **Problem**: No traces visible in console or Azure\n",
    "- **Solution**: \n",
    "  - Check if OpenAI instrumentation is properly set up\n",
    "  - Verify Azure Monitor connection string\n",
    "  - Ensure you're making actual API calls\n",
    "\n",
    "#### Application Insights Connection Issues\n",
    "- **Problem**: \"No connection string found\"\n",
    "- **Solution**:\n",
    "  - Check if Application Insights is configured in your AI Foundry project\n",
    "  - Go to Azure AI Foundry portal ‚Üí Observability ‚Üí Tracing\n",
    "  - Verify your Azure permissions\n",
    "\n",
    "#### Performance Overhead\n",
    "- **Problem**: Tracing is slowing down your application\n",
    "- **Solution**:\n",
    "  - Implement sampling (trace only 1-10% of requests)\n",
    "  - Use batch processors instead of simple processors\n",
    "  - Disable content recording in production\n",
    "\n",
    "#### Missing Trace Data\n",
    "- **Problem**: Some spans or attributes are missing\n",
    "- **Solution**:\n",
    "  - Check for exceptions in span creation\n",
    "  - Verify attribute names don't contain special characters\n",
    "  - Ensure spans are properly closed\n",
    "\n",
    "### üìö Additional Resources\n",
    "\n",
    "- [OpenTelemetry Python Documentation](https://opentelemetry.io/docs/languages/python/)\n",
    "- [Azure Monitor OpenTelemetry Documentation](https://learn.microsoft.com/azure/azure-monitor/app/opentelemetry-enable)\n",
    "- [Azure AI Foundry Tracing Guide](https://learn.microsoft.com/azure/ai-foundry/how-to/develop/trace-application)\n",
    "- [Application Insights KQL Reference](https://learn.microsoft.com/azure/data-explorer/kusto/query/)\n",
    "\n",
    "### üéÆ Try These Extensions\n",
    "\n",
    "1. **Add Custom Metrics**: Implement custom metrics for token costs\n",
    "2. **Correlation IDs**: Add correlation IDs to track requests across services  \n",
    "3. **Sampling**: Implement different sampling strategies\n",
    "4. **Dashboards**: Create custom dashboards in Application Insights\n",
    "</VSCode.Cell>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azure-openai-workshop (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
